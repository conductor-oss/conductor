{
    "docs": [
        {
            "location": "/", 
            "text": "Conductor\n\n\n\n\n\n\nConductor is an \norchestration\n engine that runs in the cloud. \n\n\nMotivation\n\n\nWe built Conductor to help us orchestrate microservices based process flows at Netflix with the following features:\n\n\n\n\nAllow creating complex process / business flows in which individual task is implemented by a microservice.\n\n\nA JSON DSL based blueprint defines the execution flow.\n\n\nProvide visibility and traceability into the these process flows.\n\n\nExpose control semantics around pause, resume, restart, etc allowing for better devops experience.\n\n\nAllow greater reuse of existing microservices providing an easier path for onboarding.\n\n\nUser interface to visualize the process flows.\n\n\nAbility to synchronously process all the tasks when needed.\n\n\nAbility to scale millions of concurrently running process flows.\n\n\nBacked by a queuing service abstracted from the clients.\n\n\nBe able to operate on HTTP or other transports e.g. gRPC.\n\n\n\n\nWhy not peer to peer choreography?\n\n\nWith peer to peer task choreography, we found it was harder to scale with growing business needs and complexities.\nPub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:\n\n\n\n\nProcess flows are \u201cembedded\u201d within the code of multiple application.\n\n\nOften, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.\n\n\nAlmost no way to systematically answer \u201chow much are we done with process X\u201d?", 
            "title": "Introduction"
        }, 
        {
            "location": "/#conductor", 
            "text": "Conductor is an  orchestration  engine that runs in the cloud.", 
            "title": "Conductor"
        }, 
        {
            "location": "/#motivation", 
            "text": "We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features:   Allow creating complex process / business flows in which individual task is implemented by a microservice.  A JSON DSL based blueprint defines the execution flow.  Provide visibility and traceability into the these process flows.  Expose control semantics around pause, resume, restart, etc allowing for better devops experience.  Allow greater reuse of existing microservices providing an easier path for onboarding.  User interface to visualize the process flows.  Ability to synchronously process all the tasks when needed.  Ability to scale millions of concurrently running process flows.  Backed by a queuing service abstracted from the clients.  Be able to operate on HTTP or other transports e.g. gRPC.   Why not peer to peer choreography?  With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities.\nPub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:   Process flows are \u201cembedded\u201d within the code of multiple application.  Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.  Almost no way to systematically answer \u201chow much are we done with process X\u201d?", 
            "title": "Motivation"
        }, 
        {
            "location": "/intro/", 
            "text": "High Level Architecture\n\n\n\n\nThe API and storage layers are pluggable and provide ability to work with different backend and queue service providers.\n\n\nInstalling and Running\n\n\n\n\nRunning in production\n\n\nFor a detailed configuration guide on installing and running Conductor server in production visit \nConductor Server\n documentation.\n\n\n\n\nRunnin In-Memory Server\n\n\nFollow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor.\n\n\n\n\nWarning\n\n\nIn-Memory server is meant for a quick demonstration purpose and does not store the data on disk.  All the data is lost once the server dies.\n\n\n\n\nCheckout the source from github\n\n\ngit clone git@github.com:Netflix/conductor.git\n\n\n\n\nStart Local Server\n\n\ncd server\n../gradlew server\n# wait for the server to come online\n\n\n\n\nSwagger APIs can be accessed at \nhttp://localhost:8080/\n\n\nStart UI Server\n\n\ncd ui\ngulp watch\n\n\n\n\nLaunch UI at \nhttp://localhost:3000/\n\n\n\n\nNote\n\n\nThe server will load a sample kitchen sink workflow definition by default.  See \nhere\n for details.\n\n\n\n\nRuntime Model\n\n\nConductor follows RPC based communication model where workers are running on a separate machine from the server.  Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.\n\n\n\n\nNotes\n\n\n\n\nWorkers are remote systems and communicates over HTTP (or any supported RPC mechanism) with conductor servers.\n\n\nTask Queues are used to schedule tasks for workers.  We use \ndyno-queues\n internally but it can easily be swapped with SQS or similar pub-sub mechanism.\n\n\nconductor-redis-persistence module uses \nDynomite\n for storing the state and metadata along with \nElasticsearch\n for indexing backend.\n\n\nSee section under extending backend for implementing support for different databases for storage and indexing.\n\n\n\n\nHigh Level Steps\n\n\nSteps required for a new workflow to be registered and get executed:\n\n\n\n\nDefine task definitions used by the workflow.\n\n\nCreate the workflow definition\n\n\nCreate task worker(s) that polls for scheduled tasks at regular interval\n\n\n\n\nTrigger Workflow Execution\n\n\nPOST /workflow/{name}\n{\n    ... //json payload as workflow input\n}\n\n\n\n\nPolling for a task\n\n\nGET /tasks/poll/batch/{taskType}\n\n\n\n\nUpdate task status\n\n\nPOST /tasks\n{\n    \noutputData\n: {\n        \nencodeResult\n:\nsuccess\n,\n        \nlocation\n: \nhttp://cdn.example.com/file/location.png\n\n        //any task specific output\n     },\n     \ntaskStatus\n: \nCOMPLETED\n\n}", 
            "title": "Getting Started"
        }, 
        {
            "location": "/intro/#high-level-architecture", 
            "text": "The API and storage layers are pluggable and provide ability to work with different backend and queue service providers.", 
            "title": "High Level Architecture"
        }, 
        {
            "location": "/intro/#installing-and-running", 
            "text": "Running in production  For a detailed configuration guide on installing and running Conductor server in production visit  Conductor Server  documentation.", 
            "title": "Installing and Running"
        }, 
        {
            "location": "/intro/#runnin-in-memory-server", 
            "text": "Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor.   Warning  In-Memory server is meant for a quick demonstration purpose and does not store the data on disk.  All the data is lost once the server dies.", 
            "title": "Runnin In-Memory Server"
        }, 
        {
            "location": "/intro/#checkout-the-source-from-github", 
            "text": "git clone git@github.com:Netflix/conductor.git", 
            "title": "Checkout the source from github"
        }, 
        {
            "location": "/intro/#start-local-server", 
            "text": "cd server\n../gradlew server\n# wait for the server to come online  Swagger APIs can be accessed at  http://localhost:8080/", 
            "title": "Start Local Server"
        }, 
        {
            "location": "/intro/#start-ui-server", 
            "text": "cd ui\ngulp watch  Launch UI at  http://localhost:3000/   Note  The server will load a sample kitchen sink workflow definition by default.  See  here  for details.", 
            "title": "Start UI Server"
        }, 
        {
            "location": "/intro/#runtime-model", 
            "text": "Conductor follows RPC based communication model where workers are running on a separate machine from the server.  Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.   Notes   Workers are remote systems and communicates over HTTP (or any supported RPC mechanism) with conductor servers.  Task Queues are used to schedule tasks for workers.  We use  dyno-queues  internally but it can easily be swapped with SQS or similar pub-sub mechanism.  conductor-redis-persistence module uses  Dynomite  for storing the state and metadata along with  Elasticsearch  for indexing backend.  See section under extending backend for implementing support for different databases for storage and indexing.", 
            "title": "Runtime Model"
        }, 
        {
            "location": "/intro/#high-level-steps", 
            "text": "Steps required for a new workflow to be registered and get executed:   Define task definitions used by the workflow.  Create the workflow definition  Create task worker(s) that polls for scheduled tasks at regular interval   Trigger Workflow Execution  POST /workflow/{name}\n{\n    ... //json payload as workflow input\n}  Polling for a task  GET /tasks/poll/batch/{taskType}  Update task status  POST /tasks\n{\n     outputData : {\n         encodeResult : success ,\n         location :  http://cdn.example.com/file/location.png \n        //any task specific output\n     },\n      taskStatus :  COMPLETED \n}", 
            "title": "High Level Steps"
        }, 
        {
            "location": "/intro/concepts/", 
            "text": "Workflow Definition\n\n\nWorkflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows.  The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine.\n\nmore details\n\n\nTask Definition\n\n\n\n\nAll tasks need to be registered before they can be used by active workflows.\n\n\nA task can be re-used within multiple workflows.\nWorker tasks fall into two categories:\n\n\nSystem Task\n\n\nWorker Task\n\n\n\n\n\n\n\n\nSystem Tasks\n\n\nSystem tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability.\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nDYNAMIC\n\n\nA worker task which is derived based on the input expression to the task, rather than being statically defined as part of the blueprint\n\n\n\n\n\n\nDECIDE\n\n\nDecision tasks - implements case...switch style fork\n\n\n\n\n\n\nFORK\n\n\nForks a parallel set of tasks.  Each set is scheduled to be executed in parallel\n\n\n\n\n\n\nFORK_JOIN_DYNAMIC\n\n\nSimilar to FORK, but rather than the set of tasks defined in the blueprint for parallel execution, FORK_JOIN_DYNAMIC spawns the parallel tasks based on the input expression to this task\n\n\n\n\n\n\nJOIN\n\n\nComplements FORK and FORK_JOIN_DYNAMIC.  Used to merge one of more parallel branches*\n\n\n\n\n\n\nSUB_WORKFLOW\n\n\nNest another workflow as a sub workflow task.  Upon execution it instantiates the sub workflow and awaits it completion\n\n\n\n\n\n\n\n\nConductor provides an API to create user defined tasks that are excuted in the same JVM as the engine.  see \nWorkflowSystemTask\n interface for details.\n\n\nWorker Taks\n\n\nWorker tasks are implemented by application(s) and runs in a separate environment from Conductor.  The worker tasks can be implemented in any langugage.  These tasks talk to Conductor server via REST API endpionts to poll for tasks and update its status after execution.\n\n\nWorker tasks are identified by task type \nSIMPLE\n in the blueprint.\n\n\nmore details", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/intro/concepts/#workflow-definition", 
            "text": "Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows.  The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. more details", 
            "title": "Workflow Definition"
        }, 
        {
            "location": "/intro/concepts/#task-definition", 
            "text": "All tasks need to be registered before they can be used by active workflows.  A task can be re-used within multiple workflows.\nWorker tasks fall into two categories:  System Task  Worker Task", 
            "title": "Task Definition"
        }, 
        {
            "location": "/intro/concepts/#system-tasks", 
            "text": "System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability.     Name  Purpose      DYNAMIC  A worker task which is derived based on the input expression to the task, rather than being statically defined as part of the blueprint    DECIDE  Decision tasks - implements case...switch style fork    FORK  Forks a parallel set of tasks.  Each set is scheduled to be executed in parallel    FORK_JOIN_DYNAMIC  Similar to FORK, but rather than the set of tasks defined in the blueprint for parallel execution, FORK_JOIN_DYNAMIC spawns the parallel tasks based on the input expression to this task    JOIN  Complements FORK and FORK_JOIN_DYNAMIC.  Used to merge one of more parallel branches*    SUB_WORKFLOW  Nest another workflow as a sub workflow task.  Upon execution it instantiates the sub workflow and awaits it completion     Conductor provides an API to create user defined tasks that are excuted in the same JVM as the engine.  see  WorkflowSystemTask  interface for details.", 
            "title": "System Tasks"
        }, 
        {
            "location": "/intro/concepts/#worker-taks", 
            "text": "Worker tasks are implemented by application(s) and runs in a separate environment from Conductor.  The worker tasks can be implemented in any langugage.  These tasks talk to Conductor server via REST API endpionts to poll for tasks and update its status after execution.  Worker tasks are identified by task type  SIMPLE  in the blueprint.  more details", 
            "title": "Worker Taks"
        }, 
        {
            "location": "/metadata/", 
            "text": "Task Definition\n\n\nConductor maintains a registry of worker task types.  A task type MUST be registered before using in a workflow.\n\n\nExample\n\n\n{\n  \nname\n: \nencode_task\n,\n  \nretryCount\n: 3,\n  \ntimeoutSeconds\n: 1200,\n  \ninputKeys\n: [\n    \nsourceRequestId\n,\n    \nqcElementType\n\n  ],\n  \noutputKeys\n: [\n    \nstate\n,\n    \nskipped\n,\n    \nresult\n\n  ],\n  \ntimeoutPolicy\n: \nTIME_OUT_WF\n,\n  \nretryLogic\n: \nFIXED\n,\n  \nretryDelaySeconds\n: 600,\n  \nresponseTimeoutSeconds\n: 3600\n}\n\n\n\n\n\n\n\n\n\n\nfield\n\n\ndescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nname\n\n\nTask Type\n\n\nUnique\n\n\n\n\n\n\nretryCount\n\n\nNo. of retries to attempt when a task is marked as failure\n\n\n\n\n\n\n\n\nretryLogic\n\n\nMechanism for the retries\n\n\nsee possible values below\n\n\n\n\n\n\ntimeoutSeconds\n\n\nTime in milliseconds, after which the task is marked as TIMED_OUT if not completed after transiting to \nIN_PROGRESS\n status\n\n\nNo timeouts if set to 0\n\n\n\n\n\n\ntimeoutPolicy\n\n\nTask's timeout policy\n\n\nsee possible values below\n\n\n\n\n\n\nresponseTimeoutSeconds\n\n\nif greater than 0, the task is rescheduled if not updated with a status after this time.  Useful when the worker polls for the task but fails to complete due to errors/network failure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutputKeys\n\n\nSet of keys of task's output.  Used for documenting task's output\n\n\n\n\n\n\n\n\n\n\nRetry Logic\n\n\n\n\nFIXED : Reschedule the task afer the \nretryDelaySeconds\n\n\nEXPONENTIAL_BACKOFF : reschedule after \nretryDelaySeconds  * attempNo\n\n\n\n\nTimeout Policy\n\n\n\n\nRETRY : Retries the task again\n\n\nTIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated\n\n\nALERT_ONLY : Registers a counter (task_timeout)\n\n\n\n\nWorkflow Definition\n\n\nWorkflows are define using a JSON based DSL.\n\n\nExample\n\n\n{\n  \nname\n: \nencode_and_deploy\n,\n  \ndescription\n: \nEncodes a file and deploys to CDN\n,\n  \nversion\n: 1,\n  \ntasks\n: [\n    {\n      \nname\n: \nencode\n,\n      \ntaskReferenceName\n: \nencode\n,\n      \ntype\n: \nSIMPLE\n,\n      \ninputParameters\n: {\n        \nfileLocation\n: \n${workflow.input.fileLocation}\n\n      }\n    },\n    {\n      \nname\n: \ndeploy\n,\n      \ntaskReferenceName\n: \nd1\n,\n      \ntype\n: \nSIMPLE\n,\n      \ninputParameters\n: {\n        \nfileLocation\n: \n${encode.output.encodeLocation}\n\n      }\n\n    }\n  ],\n  \noutputParameters\n: {\n    \ncdn_url\n: \n${d1.output.location}\n\n  },\n  \nschemaVersion\n: 2\n}\n\n\n\n\n\n\n\n\n\n\nfield\n\n\ndescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nname\n\n\nName of the workflow\n\n\n\n\n\n\n\n\ndescription\n\n\nDescriptive name of the workflow\n\n\n\n\n\n\n\n\nversion\n\n\nNumeric field used to identify the version of the schema.  Use incrementing numbers\n\n\nWhen starting a workflow execution, if not specified, the definition with highest version is used\n\n\n\n\n\n\ntasks\n\n\nAn array of task defintions as described below.\n\n\n\n\n\n\n\n\noutputParameters\n\n\nJSON template used to generate the output of the workflow\n\n\nIf not specified, the output is defined as the output of the \nlast\n executed task\n\n\n\n\n\n\ninputParameters\n\n\nList of input parameters.  Used for documenting the required inputs to workflow\n\n\noptional\n\n\n\n\n\n\n\n\nTasks within Workflow\n\n\ntasks\n property in a workflow defines an array of tasks to be executed in that order.\nBelow are the mandatory minimum parameters required for each task:\n\n\n\n\n\n\n\n\nfield\n\n\ndescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nname\n\n\nName of the task.  MUST be registered as a task type with Conductor before starting workflow\n\n\n\n\n\n\n\n\ntaskReferenceName\n\n\nAlias used to refer the task within the workflow.  MUST be unique.\n\n\n\n\n\n\n\n\ntype\n\n\nType of task. SIMPLE for tasks executed by remote workers, or one of the system task types\n\n\n\n\n\n\n\n\ninputParameters\n\n\nJSON template that defines the input given to the task\n\n\nSee \"wiring inputs and outputs\" for details\n\n\n\n\n\n\n\n\nIn addition to these paramters, additional parameters speciific to the task type are required as documented \nhere\n\n\nWiring Inputs and Outputs\n\n\nWorkflows are supplied inputs by client when a new execution is triggered. \nWorkflow input is a JSON payload that is available via \n${workflow.input...}\n expressions.\n\n\nEach task in the workflow is given input based on the \ninputParameters\n template configured in workflow definition.  \ninputParameters\n is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution.\n\n\nSyntax for mapping the values follows the pattern as: \n\n\n${SOURCE.input/output.JSONPath}\n\n\n\n\n\n\n\n\n-\n\n\n-\n\n\n\n\n\n\n\n\n\n\nSOURCE\n\n\ncan be either \"workflow\" or reference name of any of the task\n\n\n\n\n\n\ninput/output\n\n\nrefers to either the input or output of the source\n\n\n\n\n\n\nJSONPath\n\n\nJSON path expression to extract JSON fragment from source's input/output\n\n\n\n\n\n\n\n\n\n\nJSON Path Support\n\n\nConductor supports \nJSONPath\n specification and uses Java implementaion from \nhere\n.\n\n\n\n\nExample\n\n\nConsider a task with input configured to use input/output parameters from workflow and a task named \nloc_task\n.\n\n\n{\n  \ninputParameters\n: {\n    \nmovieId\n: \n${workflow.input.movieId}\n,\n    \nurl\n: \n${workflow.input.fileLocation}\n,\n    \nlang\n: \n${loc_task.output.languages[0]}\n,\n    \nhttp_request\n: {\n      \nmethod\n: \nPOST\n,\n      \nurl\n: \nhttp://example.com/${loc_task.output.fileId}/encode\n,\n      \nbody\n: {\n        \nrecipe\n: \n${workflow.input.recipe}\n,\n        \nparams\n: {\n          \nwidth\n: 100,\n          \nheight\n: 100\n        }\n      },\n      \nheaders\n: [\n        {\n          \nAccept\n: \napplication/json\n\n        },\n        {\n          \nContent-Type\n: \napplication/json\n\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nConsider the following as the \nworkflow input\n\n\n{\n  \nmovieId\n: \nmovie_123\n,\n  \nfileLocation\n:\ns3://moviebucket/file123\n,\n  \nrecipe\n:\npng\n\n}\n\n\n\n\nAnd the output of the \nloc_task\n as the following;\n\n\n{\n  \nfileId\n: \nfile_xxx_yyy_zzz\n,\n  \nlanguages\n: [\nen\n,\nja\n,\nes\n]\n}\n\n\n\n\nWhen scheduling the task, Conductor will merge the values from workflow input and loc_tak's output and create the input to the task as follows:\n\n\n{\n  \nmovieId\n: \nmovie_123\n,\n  \nurl\n: \ns3://moviebucket/file123\n,\n  \nlang\n: \nen\n,\n  \nhttp_request\n: {\n    \nmethod\n: \nPOST\n,\n    \nurl\n: \nhttp://example.com/file_xxx_yyy_zzz/encode\n,\n    \nbody\n: {\n      \nrecipe\n: \npng\n,\n      \nparams\n: {\n        \nwidth\n: 100,\n        \nheight\n: 100\n      }\n    },\n    \nheaders\n: [\n      {\n        \nAccept\n: \napplication/json\n\n      },\n      {\n        \nContent-Type\n: \napplication/json\n\n      }\n    ]\n  }\n}", 
            "title": "Metadata Definitions"
        }, 
        {
            "location": "/metadata/#task-definition", 
            "text": "Conductor maintains a registry of worker task types.  A task type MUST be registered before using in a workflow.  Example  {\n   name :  encode_task ,\n   retryCount : 3,\n   timeoutSeconds : 1200,\n   inputKeys : [\n     sourceRequestId ,\n     qcElementType \n  ],\n   outputKeys : [\n     state ,\n     skipped ,\n     result \n  ],\n   timeoutPolicy :  TIME_OUT_WF ,\n   retryLogic :  FIXED ,\n   retryDelaySeconds : 600,\n   responseTimeoutSeconds : 3600\n}     field  description  Notes      name  Task Type  Unique    retryCount  No. of retries to attempt when a task is marked as failure     retryLogic  Mechanism for the retries  see possible values below    timeoutSeconds  Time in milliseconds, after which the task is marked as TIMED_OUT if not completed after transiting to  IN_PROGRESS  status  No timeouts if set to 0    timeoutPolicy  Task's timeout policy  see possible values below    responseTimeoutSeconds  if greater than 0, the task is rescheduled if not updated with a status after this time.  Useful when the worker polls for the task but fails to complete due to errors/network failure.          outputKeys  Set of keys of task's output.  Used for documenting task's output      Retry Logic   FIXED : Reschedule the task afer the  retryDelaySeconds  EXPONENTIAL_BACKOFF : reschedule after  retryDelaySeconds  * attempNo   Timeout Policy   RETRY : Retries the task again  TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated  ALERT_ONLY : Registers a counter (task_timeout)", 
            "title": "Task Definition"
        }, 
        {
            "location": "/metadata/#workflow-definition", 
            "text": "Workflows are define using a JSON based DSL.  Example  {\n   name :  encode_and_deploy ,\n   description :  Encodes a file and deploys to CDN ,\n   version : 1,\n   tasks : [\n    {\n       name :  encode ,\n       taskReferenceName :  encode ,\n       type :  SIMPLE ,\n       inputParameters : {\n         fileLocation :  ${workflow.input.fileLocation} \n      }\n    },\n    {\n       name :  deploy ,\n       taskReferenceName :  d1 ,\n       type :  SIMPLE ,\n       inputParameters : {\n         fileLocation :  ${encode.output.encodeLocation} \n      }\n\n    }\n  ],\n   outputParameters : {\n     cdn_url :  ${d1.output.location} \n  },\n   schemaVersion : 2\n}     field  description  Notes      name  Name of the workflow     description  Descriptive name of the workflow     version  Numeric field used to identify the version of the schema.  Use incrementing numbers  When starting a workflow execution, if not specified, the definition with highest version is used    tasks  An array of task defintions as described below.     outputParameters  JSON template used to generate the output of the workflow  If not specified, the output is defined as the output of the  last  executed task    inputParameters  List of input parameters.  Used for documenting the required inputs to workflow  optional", 
            "title": "Workflow Definition"
        }, 
        {
            "location": "/metadata/#tasks-within-workflow", 
            "text": "tasks  property in a workflow defines an array of tasks to be executed in that order.\nBelow are the mandatory minimum parameters required for each task:     field  description  Notes      name  Name of the task.  MUST be registered as a task type with Conductor before starting workflow     taskReferenceName  Alias used to refer the task within the workflow.  MUST be unique.     type  Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types     inputParameters  JSON template that defines the input given to the task  See \"wiring inputs and outputs\" for details     In addition to these paramters, additional parameters speciific to the task type are required as documented  here", 
            "title": "Tasks within Workflow"
        }, 
        {
            "location": "/metadata/#wiring-inputs-and-outputs", 
            "text": "Workflows are supplied inputs by client when a new execution is triggered. \nWorkflow input is a JSON payload that is available via  ${workflow.input...}  expressions.  Each task in the workflow is given input based on the  inputParameters  template configured in workflow definition.   inputParameters  is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution.  Syntax for mapping the values follows the pattern as:   ${SOURCE.input/output.JSONPath}     -  -      SOURCE  can be either \"workflow\" or reference name of any of the task    input/output  refers to either the input or output of the source    JSONPath  JSON path expression to extract JSON fragment from source's input/output      JSON Path Support  Conductor supports  JSONPath  specification and uses Java implementaion from  here .   Example  Consider a task with input configured to use input/output parameters from workflow and a task named  loc_task .  {\n   inputParameters : {\n     movieId :  ${workflow.input.movieId} ,\n     url :  ${workflow.input.fileLocation} ,\n     lang :  ${loc_task.output.languages[0]} ,\n     http_request : {\n       method :  POST ,\n       url :  http://example.com/${loc_task.output.fileId}/encode ,\n       body : {\n         recipe :  ${workflow.input.recipe} ,\n         params : {\n           width : 100,\n           height : 100\n        }\n      },\n       headers : [\n        {\n           Accept :  application/json \n        },\n        {\n           Content-Type :  application/json \n        }\n      ]\n    }\n  }\n}  Consider the following as the  workflow input  {\n   movieId :  movie_123 ,\n   fileLocation : s3://moviebucket/file123 ,\n   recipe : png \n}  And the output of the  loc_task  as the following;  {\n   fileId :  file_xxx_yyy_zzz ,\n   languages : [ en , ja , es ]\n}  When scheduling the task, Conductor will merge the values from workflow input and loc_tak's output and create the input to the task as follows:  {\n   movieId :  movie_123 ,\n   url :  s3://moviebucket/file123 ,\n   lang :  en ,\n   http_request : {\n     method :  POST ,\n     url :  http://example.com/file_xxx_yyy_zzz/encode ,\n     body : {\n       recipe :  png ,\n       params : {\n         width : 100,\n         height : 100\n      }\n    },\n     headers : [\n      {\n         Accept :  application/json \n      },\n      {\n         Content-Type :  application/json \n      }\n    ]\n  }\n}", 
            "title": "Wiring Inputs and Outputs"
        }, 
        {
            "location": "/metadata/systask/", 
            "text": "Dynamic Task\n\n\nParameters:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndynamicTaskNameParam\n\n\nName of the parameter from the task input whose value is used to schedule the task.  e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nname\n: \nuser_task\n,\n  \ntaskReferenceName\n: \nt1\n,\n  \ninputParameters\n: {\n    \nfiles\n: \n${workflow.input.files}\n,\n    \ntaskToExecute\n: \n${workflow.input.user_supplied_task}\n\n  },\n  \ntype\n: \nDYNAMIC\n,\n  \ndynamicTaskNameParam\n: \ntaskToExecute\n\n}\n\n\n\n\nIf the workflow is started with input parameter user_supplied_task's value as \nuser_task_2\n, Conductor will schedule \nuser_task_2\n when scheduling this dynamic task.\n\n\nDecision\n\n\nA decision task is similar to \ncase...switch\n statement in a programming langugage.\nThe task takes 3 parameters:\n\n\nParameters:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ncaseValueParam\n\n\nName of the parameter in task input whose value will be used as a switch.\n\n\n\n\n\n\ndecisionCases\n\n\nMap where key is possible values of \ncaseValueParam\n with value being list of tasks to be executed.\n\n\n\n\n\n\ndefaultCase\n\n\nList of tasks to be executed when no matching value if found in decision case (default condition)\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nname\n: \ndecide_task\n,\n  \ntaskReferenceName\n: \ndecide1\n,\n  \ninputParameters\n: {\n    \ncase_value_param\n: \n${workflow.input.movieType}\n\n  },\n  \ntype\n: \nDECISION\n,\n  \ncaseValueParam\n: \ncase_value_param\n,\n  \ndecisionCases\n: {\n    \nShow\n: [\n      {\n        \nname\n: \nsetup_episodes\n,\n        \ntaskReferenceName\n: \nse1\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      },\n      {\n        \nname\n: \ngenerate_episode_artwork\n,\n        \ntaskReferenceName\n: \nga\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      }\n    ],\n    \nMovie\n: [\n      {\n        \nname\n: \nsetup_movie\n,\n        \ntaskReferenceName\n: \nsm\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      },\n      {\n        \nname\n: \ngenerate_movie_artwork\n,\n        \ntaskReferenceName\n: \ngma\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      }\n    ]\n  }\n}\n\n\n\n\nFork\n\n\nFork is used to schedule parallel set of tasks.\n\n\nParameters:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nforkTasks\n\n\nA list of list of tasks.  Each sublist is scheduled to be executed in parallel.  However, tasks within the sublists are scheduled in a serial fashion.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nforkTasks\n: [\n    [\n      {\n        \nname\n: \ntask11\n,\n        \ntaskReferenceName\n: \nt11\n\n      },\n      {\n        \nname\n: \ntask12\n,\n        \ntaskReferenceName\n: \nt12\n\n      }\n    ],\n    [\n      {\n        \nname\n: \ntask21\n,\n        \ntaskReferenceName\n: \nt21\n\n      },\n      {\n        \nname\n: \ntask22\n,\n        \ntaskReferenceName\n: \nt22\n\n      }\n    ]\n  ]\n}\n\n\n\n\nWhen executed, \ntask11\n and \ntask21\n are scheduled to be executed at the same time.\n\n\nDynamic Fork\n\n\nA dynamic fork is same as FORK_JOIN task.  Except that the list of tasks to be forked is provided at runtime using task's input.  Useful when number of tasks to be forked is not fixed and varies based on the input.\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndynamicForkTasksParam\n\n\nName of the parameter that contains list of workflow task configuration to be executed in parallel\n\n\n\n\n\n\ndynamicForkTasksInputParamName\n\n\nName of the parameter whose value should be a map with key as forked task's referece name and value as input the forked task\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \ndynamicTasks\n: \n${taskA.output.dynamicTasksJSON}\n,\n  \ndynamicTasksInput\n: \n${taskA.output.dynamicTasksInputJSON}\n,\n  \ntype\n: \nFORK_JOIN_DYNAMIC\n,\n  \ndynamicForkTasksParam\n: \ndynamicTasks\n,\n  \ndynamicForkTasksInputParamName\n: \ndynamicTasksInput\n\n}\n\n\n\n\nConsider \ntaskA\n's output as:\n\n\n{\n  \ndynamicTasksInputJSON\n: {\n    \nforkedTask1\n: {\n      \nwidth\n: 100,\n      \nheight\n: 100,\n      \nparams\n: {\n        \nrecipe\n: \njpg\n\n      }\n    },\n    \nforkedTask12\n: {\n      \nwidth\n: 200,\n      \nheight\n: 200,\n      \nparams\n: {\n        \nrecipe\n: \njpg\n\n      }\n    }\n  },\n  \ndynamicTasksJSON\n: [\n    {\n      \nname\n: \nencode_task\n,\n      \ntaskReferenceName\n: \nforkedTask1\n,\n      \ntype\n: \nSIMPLE\n\n    },\n    {\n      \nname\n: \nencode_task\n,\n      \ntaskReferenceName\n: \nforkedTask2\n,\n      \ntype\n: \nSIMPLE\n\n    }\n  ]\n}\n\n\n\n\nWhen executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_\n\n\n\n\nDyanmic Fork and Join\n\n\nA Join task MUST follow FORK_JOIN_DYNAMIC\n\n\nWorkflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task.  However, given the dynamic nature of the task, no joinOn parameters are required for this Join.  The join will wait for ALL the forked branches to complete before completing.\n\n\nUnlike FORK, which can execute parallel flows with each fork executing a series of tasks in  sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork.  However, forked task can be a Sub Workflow, allowing for more complex execution flows.\n\n\n\n\nJoin\n\n\nJoin task is used to wait for completion of one or more tasks spawned by fork tasks.\n\n\nParameters\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\njoinOn\n\n\nList of task reference name, for which the JOIN will wait for completion.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n    \njoinOn\n: [\ntaskRef1\n, \ntaskRef3\n]\n}\n\n\n\n\nJoin Task Output\n\n\nFork task's output will be a JSON object with key being the task reference name and value as the output of the fork task.\n\n\nSub Workflow\n\n\nSub Workflow task allows for nesting a workflow within another workflow.\n\n\nParameters\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nsubWorkflowParam\n\n\nList of task reference name, for which the JOIN will wait for completion.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nname\n: \nsub_workflow_task\n,\n  \ntaskReferenceName\n: \nsub1\n,\n  \ninputParameters\n: {\n    \nrequestId\n: \n${workflow.input.requestId}\n,\n    \nfile\n: \n${encode.output.location}\n\n  },\n  \ntype\n: \nSUB_WORKFLOW\n,\n  \nsubWorkflowParam\n: {\n    \nname\n: \ndeployment_workflow\n,\n    \nversion\n: 1\n  }\n}\n\n\n\n\nWhen executed, a \ndeployment_workflow\n is executed with two input parameters requestId and \nfile\n.  The task is marked as completed upon the completion of the spawned workflow.  If the sub-workflow is terminated or fails the task is marked as failure and retried if configured. \n\n\nWait\n\n\nA wait task is implemented as a gate that remains in \nIN_PROGRESS\n state unless marked as \nCOMPLETED\n or \nFAILED\n by an external trigger.\nTo use a wait task, set the task type as \nWAIT\n\n\nParameters\n\n\nNone required.\n\n\nExernal Triggers for Wait Task\n\n\nTask Resource endpoint can be used to update the status of a task to a terminate state. \n\n\nContrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on.  As the messages arrive, they are marked as \nCOMPLETED\n or \nFAILED\n.  \n\n\nSQS Queues\n\n\n\n\nSQS queues used by the server to update the task status can be retrieve using the following API:\n\n\n\n\nGET /queue\n\n\n\n\n\n\nWhen updating the status of the task, the message needs to conform to the following spec:\n\n\nMessage has to be a valid JSON string.\n\n\nThe message JSON should contain a key named \nexternalId\n with the value being a JSONified string that contains the following keys:\n\n\nworkflowId\n: Id of the workflow\n\n\ntaskRefName\n: Task reference name that should be updated.\n\n\n\n\n\n\nEach queue represents a specific task status and tasks are marked accordingly.  e.g. message coming to a \nCOMPLETED\n queue marks the task status as \nCOMPLETED\n.\n\n\nTasks' output is updated with the message.\n\n\n\n\n\n\n\n\nExample SQS Payload:\n\n\n{\n  \nsome_key\n: \nvaluex\n,\n  \nexternalId\n: \n{\\\ntaskRefName\\\n:\\\nTASK_REFERENCE_NAME\\\n,\\\nworkflowId\\\n:\\\nWORKFLOW_ID\\\n}\n\n}\n\n\n\n\nHTTP\n\n\nAn HTTP task is used to make calls to another microservice over HTTP.\n\n\nParameters\n\n\nThe task expects an input parameter named \nhttp_request\n as part of the task's input with the following details:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nuri\n\n\nURI for the service.  Can be a partial when using vipAddress or includes the server address.\n\n\n\n\n\n\nmethod\n\n\nHTTP method.  One of the GET, PUT, POST, DELETE, OPTIONS, HEAD\n\n\n\n\n\n\naccept\n\n\nAccpet header as required by server.\n\n\n\n\n\n\ncontentType\n\n\nContent Type - supported types are text/plain, text/html and, application/json\n\n\n\n\n\n\nheaders\n\n\nA map of additional http headers to be sent along with the request.\n\n\n\n\n\n\nbody\n\n\nRequest body\n\n\n\n\n\n\nvipAddress\n\n\nWhen using discovery based service URLs.\n\n\n\n\n\n\n\n\nHTTP Task Output\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nresponse\n\n\nJSON body containing the response if one is present\n\n\n\n\n\n\nheaders\n\n\nResponse Headers\n\n\n\n\n\n\nstatusCode\n\n\nInteger status code\n\n\n\n\n\n\n\n\nExample\n\n\nTask Input payload using vipAddress\n\n\n{\n  \nhttp_request\n: {\n    \nvipAddress\n: \nexamplevip-prod\n,\n    \nuri\n: \n/\n,\n    \nmethod\n: \nGET\n,\n    \naccept\n: \ntext/plain\n\n  }\n}\n\n\n\n\nTask Input using an absolute URL\n\n\n{\n  \nhttp_request\n: {\n    \nuri\n: \nhttp://example.com/\n,\n    \nmethod\n: \nGET\n,\n    \naccept\n: \ntext/plain\n\n  }\n}\n\n\n\n\nThe task is marked as \nFAILED\n if the request cannot be completed or the remote server returns non successful status code. \n\n\n\n\nNote\n\n\nHTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response.  XML input/output is currently not supported.  However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value.", 
            "title": "System Tasks"
        }, 
        {
            "location": "/metadata/systask/#dynamic-task", 
            "text": "", 
            "title": "Dynamic Task"
        }, 
        {
            "location": "/metadata/systask/#parameters", 
            "text": "name  description      dynamicTaskNameParam  Name of the parameter from the task input whose value is used to schedule the task.  e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/metadata/systask/#example", 
            "text": "{\n   name :  user_task ,\n   taskReferenceName :  t1 ,\n   inputParameters : {\n     files :  ${workflow.input.files} ,\n     taskToExecute :  ${workflow.input.user_supplied_task} \n  },\n   type :  DYNAMIC ,\n   dynamicTaskNameParam :  taskToExecute \n}  If the workflow is started with input parameter user_supplied_task's value as  user_task_2 , Conductor will schedule  user_task_2  when scheduling this dynamic task.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#decision", 
            "text": "A decision task is similar to  case...switch  statement in a programming langugage.\nThe task takes 3 parameters:", 
            "title": "Decision"
        }, 
        {
            "location": "/metadata/systask/#parameters_1", 
            "text": "name  description      caseValueParam  Name of the parameter in task input whose value will be used as a switch.    decisionCases  Map where key is possible values of  caseValueParam  with value being list of tasks to be executed.    defaultCase  List of tasks to be executed when no matching value if found in decision case (default condition)", 
            "title": "Parameters:"
        }, 
        {
            "location": "/metadata/systask/#example_1", 
            "text": "{\n   name :  decide_task ,\n   taskReferenceName :  decide1 ,\n   inputParameters : {\n     case_value_param :  ${workflow.input.movieType} \n  },\n   type :  DECISION ,\n   caseValueParam :  case_value_param ,\n   decisionCases : {\n     Show : [\n      {\n         name :  setup_episodes ,\n         taskReferenceName :  se1 ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      },\n      {\n         name :  generate_episode_artwork ,\n         taskReferenceName :  ga ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      }\n    ],\n     Movie : [\n      {\n         name :  setup_movie ,\n         taskReferenceName :  sm ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      },\n      {\n         name :  generate_movie_artwork ,\n         taskReferenceName :  gma ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      }\n    ]\n  }\n}", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#fork", 
            "text": "Fork is used to schedule parallel set of tasks.", 
            "title": "Fork"
        }, 
        {
            "location": "/metadata/systask/#parameters_2", 
            "text": "name  description      forkTasks  A list of list of tasks.  Each sublist is scheduled to be executed in parallel.  However, tasks within the sublists are scheduled in a serial fashion.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/metadata/systask/#example_2", 
            "text": "{\n   forkTasks : [\n    [\n      {\n         name :  task11 ,\n         taskReferenceName :  t11 \n      },\n      {\n         name :  task12 ,\n         taskReferenceName :  t12 \n      }\n    ],\n    [\n      {\n         name :  task21 ,\n         taskReferenceName :  t21 \n      },\n      {\n         name :  task22 ,\n         taskReferenceName :  t22 \n      }\n    ]\n  ]\n}  When executed,  task11  and  task21  are scheduled to be executed at the same time.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#dynamic-fork", 
            "text": "A dynamic fork is same as FORK_JOIN task.  Except that the list of tasks to be forked is provided at runtime using task's input.  Useful when number of tasks to be forked is not fixed and varies based on the input.     name  description      dynamicForkTasksParam  Name of the parameter that contains list of workflow task configuration to be executed in parallel    dynamicForkTasksInputParamName  Name of the parameter whose value should be a map with key as forked task's referece name and value as input the forked task", 
            "title": "Dynamic Fork"
        }, 
        {
            "location": "/metadata/systask/#example_3", 
            "text": "{\n   dynamicTasks :  ${taskA.output.dynamicTasksJSON} ,\n   dynamicTasksInput :  ${taskA.output.dynamicTasksInputJSON} ,\n   type :  FORK_JOIN_DYNAMIC ,\n   dynamicForkTasksParam :  dynamicTasks ,\n   dynamicForkTasksInputParamName :  dynamicTasksInput \n}  Consider  taskA 's output as:  {\n   dynamicTasksInputJSON : {\n     forkedTask1 : {\n       width : 100,\n       height : 100,\n       params : {\n         recipe :  jpg \n      }\n    },\n     forkedTask12 : {\n       width : 200,\n       height : 200,\n       params : {\n         recipe :  jpg \n      }\n    }\n  },\n   dynamicTasksJSON : [\n    {\n       name :  encode_task ,\n       taskReferenceName :  forkedTask1 ,\n       type :  SIMPLE \n    },\n    {\n       name :  encode_task ,\n       taskReferenceName :  forkedTask2 ,\n       type :  SIMPLE \n    }\n  ]\n}  When executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_   Dyanmic Fork and Join  A Join task MUST follow FORK_JOIN_DYNAMIC  Workflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task.  However, given the dynamic nature of the task, no joinOn parameters are required for this Join.  The join will wait for ALL the forked branches to complete before completing.  Unlike FORK, which can execute parallel flows with each fork executing a series of tasks in  sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork.  However, forked task can be a Sub Workflow, allowing for more complex execution flows.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#join", 
            "text": "Join task is used to wait for completion of one or more tasks spawned by fork tasks.", 
            "title": "Join"
        }, 
        {
            "location": "/metadata/systask/#parameters_3", 
            "text": "name  description      joinOn  List of task reference name, for which the JOIN will wait for completion.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#example_4", 
            "text": "{\n     joinOn : [ taskRef1 ,  taskRef3 ]\n}", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#join-task-output", 
            "text": "Fork task's output will be a JSON object with key being the task reference name and value as the output of the fork task.", 
            "title": "Join Task Output"
        }, 
        {
            "location": "/metadata/systask/#sub-workflow", 
            "text": "Sub Workflow task allows for nesting a workflow within another workflow.", 
            "title": "Sub Workflow"
        }, 
        {
            "location": "/metadata/systask/#parameters_4", 
            "text": "name  description      subWorkflowParam  List of task reference name, for which the JOIN will wait for completion.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#example_5", 
            "text": "{\n   name :  sub_workflow_task ,\n   taskReferenceName :  sub1 ,\n   inputParameters : {\n     requestId :  ${workflow.input.requestId} ,\n     file :  ${encode.output.location} \n  },\n   type :  SUB_WORKFLOW ,\n   subWorkflowParam : {\n     name :  deployment_workflow ,\n     version : 1\n  }\n}  When executed, a  deployment_workflow  is executed with two input parameters requestId and  file .  The task is marked as completed upon the completion of the spawned workflow.  If the sub-workflow is terminated or fails the task is marked as failure and retried if configured.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#wait", 
            "text": "A wait task is implemented as a gate that remains in  IN_PROGRESS  state unless marked as  COMPLETED  or  FAILED  by an external trigger.\nTo use a wait task, set the task type as  WAIT", 
            "title": "Wait"
        }, 
        {
            "location": "/metadata/systask/#parameters_5", 
            "text": "None required.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#exernal-triggers-for-wait-task", 
            "text": "Task Resource endpoint can be used to update the status of a task to a terminate state.   Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on.  As the messages arrive, they are marked as  COMPLETED  or  FAILED .", 
            "title": "Exernal Triggers for Wait Task"
        }, 
        {
            "location": "/metadata/systask/#sqs-queues", 
            "text": "SQS queues used by the server to update the task status can be retrieve using the following API:   GET /queue   When updating the status of the task, the message needs to conform to the following spec:  Message has to be a valid JSON string.  The message JSON should contain a key named  externalId  with the value being a JSONified string that contains the following keys:  workflowId : Id of the workflow  taskRefName : Task reference name that should be updated.    Each queue represents a specific task status and tasks are marked accordingly.  e.g. message coming to a  COMPLETED  queue marks the task status as  COMPLETED .  Tasks' output is updated with the message.", 
            "title": "SQS Queues"
        }, 
        {
            "location": "/metadata/systask/#example-sqs-payload", 
            "text": "{\n   some_key :  valuex ,\n   externalId :  {\\ taskRefName\\ :\\ TASK_REFERENCE_NAME\\ ,\\ workflowId\\ :\\ WORKFLOW_ID\\ } \n}", 
            "title": "Example SQS Payload:"
        }, 
        {
            "location": "/metadata/systask/#http", 
            "text": "An HTTP task is used to make calls to another microservice over HTTP.", 
            "title": "HTTP"
        }, 
        {
            "location": "/metadata/systask/#parameters_6", 
            "text": "The task expects an input parameter named  http_request  as part of the task's input with the following details:     name  description      uri  URI for the service.  Can be a partial when using vipAddress or includes the server address.    method  HTTP method.  One of the GET, PUT, POST, DELETE, OPTIONS, HEAD    accept  Accpet header as required by server.    contentType  Content Type - supported types are text/plain, text/html and, application/json    headers  A map of additional http headers to be sent along with the request.    body  Request body    vipAddress  When using discovery based service URLs.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#http-task-output", 
            "text": "name  description      response  JSON body containing the response if one is present    headers  Response Headers    statusCode  Integer status code", 
            "title": "HTTP Task Output"
        }, 
        {
            "location": "/metadata/systask/#example_6", 
            "text": "Task Input payload using vipAddress  {\n   http_request : {\n     vipAddress :  examplevip-prod ,\n     uri :  / ,\n     method :  GET ,\n     accept :  text/plain \n  }\n}  Task Input using an absolute URL  {\n   http_request : {\n     uri :  http://example.com/ ,\n     method :  GET ,\n     accept :  text/plain \n  }\n}  The task is marked as  FAILED  if the request cannot be completed or the remote server returns non successful status code.    Note  HTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response.  XML input/output is currently not supported.  However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/kitchensink/", 
            "text": "An example kitchensink workflow that demonstrates the usage of all the schema constructs.\n\n\nDefinition\n\n\n{\n  \nname\n: \nkitchensink\n,\n  \ndescription\n: \nkitchensink workflow\n,\n  \nversion\n: 1,\n  \ntasks\n: [\n    {\n      \nname\n: \nperf_ task_1\n,\n      \ntaskReferenceName\n: \nperf_ task_1\n,\n      \ninputParameters\n: {\n        \nmod\n: \n${workflow.input.mod}\n,\n        \noddEven\n: \n${workflow.input.oddEven}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    },\n    {\n      \nname\n: \ndyntask\n,\n      \ntaskReferenceName\n: \nperf_ task_2\n,\n      \ninputParameters\n: {\n        \ntaskToExecute\n: \n${workflow.input.task2Name}\n\n      },\n      \ntype\n: \nDYNAMIC\n,\n      \ndynamicTaskNameParam\n: \ntaskToExecute\n\n    },\n    {\n      \nname\n: \noddEvenDecision\n,\n      \ntaskReferenceName\n: \noddEvenDecision\n,\n      \ninputParameters\n: {\n        \noddEven\n: \n${perf_ task_2.output.oddEven}\n\n      },\n      \ntype\n: \nDECISION\n,\n      \ncaseValueParam\n: \noddEven\n,\n      \ndecisionCases\n: {\n        \n0\n: [\n          {\n            \nname\n: \nperf_ task_4\n,\n            \ntaskReferenceName\n: \nperf_ task_4\n,\n            \ninputParameters\n: {\n              \nmod\n: \n${perf_ task_2.output.mod}\n,\n              \noddEven\n: \n${perf_ task_2.output.oddEven}\n\n            },\n            \ntype\n: \nSIMPLE\n\n          },\n          {\n            \nname\n: \ndynamic_fanout\n,\n            \ntaskReferenceName\n: \nfanout1\n,\n            \ninputParameters\n: {\n              \ndynamicTasks\n: \n${perf_ task_4.output.dynamicTasks}\n,\n              \ninput\n: \n${perf_ task_4.output.inputs}\n\n            },\n            \ntype\n: \nFORK_JOIN_DYNAMIC\n,\n            \ndynamicForkTasksParam\n: \ndynamicTasks\n,\n            \ndynamicForkTasksInputParamName\n: \ninput\n\n          },\n          {\n            \nname\n: \ndynamic_join\n,\n            \ntaskReferenceName\n: \njoin1\n,\n            \ntype\n: \nJOIN\n\n          }\n        ],\n        \n1\n: [\n          {\n            \nname\n: \nfork_join\n,\n            \ntaskReferenceName\n: \nforkx\n,\n            \ntype\n: \nFORK_JOIN\n,\n            \nforkTasks\n: [\n              [\n                {\n                  \nname\n: \nperf_ task_10\n,\n                  \ntaskReferenceName\n: \nperf_ task_10\n,\n                  \ntype\n: \nSIMPLE\n\n                },\n                {\n                  \nname\n: \nsub_workflow_x\n,\n                  \ntaskReferenceName\n: \nwf3\n,\n                  \ninputParameters\n: {\n                    \nmod\n: \n${perf_ task_1.output.mod}\n,\n                    \noddEven\n: \n${perf_ task_1.output.oddEven}\n\n                  },\n                  \ntype\n: \nSUB_WORKFLOW\n,\n                  \nsubWorkflowParam\n: {\n                    \nname\n: \nsub_flow_1\n,\n                    \nversion\n: 1\n                  }\n                }\n              ],\n              [\n                {\n                  \nname\n: \nperf_ task_11\n,\n                  \ntaskReferenceName\n: \nperf_ task_11\n,\n                  \ntype\n: \nSIMPLE\n\n                },\n                {\n                  \nname\n: \nsub_workflow_x\n,\n                  \ntaskReferenceName\n: \nwf4\n,\n                  \ninputParameters\n: {\n                    \nmod\n: \n${perf_ task_1.output.mod}\n,\n                    \noddEven\n: \n${perf_ task_1.output.oddEven}\n\n                  },\n                  \ntype\n: \nSUB_WORKFLOW\n,\n                  \nsubWorkflowParam\n: {\n                    \nname\n: \nsub_flow_1\n,\n                    \nversion\n: 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n            \nname\n: \njoin\n,\n            \ntaskReferenceName\n: \njoin2\n,\n            \ntype\n: \nJOIN\n,\n            \njoinOn\n: [\n              \nwf3\n,\n              \nwf4\n\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \nname\n: \nsearch_elasticsearch\n,\n      \ntaskReferenceName\n: \nget_es_1\n,\n      \ninputParameters\n: {\n        \nhttp_request\n: {\n          \nuri\n: \nhttp://localhost:9200/wfe/workflow/_search?size=10\n,\n          \nmethod\n: \nGET\n\n        }\n      },\n      \ntype\n: \nHTTP\n\n    },\n    {\n      \nname\n: \nperf_task_30\n,\n      \ntaskReferenceName\n: \nperf_task_30\n,\n      \ninputParameters\n: {\n        \nstatuses\n: \n${get_es_1.output..status}\n,\n        \nfistWorkflowId\n: \n${get_es_1.output.workflowId[0]}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    }\n  ],\n  \noutputParameters\n: {\n    \nstatues\n: \n${get_es_1.output..status}\n,\n    \nworkflowIds\n: \n${get_es_1.output..workflowId}\n\n  },\n  \nschemaVersion\n: 2\n}\n\n\n\n\nVisual Flow", 
            "title": "Kitchensink Example"
        }, 
        {
            "location": "/metadata/kitchensink/#definition", 
            "text": "{\n   name :  kitchensink ,\n   description :  kitchensink workflow ,\n   version : 1,\n   tasks : [\n    {\n       name :  perf_ task_1 ,\n       taskReferenceName :  perf_ task_1 ,\n       inputParameters : {\n         mod :  ${workflow.input.mod} ,\n         oddEven :  ${workflow.input.oddEven} \n      },\n       type :  SIMPLE \n    },\n    {\n       name :  dyntask ,\n       taskReferenceName :  perf_ task_2 ,\n       inputParameters : {\n         taskToExecute :  ${workflow.input.task2Name} \n      },\n       type :  DYNAMIC ,\n       dynamicTaskNameParam :  taskToExecute \n    },\n    {\n       name :  oddEvenDecision ,\n       taskReferenceName :  oddEvenDecision ,\n       inputParameters : {\n         oddEven :  ${perf_ task_2.output.oddEven} \n      },\n       type :  DECISION ,\n       caseValueParam :  oddEven ,\n       decisionCases : {\n         0 : [\n          {\n             name :  perf_ task_4 ,\n             taskReferenceName :  perf_ task_4 ,\n             inputParameters : {\n               mod :  ${perf_ task_2.output.mod} ,\n               oddEven :  ${perf_ task_2.output.oddEven} \n            },\n             type :  SIMPLE \n          },\n          {\n             name :  dynamic_fanout ,\n             taskReferenceName :  fanout1 ,\n             inputParameters : {\n               dynamicTasks :  ${perf_ task_4.output.dynamicTasks} ,\n               input :  ${perf_ task_4.output.inputs} \n            },\n             type :  FORK_JOIN_DYNAMIC ,\n             dynamicForkTasksParam :  dynamicTasks ,\n             dynamicForkTasksInputParamName :  input \n          },\n          {\n             name :  dynamic_join ,\n             taskReferenceName :  join1 ,\n             type :  JOIN \n          }\n        ],\n         1 : [\n          {\n             name :  fork_join ,\n             taskReferenceName :  forkx ,\n             type :  FORK_JOIN ,\n             forkTasks : [\n              [\n                {\n                   name :  perf_ task_10 ,\n                   taskReferenceName :  perf_ task_10 ,\n                   type :  SIMPLE \n                },\n                {\n                   name :  sub_workflow_x ,\n                   taskReferenceName :  wf3 ,\n                   inputParameters : {\n                     mod :  ${perf_ task_1.output.mod} ,\n                     oddEven :  ${perf_ task_1.output.oddEven} \n                  },\n                   type :  SUB_WORKFLOW ,\n                   subWorkflowParam : {\n                     name :  sub_flow_1 ,\n                     version : 1\n                  }\n                }\n              ],\n              [\n                {\n                   name :  perf_ task_11 ,\n                   taskReferenceName :  perf_ task_11 ,\n                   type :  SIMPLE \n                },\n                {\n                   name :  sub_workflow_x ,\n                   taskReferenceName :  wf4 ,\n                   inputParameters : {\n                     mod :  ${perf_ task_1.output.mod} ,\n                     oddEven :  ${perf_ task_1.output.oddEven} \n                  },\n                   type :  SUB_WORKFLOW ,\n                   subWorkflowParam : {\n                     name :  sub_flow_1 ,\n                     version : 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n             name :  join ,\n             taskReferenceName :  join2 ,\n             type :  JOIN ,\n             joinOn : [\n               wf3 ,\n               wf4 \n            ]\n          }\n        ]\n      }\n    },\n    {\n       name :  search_elasticsearch ,\n       taskReferenceName :  get_es_1 ,\n       inputParameters : {\n         http_request : {\n           uri :  http://localhost:9200/wfe/workflow/_search?size=10 ,\n           method :  GET \n        }\n      },\n       type :  HTTP \n    },\n    {\n       name :  perf_task_30 ,\n       taskReferenceName :  perf_task_30 ,\n       inputParameters : {\n         statuses :  ${get_es_1.output..status} ,\n         fistWorkflowId :  ${get_es_1.output.workflowId[0]} \n      },\n       type :  SIMPLE \n    }\n  ],\n   outputParameters : {\n     statues :  ${get_es_1.output..status} ,\n     workflowIds :  ${get_es_1.output..workflowId} \n  },\n   schemaVersion : 2\n}", 
            "title": "Definition"
        }, 
        {
            "location": "/metadata/kitchensink/#visual-flow", 
            "text": "", 
            "title": "Visual Flow"
        }, 
        {
            "location": "/server/", 
            "text": "Instaling\n\n\nRequirements\n\n\n\n\nDatabase\n: \nDynomite\n\n\nIndexing Backend\n: \nElasticsearch 2.x\n\n\nServlet Container\n: Tomcat, Jetty, or similar running JDK 1.8 or higher\n\n\n\n\nThere are 3 ways in which you can install Conductor:\n\n\n1. Build from source\n\n\nTo build from source, checkout the code from github and build server module using \ngradle build\n command.  This should produce a conductor-server-all-VERSION.jar.\nThe jar can be executed using:\n\n\njava -jar conductor-server-all-VERSION.jar\n\n\n\n\n2. Download pre-built binaries from jcenter or maven central\n\n\nUse the following coordiates:\n\n\n\n\n\n\n\n\ngroup\n\n\nartifact\n\n\nversion\n\n\n\n\n\n\n\n\n\n\ncom.netflix.conductor\n\n\nconductor-server-all\n\n\n1.6.+\n\n\n\n\n\n\n\n\n3. Use the pre-configured Docker image\n\n\nComing soon...\n\n\nConfiguration\n\n\nConductor server uses a property file based configuration.  The property file is passed to the Main class as a command line argument.\n\n\njava -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path]\n\n\n\n\nlog4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console).\n\n\nConfiguration Parameters\n\n\n\n# Database persistence model.  Possible values are memory, redis, and dynomite.\n# If ommitted, the persistence used is memory\n#\n# memory : The data is stored in memory and lost when the server dies.  Useful for testing or demo\n# redis : non-Dynomite based redis instance\n# dynomite : Dynomite cluster.  Use this for HA configuration.\ndb=dynomite\n\n# Dynomite Cluster details.\n# format is host:port:rack separated by semicolon  \nworkflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e\n\n# Dynomite cluster name\nworkflow.dynomite.cluster.name=dyno_cluster_name\n\n# Namespace for the keys stored in Dynomite/Redis \nworkflow.namespace.prefix=conductor\n\n# Namespace prefix for the dyno queues\nworkflow.namespace.queue.prefix=conductor_queues\n\n# No. of threads allocated to dyno-queues (optional)\nqueues.dynomite.threads=10\n\n# Non-quorum port used to connect to local redis.  Used by dyno-queues.\n# When using redis directly, set this to the same port as redis server\n# For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.\nqueues.dynomite.nonQuorum.port=22122\n\n# Transport address to elasticsearch\nworkflow.elasticsearch.url=localhost:9300\n\n# Name of the elasticsearch cluster\nworkflow.elasticsearch.index.name=conductor\n\n# Additional modules (optional)\nconductor.additional.modules=class_extending_com.google.inject.AbstractModule\n\n\n\n\n\nHigh Availability Configuration\n\n\nConductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs.  The scalability of the server is achieved by scaling the \nDynomite\n cluster along with \ndyno-queues\n which is used for queues.\n\n\nClients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack).", 
            "title": "Conductor Server"
        }, 
        {
            "location": "/server/#instaling", 
            "text": "", 
            "title": "Instaling"
        }, 
        {
            "location": "/server/#requirements", 
            "text": "Database :  Dynomite  Indexing Backend :  Elasticsearch 2.x  Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher   There are 3 ways in which you can install Conductor:", 
            "title": "Requirements"
        }, 
        {
            "location": "/server/#1-build-from-source", 
            "text": "To build from source, checkout the code from github and build server module using  gradle build  command.  This should produce a conductor-server-all-VERSION.jar.\nThe jar can be executed using:  java -jar conductor-server-all-VERSION.jar", 
            "title": "1. Build from source"
        }, 
        {
            "location": "/server/#2-download-pre-built-binaries-from-jcenter-or-maven-central", 
            "text": "Use the following coordiates:     group  artifact  version      com.netflix.conductor  conductor-server-all  1.6.+", 
            "title": "2. Download pre-built binaries from jcenter or maven central"
        }, 
        {
            "location": "/server/#3-use-the-pre-configured-docker-image", 
            "text": "Coming soon...", 
            "title": "3. Use the pre-configured Docker image"
        }, 
        {
            "location": "/server/#configuration", 
            "text": "Conductor server uses a property file based configuration.  The property file is passed to the Main class as a command line argument.  java -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path]  log4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console).", 
            "title": "Configuration"
        }, 
        {
            "location": "/server/#configuration-parameters", 
            "text": "# Database persistence model.  Possible values are memory, redis, and dynomite.\n# If ommitted, the persistence used is memory\n#\n# memory : The data is stored in memory and lost when the server dies.  Useful for testing or demo\n# redis : non-Dynomite based redis instance\n# dynomite : Dynomite cluster.  Use this for HA configuration.\ndb=dynomite\n\n# Dynomite Cluster details.\n# format is host:port:rack separated by semicolon  \nworkflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e\n\n# Dynomite cluster name\nworkflow.dynomite.cluster.name=dyno_cluster_name\n\n# Namespace for the keys stored in Dynomite/Redis \nworkflow.namespace.prefix=conductor\n\n# Namespace prefix for the dyno queues\nworkflow.namespace.queue.prefix=conductor_queues\n\n# No. of threads allocated to dyno-queues (optional)\nqueues.dynomite.threads=10\n\n# Non-quorum port used to connect to local redis.  Used by dyno-queues.\n# When using redis directly, set this to the same port as redis server\n# For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.\nqueues.dynomite.nonQuorum.port=22122\n\n# Transport address to elasticsearch\nworkflow.elasticsearch.url=localhost:9300\n\n# Name of the elasticsearch cluster\nworkflow.elasticsearch.index.name=conductor\n\n# Additional modules (optional)\nconductor.additional.modules=class_extending_com.google.inject.AbstractModule", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/server/#high-availability-configuration", 
            "text": "Conductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs.  The scalability of the server is achieved by scaling the  Dynomite  cluster along with  dyno-queues  which is used for queues.  Clients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack).", 
            "title": "High Availability Configuration"
        }, 
        {
            "location": "/extend/", 
            "text": "Backend\n\n\nConductor provides a pluggable backend.  The current implementation uses Dynomite.\n\n\nThere are 4 interfaces that needs to be implemented for each backend:\n\n\n//Store for workfow and task definitions\ncom.netflix.conductor.dao.MetadataDAO\n\n\n\n\n//Store for workflow executions\ncom.netflix.conductor.dao.ExecutionDAO\n\n\n\n\n//Index for workflow executions\ncom.netflix.conductor.dao.IndexDAO\n\n\n\n\n//Queue provider for tasks\ncom.netflix.conductor.dao.QueueDAO\n\n\n\n\nIt is possible to mix and match different implementation for each of these.\ne.g. SQS for queueing and a relational store for others.\n\n\nSystem Tasks\n\n\nTo create system tasks follow the steps below:\n\n\n\n\nExtend \ncom.netflix.conductor.core.execution.tasks.WorkflowSystemTask\n\n\nInstantiate the new classs as part of the statup (eager singleton)", 
            "title": "Extending Conductor"
        }, 
        {
            "location": "/extend/#backend", 
            "text": "Conductor provides a pluggable backend.  The current implementation uses Dynomite.  There are 4 interfaces that needs to be implemented for each backend:  //Store for workfow and task definitions\ncom.netflix.conductor.dao.MetadataDAO  //Store for workflow executions\ncom.netflix.conductor.dao.ExecutionDAO  //Index for workflow executions\ncom.netflix.conductor.dao.IndexDAO  //Queue provider for tasks\ncom.netflix.conductor.dao.QueueDAO  It is possible to mix and match different implementation for each of these.\ne.g. SQS for queueing and a relational store for others.", 
            "title": "Backend"
        }, 
        {
            "location": "/extend/#system-tasks", 
            "text": "To create system tasks follow the steps below:   Extend  com.netflix.conductor.core.execution.tasks.WorkflowSystemTask  Instantiate the new classs as part of the statup (eager singleton)", 
            "title": "System Tasks"
        }, 
        {
            "location": "/runtime/", 
            "text": "Metadata\n\n\nWorkflow blueprints are managed via \n/metadata\n resource endpoints.\n\n\nInstances\n\n\nEach running workflow instance is identified by a unique instance id.  This id is used for managing the lifecycle of workflow instance.  The following operations are possible:\n\n\n\n\nStart a workflow\n\n\nTerminate\n\n\nRestart\n\n\nPause\n\n\nResume\n\n\nRerun\n\n\nSearch for workflows", 
            "title": "Workflow Management"
        }, 
        {
            "location": "/runtime/#metadata", 
            "text": "Workflow blueprints are managed via  /metadata  resource endpoints.", 
            "title": "Metadata"
        }, 
        {
            "location": "/runtime/#instances", 
            "text": "Each running workflow instance is identified by a unique instance id.  This id is used for managing the lifecycle of workflow instance.  The following operations are possible:   Start a workflow  Terminate  Restart  Pause  Resume  Rerun  Search for workflows", 
            "title": "Instances"
        }, 
        {
            "location": "/metrics/", 
            "text": "Conductor uses \nspectator\n to collect the metrics.\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\nTags\n\n\n\n\n\n\n\n\n\n\nworkflow_server_error\n\n\nRate at which server side error is happening\n\n\nmethodName\n\n\n\n\n\n\nworkflow_failure\n\n\nCounter for failing workflows\n\n\nworkflowName, status\n\n\n\n\n\n\nworkflow_start_error\n\n\nCounter for failing to start a workflow\n\n\nworkflowName\n\n\n\n\n\n\nworkflow_running\n\n\nCounter for no. of running workflows\n\n\nworkflowName, version\n\n\n\n\n\n\ntask_queue_wait\n\n\nTime spent by a task in queue\n\n\ntaskType\n\n\n\n\n\n\ntask_execution\n\n\nTime taken to execute a task\n\n\ntaskType, includeRetries, status\n\n\n\n\n\n\ntask_poll\n\n\nTime taken to poll for a task\n\n\ntaskType\n\n\n\n\n\n\ntask_queue_depth\n\n\nPending tasks queue depth\n\n\ntaskType\n\n\n\n\n\n\ntask_timeout\n\n\nCounter for timed out tasks\n\n\ntaskType", 
            "title": "Server Metrics"
        }, 
        {
            "location": "/metrics/client/", 
            "text": "Conductor uses \nspectator\n to collect the metrics.\n\n\nWhen using the Java client, the following metrics are published:\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\nTags\n\n\n\n\n\n\n\n\n\n\ntask_execution_queue_full\n\n\nCounter to record execution queue has saturated\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_error\n\n\nClient error when polling for a task queue\n\n\ntaskType, includeRetries, status\n\n\n\n\n\n\ntask_execute_error\n\n\nExcution error\n\n\ntaskType\n\n\n\n\n\n\ntask_ack_failed\n\n\nTask ack failed\n\n\ntaskType\n\n\n\n\n\n\ntask_ack_error\n\n\nTask ack has encountered an exception\n\n\ntaskType\n\n\n\n\n\n\ntask_update_error\n\n\nTask status cannot be updated back to server\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_counter\n\n\nIncremented each time polling is done\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_time\n\n\nTime to poll for a batch of tasks\n\n\ntaskType\n\n\n\n\n\n\ntask_execute_time\n\n\nTime to execute a task\n\n\ntaskType\n\n\n\n\n\n\n\n\nMetrics on client side supplements the one collected from server in identifying the network as well as client side issues.", 
            "title": "Worker Metrics"
        }, 
        {
            "location": "/license/", 
            "text": "Copyright 2016 Netflix, Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "License"
        }
    ]
}