{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"Scalable Workflow Orchestration                 Conductor is a platform originally created at Netflix to orchestrate workflows that span across microservices.        Get Started  Open Source                 Apache-2.0 license for commercial and non-commerical use. Freedom to deploy, modify and contribute back.         Modular                 A fully abstracted backend enables you choose your own database persistence layer and queueing service.         Proven                         Enterprise ready, Java Spring based platform that has been battle tested in production systems at Netflix and elsewhere.         Control                 Powerful flow control constructs including Decisions, Dynamic Fork-Joins and Subworkflows. Variables and templates are supported.         Polyglot                 Client libraries in multiple languages allows workers to be implemented in Java, Node JS, Python and C#.         Scalable                 Distributed architecture for both orchestrator and workers scalable from a single workflow to millions of concurrent processes.                 Developer Experience        <ul> <li>Discover and visualize the process flows from the bundled UI</li> <li>Integrated interface to create, refine and validate workflows</li> <li>JSON based workflow definition DSL</li> <li>Full featured API for custom automation</li>          Observability        <ul> <li>Understand, debug and iterate on task and workflow executions.</li> <li>Fine grain operational control over workflows with the ability to pause, resume, restart, retry and terminate</li> </ul> Why Conductor?  Service Orchestration          <p>Workflow definitions are decoupled from task implementations. This allows the creation of process flows in which each individual task can be implemented            by an encapsulated microservice.</p> <p>Designing a workflow orchestrator that is resilient and horizontally scalable is not a simple problem. Conductor was developed as a solution to that problem.</p>  Service Choreography                     Process flows are implicitly defined across multiple service implementations, often with           tight peer-to-peer coupling between services. Multiple event buses and complex           pub/sub models limit observability around process progress and capacity."},{"location":"devguide/bestpractices.html","title":"Best Practices","text":""},{"location":"devguide/bestpractices.html#response-timeout","title":"Response Timeout","text":"<ul> <li>Configure the responseTimeoutSeconds of each task to be &gt; 0.</li> <li>Should be less than or equal to timeoutSeconds.</li> </ul>"},{"location":"devguide/bestpractices.html#payload-sizes","title":"Payload sizes","text":"<ul> <li>Configure your workflows such that conductor is not used as a persistence store.</li> <li>Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result.</li> <li>In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large (&gt; 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.</li> </ul>"},{"location":"devguide/faq.html","title":"Frequently asked Questions","text":""},{"location":"devguide/faq.html#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc","title":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)","text":"<p>After polling for the task update the status of the task to <code>IN_PROGRESS</code> and set the <code>callbackAfterSeconds</code> value to the desired time.  The task will remain in the queue until the specified second before worker polling for it will receive it again.</p> <p>If there is a timeout set for the task, and the <code>callbackAfterSeconds</code> exceeds the timeout value, it will result in task being TIMED_OUT.</p>"},{"location":"devguide/faq.html#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months","title":"How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?","text":"<p>Yes.  As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.</p>"},{"location":"devguide/faq.html#my-workflow-fails-to-start-with-missing-task-error","title":"My workflow fails to start with missing task error","text":"<p>Ensure all the tasks are registered via <code>/metadata/taskdefs</code> APIs.  Add any missing task definition (as reported in the error) and try again.</p>"},{"location":"devguide/faq.html#where-does-my-worker-run-how-does-conductor-run-my-tasks","title":"Where does my worker run?  How does conductor run my tasks?","text":"<p>Conductor does not run the workers.  When a task is scheduled, it is put into the queue maintained by Conductor.  Workers are required to poll for tasks using <code>/tasks/poll</code> API at periodic interval, execute the business logic for the task and report back the results using <code>POST {{ api_prefix }}/tasks</code> API call.  Conductor, however will run system tasks on the Conductor server.</p>"},{"location":"devguide/faq.html#how-can-i-schedule-workflows-to-run-at-a-specific-time","title":"How can I schedule workflows to run at a specific time?","text":"<p>Conductor itself does not provide any scheduling mechanism.  But there is a community project Schedule Conductor Workflows which provides workflow scheduling capability as a pluggable module as well as workflow server. Other way is you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow.  Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing.</p>"},{"location":"devguide/faq.html#how-do-i-setup-dynomite-cluster","title":"How do I setup Dynomite cluster?","text":"<p>Visit Dynomite's Github page to find details on setup and support mechanism.</p>"},{"location":"devguide/faq.html#can-i-use-conductor-with-ruby-go-python","title":"Can I use conductor with Ruby / Go / Python?","text":"<p>Yes.  Workers can be written any language as long as they can poll and update the task results via HTTP endpoints.</p> <p>Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server.</p> <p>Note: Python and Go clients have been contributed by the community.</p>"},{"location":"devguide/faq.html#how-can-i-get-help-with-dynomite","title":"How can I get help with Dynomite?","text":"<p>Visit Dynomite's Github page to find details on setup and support mechanism.</p>"},{"location":"devguide/faq.html#my-workflow-is-running-and-the-task-is-scheduled-but-it-is-not-being-processed","title":"My workflow is running and the task is SCHEDULED but it is not being processed.","text":"<p>Make sure that the worker is actively polling for this task. Navigate to the <code>Task Queues</code> tab on the Conductor UI and select your task name in the search box. Ensure that <code>Last Poll Time</code> for this task is current.</p> <p>In Conductor 3.x, <code>conductor.redis.availabilityZone</code> defaults to <code>us-east-1c</code>.  Ensure that this matches where your workers are, and that it also matches<code>conductor.redis.hosts</code>.</p>"},{"location":"devguide/faq.html#how-do-i-configure-a-notification-when-my-workflow-completes-or-fails","title":"How do I configure a notification when my workflow completes or fails?","text":"<p>When a workflow fails, you can configure a \"failure workflow\" to run using the<code>failureWorkflow</code> parameter. By default, three parameters are passed:</p> <ul> <li>reason</li> <li>workflowId: use this to pull the details of the failed workflow.</li> <li>failureStatus</li> </ul> <p>You can also use the Workflow Status Listener: </p> <ul> <li>Set the workflowStatusListenerEnabled field in your workflow definition to true which enables notifications.</li> <li>Add a custom implementation of the Workflow Status Listener. Refer this.</li> <li>This notification can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here.</li> </ul> <p>Refer to this documentation to extend conductor to send out events/notifications upon workflow completion/failure. </p>"},{"location":"devguide/faq.html#i-want-my-worker-to-stop-polling-and-executing-tasks-when-the-process-is-being-terminated-java-client","title":"I want my worker to stop polling and executing tasks when the process is being terminated. (Java client)","text":"<p>In a <code>PreDestroy</code> block within your application, call the <code>shutdown()</code> method on the <code>TaskRunnerConfigurer</code> instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated.</p>"},{"location":"devguide/faq.html#can-i-exit-early-from-a-task-without-executing-the-configured-automatic-retries-in-the-task-definition","title":"Can I exit early from a task without executing the configured automatic retries in the task definition?","text":"<p>Set the status to <code>FAILED_WITH_TERMINAL_ERROR</code> in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.</p>"},{"location":"devguide/architecture/index.html","title":"Architecture Overview","text":"<p>The API and storage layers are pluggable and provide ability to work with different backends and queue service providers.</p>"},{"location":"devguide/architecture/index.html#runtime-model","title":"Runtime Model","text":"<p>Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.</p> <p></p>"},{"location":"devguide/architecture/index.html#notes","title":"Notes","text":"<ul> <li>Workers are remote systems that communicate over HTTP with the conductor servers.</li> <li>Task Queues are used to schedule tasks for workers.  We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism.</li> <li>conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend.</li> <li>See section under extending backend for implementing support for different databases for storage and indexing.</li> </ul>"},{"location":"devguide/architecture/directed-acyclic-graph.html","title":"Directed Acyclic Graph (DAG)","text":""},{"location":"devguide/architecture/directed-acyclic-graph.html#what-is-a-directed-acyclic-graph-dag","title":"What is a Directed Acyclic Graph (DAG)?","text":"<p>Conductor workflows are directed acyclic graphs (DAGs). But, what exactly is a DAG?</p> <p>To understand a DAG, we'll walk through each term (but not in order):</p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#graph","title":"Graph","text":"<p>A graph is \"a collection of vertices (or point) and edges (or lines) that indicate connections between the vertices.\"  </p> <p>By this definition, this is a graph - just not exactly correct in the context of DAGs:</p> <p></p> <p>But in the context of workflows, we're thinking of a graph more like this:</p> <p></p> <p>Imagine each vertex as a microservice, and the lines are how the microservices are connected together. However, this graph is not a directed graph - as there is no direction given to each connection.</p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#directed","title":"Directed","text":"<p>A directed graph means that there is a direction to each connection. For example, this graph is directed:</p> <p></p> <p>Each arrow has a direction, Point \"N\" can proceed directly to \"B\", but \"B\" cannot proceed to \"N\" in the opposite direction.  </p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#acyclic","title":"Acyclic","text":"<p>Acyclic means without circular or cyclic paths.  In the directed example above,  A -&gt; B -&gt; D -&gt; A is a cyclic loop.  </p> <p>So a Directed Acyclic Graph is a set of vertices where the connections are directed without any looping.  DAG charts can only \"move forward\" and cannot redo a step (or series of steps.)</p> <p>Since a Conductor workflow is a series of vertices that can connect in only a specific direction and cannot loop, a Conductor workflow is thus a directed acyclic graph:</p> <p></p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#can-a-workflow-have-loops-and-still-be-a-dag","title":"Can a workflow have loops and still be a DAG?","text":"<p>Yes. For example, Conductor workflows have Do-While loops:</p> <p></p> <p>This is still a DAG, because the loop is just shorthand for running the tasks inside the loop over and over again.  For example, if the 2nd loop in the above image is run 3 times, the workflow path will be:</p> <ol> <li>zero_offset_fix_1</li> <li>post_to_orbit_ref_1</li> <li>zero_offset_fix_2</li> <li>post_to_orbit_ref_2</li> <li>zero_offset_fix_3</li> <li>post_to_orbit_ref_3</li> </ol> <p>The path is directed forward, and the loop just makes it easier to define the workflow.</p>"},{"location":"devguide/architecture/tasklifecycle.html","title":"Task Lifecycle","text":""},{"location":"devguide/architecture/tasklifecycle.html#task-state-transitions","title":"Task state transitions","text":"<p>The figure below depicts the state transitions that a task can go through within a workflow execution.</p> <p></p>"},{"location":"devguide/architecture/tasklifecycle.html#retries-and-failure-scenarios","title":"Retries and Failure Scenarios","text":""},{"location":"devguide/architecture/tasklifecycle.html#task-failure-and-retries","title":"Task failure and retries","text":"<p>Retries for failed task executions of each task can be configured independently. <code>retryCount</code>, <code>retryDelaySeconds</code> and <code>retryLogic</code> can be used to configure the retry mechanism.</p> <p></p> <ol> <li>Worker (W1) polls for task T1 from the Conductor server and receives the task.</li> <li>Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds.</li> <li>The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds.</li> </ol>"},{"location":"devguide/architecture/tasklifecycle.html#poll-timeout-seconds","title":"Poll Timeout Seconds","text":"<p>Poll timeout is the maximum amount of time by which a worker needs to poll a task, else the task will be marked as <code>TIMED_OUT</code>.</p> <p></p> <p>In the figure above, task T1 does not get polled by the worker within 60 seconds, so Conductor marks it as <code>TIMED_OUT</code>.</p>"},{"location":"devguide/architecture/tasklifecycle.html#timeout-seconds","title":"Timeout seconds","text":"<p>Timeout is the maximum amount of time that the task must reach a terminal state in, else it will be marked as <code>TIMED_OUT</code>.</p> <p></p> <p>0 seconds -&gt; Worker polls for task T1 from the Conductor server and receives the task. T1 is put into <code>IN_PROGRESS</code> status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to <code>IN_PROGRESS</code> status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds.  </p> <p>9,18 seconds -&gt; Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds.</p> <p>27 seconds -&gt; Worker polls and receives task T1 from the server and is now able to process this task.</p> <p>30 seconds (T1 timeout) -&gt; Server marks T1 as <code>TIMED_OUT</code> because it is not in a terminal state after first being moved to <code>IN_PROGRESS</code> status. Server schedules a new task based on the retry count.</p> <p>32 seconds -&gt; Worker completes processing of T1 and updates the server with <code>COMPLETED</code> status. Server will ignore this update since T1 has already been moved to a terminal status (<code>TIMED_OUT</code>).</p>"},{"location":"devguide/architecture/tasklifecycle.html#response-timeout-seconds","title":"Response timeout seconds","text":"<p>Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT.</p> <p></p> <p>0 seconds -&gt; Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into <code>IN_PROGRESS</code> status by the server.</p> <p>Worker starts processing the task but the worker instance dies during this execution.</p> <p>20 seconds (T1 responseTimeout) -&gt; Server marks T1 as <code>TIMED_OUT</code> since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration.</p> <p>25 seconds -&gt; The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.</p>"},{"location":"devguide/architecture/technicaldetails.html","title":"Technical Details","text":""},{"location":"devguide/architecture/technicaldetails.html#grpc-framework","title":"gRPC Framework","text":"<p>As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving  the Jetty server into a separate module and the conformance to Guice guidelines and best practices.  This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API. The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects.</p>"},{"location":"devguide/architecture/technicaldetails.html#cassandra-persistence","title":"Cassandra Persistence","text":"<p>The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \"workflows\" table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \"task_lookup\" table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId.  All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO, PollDataDAO and RateLimitingDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from.</p>"},{"location":"devguide/architecture/technicaldetails.html#external-payload-storage","title":"External Payload Storage","text":"<p>The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system. The externalization of payloads is enforced using two kinds of barriers. Soft barriers are used when the  payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed. The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location.</p>"},{"location":"devguide/architecture/technicaldetails.html#dynamic-workflow-executions","title":"Dynamic Workflow Executions","text":"<p>In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed:</p> <ul> <li>Registration of the 10 task definitions if they don't exist (assuming workflow task type SIMPLE for simplicity)</li> <li>Registration of the workflow definition</li> <li>Each time a definition needs to be retrieved, a call to the metadata store needed to be performed</li> <li>In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions</li> </ul> <p>To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest.</p> <p>Following benefits are seen as a result of these changes:</p> <ul> <li>Grants immutability of the definition stored within the execution data against modifications to the metadata store</li> <li>Better testability of workflows with faster experimental changes to definitions</li> <li>Reduced stress on the datastore due to prefetching the metadata only once at the start</li> </ul>"},{"location":"devguide/architecture/technicaldetails.html#decoupling-elasticsearch-from-persistence","title":"Decoupling Elasticsearch from Persistence","text":"<p>In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa. We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution.</p>"},{"location":"devguide/architecture/technicaldetails.html#elasticsearch-56-support","title":"Elasticsearch 5/6 Support","text":"<p>Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI.  In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 by following the simple instructions here.</p>"},{"location":"devguide/architecture/technicaldetails.html#maintaining-workflow-consistency-with-distributed-locking-and-fencing-tokens","title":"Maintaining workflow consistency with distributed locking and fencing tokens","text":""},{"location":"devguide/architecture/technicaldetails.html#problem","title":"Problem","text":"<p>Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow.</p> <p>In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent.</p>"},{"location":"devguide/architecture/technicaldetails.html#a-two-part-solution-to-maintain-workflow-consistency","title":"A two-part solution to maintain Workflow Consistency","text":"<p>Preventing concurrent decides with distributed locking: The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided.</p> <p>Preventing stale data updates with fencing tokens: While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct.</p> <p>Netflix uses Cassandra. Considering the tradeoffs of Cassandra\u2019s Lightweight Transactions (LWT) and the probability of this stale updates happening, and our testing results, we\u2019ve decided to first only rollout distributed locking with Zookeeper. We'll monitor our system and add C LWT if needed.</p>"},{"location":"devguide/architecture/technicaldetails.html#setting-up-desired-level-of-consistency","title":"Setting up desired level of consistency","text":"<p>Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations.</p>"},{"location":"devguide/architecture/technicaldetails.html#alternative-solution-to-distributed-decide-evaluation","title":"Alternative solution to distributed \"decide\" evaluation","text":"<p>As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service.</p> <p>Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out.</p> <p>In a single node deployment (single dynomite rack and single conductor server) this shouldn't be a problem. But when running multiple replicated dynomite racks and a conductor server on top of each rack, this might trigger the race condition described in previous section.</p> <p>Dynomite rack is a single or multiple instance dynomite setup that holds all the data.</p> <p>More on dynomite HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404)</p> <p>In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2.</p>"},{"location":"devguide/architecture/technicaldetails.html#more-on-dyno-queues","title":"More on dyno queues","text":"<p>Dyno queues are the default queuing mechanism of conductor.</p> <p>Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue)   * This queue is used by SweeperService</p> <p>Each conductor server instance gets its own set of queues. Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g.</p> <pre><code>conductor_queues.test.QUEUE._deciderQueue.c\nconductor_queues.test.QUEUE._deciderQueue.d\nconductor_queues.test.QUEUE.HTTP.c\nconductor_queues.test.QUEUE.HTTP.d\nconductor_queues.test.QUEUE.LAMBDA.c\nconductor_queues.test.QUEUE.LAMBDA.d\n</code></pre> <p>The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively.</p> <p>The shard names are extracted from dynomite rack name such as us-east-1c that is set in \"LOCAL_RACK\" or \"EC2_AVAILABILTY_ZONE\"</p> <p>Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows:</p> <pre><code>Workflow execution    -&gt; conductor_queues.test.QUEUE._deciderQueue.c\nHTTP taks execution   -&gt; conductor_queues.test.QUEUE.HTTP.d\nLAMBDA task execution -&gt; conductor_queues.test.QUEUE.LAMBDA.c\n</code></pre> <p>Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task.</p> <p>This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next.</p> <p>The round-robin strategy responsible for work distribution is defined here</p>"},{"location":"devguide/architecture/technicaldetails.html#back-to-alternative-solution","title":"Back to alternative solution","text":"<p>The alternative solution here is Switching round-robin queue allocation for a local-only strategy. Meaning that a workflow and its task executions are queued only for the conductor instance which started the workflow.</p> <p>This completely avoids the race condition for the price of removing task execution distribution.</p> <p>Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances.</p> <p>The downside here is that the workload is not distributed across all conductor servers. Which might be an advantage in active-standby deployments.</p> <p>Considering other downsides ...</p> <p>Considering a situation where a conductor instance goes down: * With local-only strategy, the workflow executions from failed conductor instance will not progress until:   * The conductor instance is restarted or   * The executions are manually terminated and restarted from a different node * With round-robin strategy, there is a chance the tasks will be rescheduled on a different conductor node   * This is nondeterministic though</p> <p>Enabling local only queue allocation strategy for dyno queues:</p> <p>Just enable following setting the config.properties:</p> <pre><code>workflow.dyno.queue.sharding.strategy=localOnly\n</code></pre> <p>The default is roundRobin</p>"},{"location":"devguide/concepts/index.html","title":"Basic Concepts","text":"<p>Conductor allows you to build a complex application using simple and granular tasks that do not need to be aware of or keep track of the state of your application's execution flow. Conductor keeps track of the state, calls tasks in the right order (sequentially or in parallel, as defined by you), retry calls if needed, handle failure scenarios gracefully, and outputs the final result.</p> <p></p> <p>Leveraging workflows in Conductor enables developers to truly focus on their core mission - building their application code in the languages of their choice. Conductor does the heavy lifting associated with ensuring high reliability, transactional consistency, and long durability of their workflows. Simply put, wherever your application's component lives and whichever languages they were written in, you can build a workflow in Conductor to orchestrate their execution in a reliable &amp; scalable manner.</p> <p>Workflows and Tasks are the two key concepts that underlie the Conductor system. </p>"},{"location":"devguide/concepts/tasks.html","title":"Tasks","text":"<p>Tasks are the building blocks of Conductor Workflows. There must be at least one task configured in each Workflow Definition. A typical Conductor workflow defines a lists of tasks that are executed until the completion or termination of the workflow.</p> <p>Tasks can be categorized into three types: </p>"},{"location":"devguide/concepts/tasks.html#types-of-tasks","title":"Types of Tasks","text":""},{"location":"devguide/concepts/tasks.html#system-tasks","title":"System Tasks","text":"<p>System Tasks are built-in tasks that are general purpose and re-usable. They are executed within the JVM of the Conductor server and managed by Conductor for execution and scalability. Such tasks allow you to get started without having to write custom workers. </p>"},{"location":"devguide/concepts/tasks.html#simple-tasks","title":"Simple Tasks","text":"<p>Simple Tasks or Worker Tasks are implemented by your application and run in a separate environment from Conductor. These tasks talk to the Conductor server via REST/gRPC to poll for tasks and update its status after execution.</p>"},{"location":"devguide/concepts/tasks.html#operators","title":"Operators","text":"<p>Operators are built-in primitives in Conductor that allow you control the flow of tasks in your workflow. Operators are similar to programming constructs such as <code>for</code> loops, <code>switch</code> blocks, etc.</p>"},{"location":"devguide/concepts/tasks.html#task-configuration","title":"Task Configuration","text":"<p>Task Configurations appear within the <code>tasks</code> array property of the Workflow Definition. This array is the blueprint that describes how a workflow will process an input payload by passing it through successive tasks.</p> <ul> <li>For all tasks, the configuration will specifiy what input parameters the task takes. </li> <li>For SIMPLE (worker based) tasks, the configuration will contain a reference to a registered worker <code>taskName</code>. </li> <li>For System Tasks and Operators, the task configuration will contain important parameters that control the behavior of the task. For example, the task configuration of an HTTP task will specify an endpoint URL and the templatized payload that it will be called with when the task executes.</li> </ul>"},{"location":"devguide/concepts/tasks.html#task-definition","title":"Task Definition","text":"<p>Not to be confused with Task Configurations, Task Definitions help define default task level parameters like inputs and outputs, timeouts, retries etc. for SIMPLE (i.e. worker implemented) tasks.</p> <ul> <li>All simple tasks need to be registered before they can be used by active workflows.</li> <li>Task definitions can be registered via the UI, or through the API.</li> <li>A registered task definition can be referenced from within different workflows.</li> </ul>"},{"location":"devguide/concepts/tasks.html#task-execution","title":"Task Execution","text":"<p>Each time a workload is passed into a configured task, a Task Execution object is created. This object has a unique ID and represents the result of the operation. This includes the status (i.e. whether the task was completed successfully), and any input, output and variables associated with the task. </p>"},{"location":"devguide/concepts/why.html","title":"Why Conductor?","text":"<p>Conductor was built to help orchestrate microservices based process flows.</p>"},{"location":"devguide/concepts/why.html#features","title":"Features","text":"<ul> <li>A distributed server ecosystem, which stores workflow state information efficiently.</li> <li>Allow creation of process / business flows in which each individual task can be implemented by the same / different microservices.</li> <li>A DAG (Directed Acyclic Graph) based workflow definition.</li> <li>Workflow definitions are decoupled from the service implementations.</li> <li>Provide visibility and traceability into these process flows.</li> <li>Simple interface to connect workers, which execute the tasks in workflows.</li> <li>Workers are language agnostic, allowing each microservice to be written in the language most suited for the service.</li> <li>Full operational control over workflows with the ability to pause, resume, restart, retry and terminate.</li> <li>Allow greater reuse of existing microservices providing an easier path for onboarding.</li> <li>User interface to visualize, replay and search the process flows.</li> <li>Ability to scale to millions of concurrently running process flows.</li> <li>Backed by a queuing service abstracted from the clients.</li> <li>Be able to operate on HTTP or other transports e.g. gRPC.</li> <li>Event handlers to control workflows via external actions.</li> <li>Client implementations in Java, Python and other languages.</li> <li>Various configurable properties with sensible defaults to fine tune workflow and task executions like rate limiting, concurrent execution limits etc.</li> </ul>"},{"location":"devguide/concepts/why.html#why-not-peer-to-peer-choreography","title":"Why not peer to peer choreography?","text":"<p>With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:</p> <ul> <li>Process flows are \"embedded\" within the code of multiple application.</li> <li>Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.</li> <li>Almost no way to systematically answer \"How much are we done with process X\"?</li> </ul>"},{"location":"devguide/concepts/workers.html","title":"Workers","text":"<p>A worker is responsible for executing a task. Workers can be implemented in any language, and Conductor provides a polyglot set of worker frameworks that provide features such as polling threads, metrics and server communication that makes creating workers easy.</p> <p>Each worker embodies the Microservice design pattern and follows certain basic principles:</p> <ol> <li>Workers are stateless and do not implement a workflow specific logic.  </li> <li>Each worker executes a very specific task and produces well defined output given specific inputs.</li> <li>Workers are meant to be idempotent (or should handle cases where the task that partially executed gets rescheduled due to timeouts etc.)</li> <li>Workers do not implement the logic to handle retries etc, that is taken care by the Conductor server.</li> </ol> <p>Conductor maintains a registry of worker tasks.  A task MUST be registered before being used in a workflow. This can be done by creating and saving a Task Definition.</p>"},{"location":"devguide/concepts/workflows.html","title":"Workflows","text":"<p>We will talk about two distinct topics, defining a workflow and executing a workflow.</p>"},{"location":"devguide/concepts/workflows.html#workflow-definition","title":"Workflow Definition","text":"<p>The Workflow Definition is the Conductor primitive that encompasses the flow of your business logic. It contains all the information necessary to describe the behavior of a workflow.</p> <p>A Workflow Definition contains a collection of Task Configurations. This is the blueprint which specifies the order of execution of tasks within a workflow. This blueprint also specifies how data/state is passed from one task to another (using task input/output parameters).</p> <p>Additionally, the Workflow Definition contains metadata regulating the runtime behavior workflow, such what input and output parameters are expected for the entire workflow, and the workflow's the timeout and retry settings.</p>"},{"location":"devguide/concepts/workflows.html#workflow-execution","title":"Workflow Execution","text":"<p>If Workflow Definitions are like OOP classes, then Workflows Executions are like object instances. Each time a Workflow Definition is invoked with a given input, a new Workflow Execution with a unique ID is created. Definitions to Executions have a 1:N relationship.</p>"},{"location":"devguide/how-tos/Monitoring/Conductor-LogLevel.html","title":"Conductor Log Level","text":"<p>Conductor is based on Spring Boot, so the log levels are set via Spring Boot properties:</p> <p>From the Spring Boot Docs:</p> <p>All the supported logging systems can have the logger levels set in the Spring Environment (for example, in application.properties) by using <code>logging.level.&lt;logger-name&gt;=&lt;level&gt;</code> where level is one of TRACE, DEBUG, INFO, WARN, ERROR, FATAL, or OFF. The <code>root</code> logger can be configured by using logging.level.root.</p> <p>The following example shows potential logging settings in <code>application.properties</code>:</p> <pre><code>logging.level.root=warn\nlogging.level.org.springframework.web=debug\nlogging.level.org.hibernate=error\n</code></pre> <p>It\u2019s also possible to set logging levels using environment variables. For example, <code>LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_WEB=DEBUG</code> will set <code>org.springframework.web</code> to <code>DEBUG</code>.</p>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html","title":"Creating Task Definitions","text":"<p>Tasks can be created using the tasks metadata API</p> <p><code>POST {{ api_prefix }}/metadata/taskdefs</code></p> <p>This API takes an array of new task definitions.</p>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#examples","title":"Examples","text":""},{"location":"devguide/how-tos/Tasks/creating-tasks.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl '{{ server_host }}{{ api_prefix }}/metadata/taskdefs' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '[{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}]'\n</code></pre>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"{{ server_host }}{{ api_prefix }}/metadata/taskdefs\", {\n    \"headers\": {\n        \"accept\": \"*/*\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"[{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}]\",\n    \"method\": \"POST\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#best-practices","title":"Best Practices","text":"<ol> <li>You can update a set of tasks together in this API</li> <li>Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details' </li> <li>You can also use the Conductor Swagger UI to update the tasks</li> </ol>"},{"location":"devguide/how-tos/Tasks/dynamic-vs-switch-tasks.html","title":"Dynamic vs Switch Tasks","text":"<p>Dynamic Tasks are useful in situations when need to run a task of which the task type is determined at runtime instead of during the configuration. It is similar to the <code>SWITCH</code> use case but with <code>DYNAMIC</code> we won't need to preconfigure all case options in the workflow definition itself. Instead, we can mark the task as <code>DYNAMIC</code> and determine which underlying task does it run during the workflow execution itself.</p> <ul> <li>Use DYNAMIC task as a replacement for SWITCH if you have too many case options</li> <li>DYNAMIC task is an option when you want to programmatically determine the next task to run instead of using expressions</li> <li>DYNAMIC task simplifies the workflow execution UI view which will now only show the selected task</li> <li>SWITCH task visualization is helpful as a documentation - showing you all options that the workflow could have    taken</li> <li>SWITCH task comes with a default task option which can be useful in some use cases</li> </ul> <p>Learn more about</p> <ul> <li>Dynamic Tasks</li> <li>Switch Tasks</li> </ul>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html","title":"Extending System Tasks","text":"<p>System tasks allow Conductor to run simple tasks on the server - removing the need to build (and deploy) workers for basic tasks.  This allows for automating more mundane tasks without building specific microservices for them.</p> <p>However, sometimes it might be necessary to add additional parameters to a System Task to gain the behavior that is desired.</p>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html#example-http-task","title":"Example HTTP Task","text":"<pre><code>{\n  \"name\": \"get_weather_90210\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"get_weather_90210\",\n      \"taskReferenceName\": \"get_weather_90210\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"https://weatherdbi.herokuapp.com/data/weather/90210\",\n          \"method\": \"GET\",\n          \"connectionTimeOut\": 1300,\n          \"readTimeOut\": 1300\n        }\n      },\n      \"type\": \"HTTP\",\n      \"decisionCases\": {},\n      \"defaultCase\": [],\n      \"forkTasks\": [],\n      \"startDelay\": 0,\n      \"joinOn\": [],\n      \"optional\": false,\n      \"defaultExclusiveJoinTask\": [],\n      \"asyncComplete\": false,\n      \"loopOver\": []\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"data\": \"${get_weather_ref.output.response.body.currentConditions.comment}\"\n  },\n  \"schemaVersion\": 2,\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0,\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>This very simple workflow has a single HTTP Task inside.  No parameters need to be passed, and when run, the HTTP task will return the weather in Beverly Hills, CA (Zip code = 90210).</p> <p>This API has a very slow response time. In the HTTP task, the connection is set to time out after 1300ms, which is too short for this API, resulting in a timeout.  This API will work if we allowed for a longer timeout, but in order to demonstrate adding retries to the HTTP Task, we will artificially force the API call to fail.</p> <p>When this workflow is run - it fails, as expected.</p> <p>Now, sometimes an API call might fail due to an issue on the remote server, and retrying the call will result in a response.  With many Conductor tasks,  <code>retryCount</code>, <code>retryDelaySeconds</code> and <code>retryLogic</code> fields can be applied to retry the worker (with the desired parameters).</p> <p>By default, the HTTP Task does not have <code>retryCount</code>, <code>retryDelaySeconds</code> or <code>retryLogic</code> built in.  Attempting to add these parameters to a HTTP Task results in an error.</p>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html#the-solution","title":"The Solution","text":"<p>We can create a task with the same name with the desired parameters.  Defining the following task (note that the <code>name</code> is identical to the one in the workflow):</p> <pre><code>{\n\n  \"createdBy\": \"\",\n  \"name\": \"get_weather_90210\",\n  \"description\": \"editing HTTP task\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 5,\n  \"inputKeys\": [],\n  \"outputKeys\": [],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 5,\n  \"responseTimeoutSeconds\": 5,\n  \"inputTemplate\": {},\n  \"rateLimitPerFrequency\": 0,\n  \"rateLimitFrequencyInSeconds\": 1\n}\n</code></pre> <p>We've added the three parameters: <code>retryCount: 3, retryDelaySeconds: 5, retryLogic: FIXED</code></p> <p>The <code>get_weather_90210</code> task will now run 4 times (it will fail once, and then retry 3 times), with a <code>FIXED</code> 5 second delay between attempts.</p> <p>Re-running the task (and looking at the timeline view) shows that this is what occurs.  There are 4 attempts, with a 5 second delay between them.</p> <p>If we change the <code>retryLogic</code> to EXPONENTIAL_BACKOFF, the delay between attempts grows exponentially:</p> <ol> <li>5*2^0 = 5 seconds</li> <li>5*2^1 = 10 seconds</li> <li>5*2^2 = 20 seconds</li> </ol>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html","title":"Monitoring Task Queues","text":"<p>Conductor offers an API and UI interface to monitor the task queues. This is useful to see details of the number of workers polling and monitoring the queue backlog.</p>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html#using-the-ui","title":"Using the UI","text":"<pre><code>&lt;your UI server URL&gt;/taskQueue\n</code></pre> <p>Access this screen via - Home &gt; Task Queues</p> <p>On this screen you can select and view the details of the task queue. The following information is shown:</p> <ol> <li>Queue Size - The number of tasks waiting to be executed</li> <li>Workers - The count and list of works and their instance reference who are polling for work for this task</li> </ol>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html#using-apis","title":"Using APIs","text":"<p>To see the size of the task queue via API:</p> <pre><code>curl '{{ server_host }}{{ api_prefix }}/tasks/queue/sizes?taskType=&lt;TASK_NAME&gt;' \\\n  -H 'accept: */*' \n</code></pre> <p>To see the worker poll information of the task queue via API:</p> <pre><code>curl '{{ server_host }}{{ api_prefix }}/tasks/queue/polldata?taskType=&lt;TASK_NAME&gt;' \\\n  -H 'accept: */*'\n</code></pre> <p>Note</p> <p>Replace <code>&lt;TASK_NAME&gt;</code> with your task name</p>"},{"location":"devguide/how-tos/Tasks/reusing-tasks.html","title":"Reusing Tasks","text":"<p>A powerful feature of Conductor is that it supports and enables re-usability out of the box. Task workers typically perform a unit of work and is usually a part of a larger workflow. Such workers are often re-usable in multiple workflows. Once a task is defined, you can use it across as any workflow.</p> <p>When re-using tasks, it's important to think of situations that a multi-tenant system faces. All the work assigned to this worker by default goes to the same task scheduling queue. This could result in your worker not being polled quickly if there is a noisy neighbour in the ecosystem. One way you can tackle this situation is by re-using the worker code, but having different task names registered for different use cases. And for each task name, you can run an appropriate number of workers based on expected load.</p>"},{"location":"devguide/how-tos/Tasks/task-configurations.html","title":"Task Configurations","text":"<p>Refer to Task Definitions for details on how to configure task definitions</p>"},{"location":"devguide/how-tos/Tasks/task-configurations.html#example","title":"Example","text":"<p>Here is a task template payload with commonly used fields:</p> <pre><code>{\n  \"createdBy\": \"user\",\n  \"name\": \"sample_task_name_1\",\n  \"description\": \"This is a sample task for demo\",\n  \"responseTimeoutSeconds\": 10,\n  \"timeoutSeconds\": 30,\n  \"inputKeys\": [],\n  \"outputKeys\": [],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryCount\": 3,\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 5,\n  \"inputTemplate\": {},\n  \"rateLimitPerFrequency\": 0,\n  \"rateLimitFrequencyInSeconds\": 1\n}\n</code></pre>"},{"location":"devguide/how-tos/Tasks/task-configurations.html#best-practices","title":"Best Practices","text":"<ol> <li>Refer to Task Timeouts for additional information on how the various timeout settings work</li> <li>Refer to Monitoring Task Queues on how to monitor task queues</li> </ol>"},{"location":"devguide/how-tos/Tasks/task-inputs.html","title":"Task Inputs","text":"<p>Task inputs can be provided in multiple ways. This is configured in the workflow definition when a task is participating in the workflow.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#inputs-referred-from-workflow-inputs","title":"Inputs referred from Workflow inputs","text":"<p>When we start a workflow, we can provide inputs to the workflow in a json format. For example:</p> <pre><code>{\n  \"worfklowInputNumberExample\": 1,\n  \"worfklowInputTextExample\": \"SAMPLE\",\n  \"worfklowInputJsonExample\": {\n    \"nestedKey\": \"nestedValue\"\n  }\n}\n</code></pre> <p>These values can be referred as inputs into your task using the following expression:</p> <pre><code>{\n  \"taskInput1Key\": \"${workflow.input.worfklowInputNumberExample}\",\n  \"taskInput2Key\": \"${workflow.input.worfklowInputJsonExample.nestedKey}\"\n}\n</code></pre> <p>In this example, the tasks will receive the following inputs after they are evaluated: <pre><code>{\n  \"taskInput1Key\": 1,\n  \"taskInput2Key\": \"nestedValue\"\n}\n</code></pre></p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#inputs-referred-from-other-task-outputs","title":"Inputs referred from other Task outputs","text":"<p>Similar to how we can refer to workflow inputs, we can also refer to an output field that was generated by a task that executed before.</p> <p>Let's assume a task with the task reference name <code>previousTaskReference</code> executed and produced the following output:</p> <pre><code>{\n  \"taskOutputKey1\": \"outputValue\",\n  \"taskOutputKey2\": {\n    \"nestedKey1\": \"outputValue-1\"\n  }\n}\n</code></pre> <p>We can refer to these as the new task's input by using the following expression:</p> <pre><code>{\n  \"taskInput1Key\": \"${previousTaskReference.output.taskOutputKey1}\",\n  \"taskInput2Key\": \"${previousTaskReference.output.taskOutputKey2.nestedKey1}\"\n}\n</code></pre> <p>The expression format is based on Json Path and you can construct complex input params based on the syntax.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#hard-coded-inputs","title":"Hard coded inputs","text":"<p>Task inputs can also be hard coded in the workflow definitions. This is useful when you have a re-usable task which has configurable options that can be applied in different workflow contexts.</p> <pre><code>{\n  \"taskInput1\": \"OPTION_A\",\n  \"taskInput2\": 100\n}\n</code></pre>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html","title":"Task Timeouts","text":"<p>Tasks can be configured to handle various scenarios of timeouts. Here are some scenarios and the relevance configuration fields.</p> Scenario Configuration A task worker picked up the task, but fails to respond back with an update <code>responseTimeoutSeconds</code> A task worker picked up the task and updates progress, but fails to complete within an expected timeframe <code>timeoutSeconds</code> A task is stuck in a retry loop with repeated failures beyond an expected timeframe <code>timeoutSeconds</code> Task doesn't get picked by any workers for a specific amount of time <code>pollTimeoutSeconds</code> Task isn't completed within a specified amount of time despite being picked up by task workers <code>timeoutSeconds</code> <p><code>timeoutSeconds</code> should always be greater than <code>responseTimeoutSeconds</code></p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#timeout-seconds","title":"Timeout Seconds","text":"<pre><code>\"timeoutSeconds\" : 30\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this task to complete successfully up until this number of seconds from when the task is first polled. We can use this to fail a workflow when a task breaches the overall SLA for completion.</p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#response-timeout-seconds","title":"Response Timeout Seconds","text":"<pre><code>\"responseTimeoutSeconds\" : 10\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this number of seconds from when the task is polled before the worker updates back with a status. The worker can keep the task in <code>IN_PROGRESS</code> state if it requires more time to complete.</p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#poll-timeout-seconds","title":"Poll Timeout Seconds","text":"<pre><code>\"pollTimeoutSeconds\" : 10\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this number of seconds for the task to be picked up by a task worker. Useful when you want to detect a backlogged task queue with not enough workers.</p>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html","title":"Updating Task Definitions","text":"<p>Updates to the task definitions can be made using the following API</p> <pre><code>PUT {{ api_prefix }}/metadata/taskdefs\n</code></pre> <p>This API takes a single task definition and updates itself. </p>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#examples","title":"Examples","text":""},{"location":"devguide/how-tos/Tasks/updating-tasks.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl '{{ server_host }}{{ api_prefix }}/metadata/taskdefs' \\\n  -X 'PUT' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}'\n</code></pre>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"{{ server_host }}{{ api_prefix }}/metadata/taskdefs\", {\n    \"headers\": {\n        \"accept\": \"*/*\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}\",\n    \"method\": \"PUT\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#best-practices","title":"Best Practices","text":"<ol> <li>You can also use the Conductor Swagger UI to update the tasks </li> <li>Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details'</li> </ol>"},{"location":"devguide/how-tos/Workers/build-a-golang-task-worker.html","title":"Build a Go Task Worker","text":"<p>See conductor-sdk/conductor-go</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html","title":"Build a Java Task Worker","text":"<p>This guide provides introduction to building Task Workers in Java.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#dependencies","title":"Dependencies","text":"<p>Conductor provides Java client libraries, which we will use to build a simple task worker.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#maven-dependency","title":"Maven Dependency","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.netflix.conductor&lt;/groupId&gt;\n    &lt;artifactId&gt;conductor-client&lt;/artifactId&gt;\n    &lt;version&gt;3.13.2&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.netflix.conductor&lt;/groupId&gt;\n    &lt;artifactId&gt;conductor-common&lt;/artifactId&gt;\n    &lt;version&gt;3.13.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#gradle","title":"Gradle","text":"<pre><code>implementation group: 'com.netflix.conductor', name: 'conductor-client', version: '3.13.2'\nimplementation group: 'com.netflix.conductor', name: 'conductor-common', version: '3.13.2'\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#implementing-a-task-worker","title":"Implementing a Task Worker","text":"<p>To create a worker, implement the <code>Worker</code> interface.</p> <pre><code>public class SampleWorker implements Worker {\n\n    private final String taskDefName;\n\n    public SampleWorker(String taskDefName) {\n        this.taskDefName = taskDefName;\n    }\n\n    @Override\n    public String getTaskDefName() {\n        return taskDefName;\n    }\n\n    @Override\n    public TaskResult execute(Task task) {\n        TaskResult result = new TaskResult(task);\n        result.setStatus(Status.COMPLETED);\n\n        //Register the output of the task\n        result.getOutputData().put(\"outputKey1\", \"value\");\n        result.getOutputData().put(\"oddEven\", 1);\n        result.getOutputData().put(\"mod\", 4);\n\n        return result;\n    }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#implementing-workers-logic","title":"Implementing worker's logic","text":"<p>Worker's core implementation logic goes in the <code>execute</code> method. Upon completion, set the <code>TaskResult</code> with status as one of the following:</p> <ol> <li>COMPLETED: If the task has completed successfully.</li> <li>FAILED: If there are failures - business or system failures. Based on the task's configuration, when a task fails, it may be retried.</li> </ol> <p>The <code>getTaskDefName()</code> method returns the name of the task for which this worker provides the execution logic.</p> <p>See SampleWorker.java for the complete example.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#configuring-polling-using-taskrunnerconfigurer","title":"Configuring polling using TaskRunnerConfigurer","text":"<p>The <code>TaskRunnerConfigurer</code> can be used to register the worker(s) and initialize the polling loop. It manages the task workers thread pool and server communication (poll and task update).</p> <p>Use the Builder to create an instance of the <code>TaskRunnerConfigurer</code>. The builder accepts the following parameters:</p> <pre><code> TaskClient taskClient = new TaskClient();\n taskClient.setRootURI(\"{{ server_host }}{{ api_prefix }}/\");        //Point this to the server API\n\n        int threadCount = 2;            //number of threads used to execute workers.  To avoid starvation, should be same or more than number of workers\n\n        Worker worker1 = new SampleWorker(\"task_1\");\n        Worker worker2 = new SampleWorker(\"task_5\");\n\n        // Create TaskRunnerConfigurer\n        TaskRunnerConfigurer configurer = new TaskRunnerConfigurer.Builder(taskClient, Arrays.asList(worker1, worker2))\n            .withThreadCount(threadCount)\n            .build();\n\n        // Start the polling and execution of tasks\n        configurer.init();\n</code></pre> <p>See Sample for full example.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#configuration-details","title":"Configuration Details","text":"<p>Initialize the <code>Builder</code> with the following:</p> Parameter Description TaskClient TaskClient used to communicate with the Conductor server Workers Workers that will be used for polling work and task execution. Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless <code>pollOutOfDiscovery</code> is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- <p>Once an instance is created, call <code>init()</code> method to initialize the <code>TaskPollExecutor</code> and begin the polling and execution of tasks.</p> <p>Note</p> <p>To ensure that the <code>TaskRunnerConfigurer</code> stops polling for tasks when the instance becomes unhealthy, call the provided <code>shutdown()</code> hook in a <code>PreDestroy</code> block.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#properties","title":"Properties","text":"<p>The worker behavior can be further controlled by using these properties:</p> Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false <p>Further, these properties can be set either by a <code>Worker</code> implementation or by setting the following system properties in the JVM:</p> Name Description <code>conductor.worker.&lt;property&gt;</code> Applies to ALL the workers in the JVM. <code>conductor.worker.&lt;taskDefName&gt;.&lt;property&gt;</code> Applies to the specified worker.  Overrides the global property."},{"location":"devguide/how-tos/Workers/build-a-python-task-worker.html","title":"Build a Python Task Worker","text":"<p>See  conductor-sdk/conductor-python</p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html","title":"Debugging Workflows","text":"<p>Conductor UI is a tool that we can leverage for debugging issues. Refer to the following articles to search and view your workflow execution.</p> <ol> <li>Searching Workflows</li> <li>View Workflow Executions</li> </ol>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#debugging-executions","title":"Debugging Executions","text":"<p>Open the Tasks &gt; Diagram tab to see the diagram of the overall workflow execution</p> <p>If there is a failure, you will them on the view marked as red. In most cases it should be clear what went wrong from the view itself. To see details of the failure, you can click on the failed task.</p> <p>The following fields are useful in debugging</p> Field Name Description Task Detail &gt; Summary &gt; Reason for Incompletion If an exception was thrown by the worker, it will be captured and displayed here Task Detail &gt; Summary &gt; Worker The worker instance id where this failure last occurred. Useful to dig for detailed logs if not already captured by Conductor Task Detail &gt; Input Verify if the task inputs were computed and provided correctly to the task Task Detail &gt; Output If output of a previous task is used as an input to your next task, refer here for what was produced Task Detail &gt; Logs If your task is supplying logs, we can look at that here Task Detail &gt; Retried Task - Select an instance If your task was retried, we can see all the attempts and correponding details here <p>Note: We can also access the task list from Tasks &gt; Task List tab.</p> <p>Here is a screen grab of the fields referred above.</p> <p></p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#recovering-from-failures","title":"Recovering From Failures","text":"<p>Once we have resolved the underlying issue of workflow execution failure, we might want to replay or retry failed workflows. The UI has functions that would allow us to do this:</p> <p>The Actions button provides the following options:</p> Action Name Description Restart with Current Definitions Restart this workflow from the beginning using the same version of the workflow definition that originally ran this workflow execution. This is useful if the workflow definition has changed and we want to retain this instance to the original version Restart with Latest Definitions Restart this workflow from the beginning using the latest definition of the workflow. If we made changes to definition, we can use this option to re-run this flow with the latest version Retry - From failed task Retry this workflow from the failed task <p></p> <p>Note: Conductor configurations allow your tasks to be retried automatically for transient failures. Refer to the task configuration options on how to leverage this.  </p>"},{"location":"devguide/how-tos/Workflows/handling-errors.html","title":"Handling Errors","text":"<p>When a workflow fails, there are 2 ways to handle the exception.</p>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#set-failureworkflow-in-workflow-definition","title":"Set <code>failureWorkflow</code> in Workflow Definition","text":"<p>In your main workflow definition, you can configure a workflow to run upon failure, by adding the following parameter to the workflow:</p> <pre><code>\"failureWorkflow\": \"&lt;name of your failure workflow\",\n</code></pre> <p>When there is an issue with your workflow, Conductor will start the failure workflow.  By default, three parameters are passed:</p> <ul> <li>reason</li> <li>workflowId: use this to pull the details of the failed workflow.</li> <li>failureStatus</li> </ul>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#example","title":"Example","text":"<p>Here is a sample failure workflow that sends a message to Slack when the workflow fails. It posts the reason and the workflowId into a slack message - to allow for quick debugging:</p> <pre><code>{\n  \"name\": \"shipping_failure\",\n  \"description\": \"workflow for failures with Bobs widget workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"slack_message\",\n      \"taskReferenceName\": \"send_slack_message\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"headers\": {\n            \"Content-type\": \"application/json\"\n          },\n          \"uri\": \"https://hooks.slack.com/services/&lt;Unique Slack generated Key goes here&gt;\",\n          \"method\": \"POST\",\n          \"body\": {\n            \"text\": \"workflow: ${workflow.input.workflowId} failed. ${workflow.input.reason}\"\n          },\n          \"connectionTimeOut\": 5000,\n          \"readTimeOut\": 5000\n        }\n      },\n      \"type\": \"HTTP\",\n      \"retryCount\": 3\n    }\n  ],\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#set-workfowstatuslistenerenabled","title":"Set <code>workfowStatusListenerEnabled</code>","text":"<p>When this is enabled, notifications are now possible, and by building a custom implementation of the Workflow Status Listener, a notification can be sent to an external service. More details.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html","title":"Searching Workflows","text":"<p>In this article we will learn how to search through workflow executions via the UI.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#ui-workflows-view","title":"UI Workflows View","text":"<p>Open the home page of the UI installation. It will take you to the <code>Workflow Executions</code> view. This is where we can look at available workflow executions.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#basic-search","title":"Basic Search","text":"<p>The following fields are available for searching for workflows.</p> Search Field Name Description Workflow Name Use this field to filter workflows by the configured name Workflow ID Use this field to filter to a specific workflow by its id Status Use this field to filter by status - available options are presented as a multi-select option Start Time - From Use this field to filter workflows that started on or after the time specified Start Time - To Use this field to filter workflows that started on or before the time specified Lookback (days) Use this field to filter workflows that ran in the last given number of days Free Text Query If you have indexing enabled, you can query by values that was part of your workflow inputs and outputs <p>The table listing has options to 1. Select columns for display 2. Sort by column value</p> <p>At the bottom of the table, there are options to 1. Select number of rows per page 2. Navigating through pages</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#find-by-tasks","title":"Find by Tasks","text":"<p>In addition to the options listed in Basic Search view, we have the following options in the Find by Tasks view.</p> Search Field Name Description Include Task ID Use this field to filter workflows that contains a task with this id Include Task Name Use this field to filter workflows that contains a task with name Free Text in Tasks If you have indexing enabled, you can query by values that was part of your workflow task inputs and outputs"},{"location":"devguide/how-tos/Workflows/starting-workflows.html","title":"Starting Workflows","text":"<p>Workflow executions can be started by using the following API:</p> <pre><code>POST {{ api_prefix }}/workflow/{name}\n</code></pre> <p><code>{name}</code> is the placeholder for workflow name. The POST API body is your workflow input parameters which can be empty if there are none.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#using-client-sdks","title":"Using Client SDKs","text":"<p>Conductor offers client SDKs for popular languages which has library methods that can be used to make this API call. Refer to the SDK documentation to configure a client in your selected language to invoke workflow executions.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl '{{ server_host }}{{ api_prefix }}/workflow/sample_workflow' \\\n  -H 'accept: text/plain' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"service\":\"fedex\"}'\n</code></pre> <p>In this example we are specifying one input param called <code>service</code> with a value of <code>fedex</code> and the name of the workflow is <code>sample_workflow</code></p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"{{ server_host }}{{ api_prefix }}/workflow/sample_workflow\", {\n    \"headers\": {\n        \"accept\": \"text/plain\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"{\\\"service\\\":\\\"fedex\\\"}\",\n    \"method\": \"POST\",\n});\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html","title":"Updating Workflows","text":"<p>Workflows can be created or updated using the workflow metadata API</p> <pre><code>PUT {{ api_prefix }}/metadata/workflow\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl '{{ server_host }}{{ api_prefix }}/metadata/workflow' \\\n  -X 'PUT' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '[{\"name\":\"sample_workflow\",\"version\":1,\"tasks\":[{\"name\":\"ship_via_fedex\",\"taskReferenceName\":\"ship_via_fedex\",\"type\":\"SIMPLE\"}],\"schemaVersion\":2}]'\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"{{ server_host }}{{ api_prefix }}/metadata/workflow\", {\n  \"headers\": {\n    \"accept\": \"*/*\",\n    \"content-type\": \"application/json\"\n  },\n  \"body\": \"[{\\\"name\\\":\\\"sample_workflow\\\",\\\"version\\\":1,\\\"tasks\\\":[{\\\"name\\\":\\\"ship_via_fedex\\\",\\\"taskReferenceName\\\":\\\"ship_via_fedex\\\",\\\"type\\\":\\\"SIMPLE\\\"}],\\\"schemaVersion\\\":2}]\",\n  \"method\": \"PUT\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html#best-practices","title":"Best Practices","text":"<ol> <li>If you are updating the workflow with new tasks, remember to register the task definitions first</li> <li>You can also use the Conductor Swagger UI to update the workflows </li> </ol>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html","title":"Versioning Workflows","text":"<p>Every workflow has a version number (this number must be an integer.)</p> <p>Versioning allows you to run different versions of the same workflow simultaneously.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#summary","title":"Summary","text":"<p>Use Case:  A new version of your core workflow will add a capability that is required for veryImportantCustomer.  However, otherVeryImportantCustomer will not be ready to implement this code for another 6 months.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#version-1","title":"Version 1","text":"<pre><code>{\n  \"name\": \"Core_workflow\",\n  \"description\": \"Very_important_business\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n        &lt;list of tasks&gt;\n    }\n  ],\n  \"outputParameters\": {\n  }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#version-2","title":"Version 2","text":"<pre><code>{\n  \"name\": \"Core_workflow\",\n  \"description\": \"Very_important_business\",\n  \"version\": 2,\n  \"tasks\": [\n    {\n        &lt;a different list of tasks&gt;\n    }\n  ],\n  \"outputParameters\": {\n  }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#version-2-launch","title":"Version 2 launch","text":"<p>Initially, both customers are on version 1 of the workflow.</p> <ul> <li>veryImportantCustomer may begin transitioning traffic onto version 2.  Any tasks that remain unfinished on version 1 stay* on version 1.  </li> <li>otherVeryImportantCustomer remains on version 1.</li> </ul>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#6-months-later","title":"6 months later","text":"<ul> <li>All veryImportantCustomer workflows are on version 2.</li> <li>otherVeryImportantCustomer may begin transitioning traffic onto version 2.  Any tasks that remain unfinished on version 1 stay on version 1. </li> </ul>"},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html","title":"View Workflow Executions","text":"<p>In this article we will learn how to view workflow executions via the UI.</p>"},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html#viewing-a-workflow-execution","title":"Viewing a Workflow Execution","text":"<p>Refer to Searching Workflows to filter and find an execution you want to view. Click on the workflow id hyperlink to open the Workflow Execution Details page.</p> <p>The following tabs are available to view the details of the Workflow Execution</p> Tab Name Description Tasks Shows a view with the sub tabs Diagram, Task List and Timeline Tasks &gt; Diagram Visual view of the workflow and its tasks. Tasks &gt; Task List Tabular view of the task executions under this workflow. If there were failures, we will be able to see that here Tasks &gt; Timeline Shows the time each of the tasks took for execution in a timeline view Summary Summary view of the workflow execution Workflow Input/Output Shows the input and output payloads of the workflow. Enable copy mode to copy all or parts of the payload JSON Full JSON payload of the workflow including all tasks, inputs and outputs. Useful for detailed debugging."},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html#viewing-a-workflow-task-detail","title":"Viewing a Workflow Task Detail","text":"<p>From both the Tasks &gt; Diagram and Tasks &gt; Task List views, we can click to see a task execution detail. This opens a flyout panel from the side and contains the following tabs.</p> Tab Name Description Summary Summary info of the task execution Input Task input payload - refer to this tab to see computed inputs passed into the task. Enable copy mode to copy all or parts of the payload Output Shows the output payload produced by the executed task. Enable copy mode to copy all or parts of the payload Log Any log messages logged by the task worked will show up here JSON Complete JSON payload for debugging issues Definition Task definition used when executing this task"},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html#execution-path","title":"Execution Path","text":"<p>An exciting feature of conductor is the ability to see the exact execution path of a workflow. The executed paths are shown in green and is easy to follow like the example below. The alternative paths are greyed out for reference</p> <p></p> <p>Errors will be visible on the UI in ref such as the example below</p> <p></p>"},{"location":"devguide/labs/index.html","title":"Guided Tutorial","text":""},{"location":"devguide/labs/index.html#high-level-steps","title":"High Level Steps","text":"<p>Generally, these are the steps necessary in order to put Conductor to work for your business workflow:</p> <ol> <li>Create task worker(s) that poll for scheduled tasks at regular interval</li> <li>Create task definitions for these workers and register them.</li> <li>Create the workflow definition</li> </ol>"},{"location":"devguide/labs/index.html#before-we-begin","title":"Before We Begin","text":"<p>Ensure you have a Conductor instance up and running. This includes both the Server and the UI. We recommend following the Docker Instructions.</p>"},{"location":"devguide/labs/index.html#tools","title":"Tools","text":"<p>For the purpose of testing and issuing API calls, the following tools are useful</p> <ul> <li>Linux cURL command</li> <li>Postman or similar REST client</li> </ul>"},{"location":"devguide/labs/index.html#lets-go","title":"Let's Go","text":"<p>We will begin by defining a simple workflow that utilizes System Tasks. </p> <p>Next</p>"},{"location":"devguide/labs/eventhandlers.html","title":"Events and Event Handlers","text":"<p>In this exercise, we shall:</p> <ul> <li>Publish an Event to Conductor using <code>Event</code> task.</li> <li>Subscribe to Events, and perform actions:<ul> <li>Start a Workflow</li> <li>Complete Task</li> </ul> </li> </ul> <p>Conductor supports eventing with two Interfaces:</p> <ul> <li>Event Task</li> <li>Event Handlers</li> </ul>"},{"location":"devguide/labs/eventhandlers.html#create-workflow-definitions","title":"Create Workflow Definitions","text":"<p>Let's create two workflows:</p> <ul> <li><code>test_workflow_for_eventHandler</code> which will have an <code>Event</code> task to start another workflow, and a <code>WAIT</code> System task that will be completed by an event.</li> <li><code>test_workflow_startedBy_eventHandler</code> which will have an <code>Event</code> task to generate an event to complete <code>WAIT</code> task in the above workflow.</li> </ul> <p>Send <code>POST</code> requests to <code>/metadata/workflow</code> endpoint with below payloads:</p> <pre><code>{\n  \"name\": \"test_workflow_for_eventHandler\",\n  \"description\": \"A test workflow to start another workflow with EventHandler\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"test_start_workflow_event\",\n      \"taskReferenceName\": \"start_workflow_with_event\",\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    },\n    {\n      \"name\": \"test_task_tobe_completed_by_eventHandler\",\n      \"taskReferenceName\": \"test_task_tobe_completed_by_eventHandler\",\n      \"type\": \"WAIT\"\n    }\n  ]\n}\n</code></pre> <pre><code>{\n  \"name\": \"test_workflow_startedBy_eventHandler\",\n  \"description\": \"A test workflow which is started by EventHandler, and then goes on to complete task in another workflow.\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"test_complete_task_event\",\n      \"taskReferenceName\": \"complete_task_with_event\",\n      \"inputParameters\": {\n        \"sourceWorkflowId\": \"${workflow.input.sourceWorkflowId}\"\n      },\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    }\n  ]\n}\n</code></pre>"},{"location":"devguide/labs/eventhandlers.html#event-tasks-in-workflow","title":"Event Tasks in Workflow","text":"<p><code>EVENT</code> task is a System task, and we shall define it just like other Tasks in Workflow, with <code>sink</code> parameter. Also, <code>EVENT</code> task doesn't have to be registered before using in Workflow. This is also true for the <code>WAIT</code> task. Hence, we will not be registering any tasks for these workflows.</p>"},{"location":"devguide/labs/eventhandlers.html#events-are-sent-but-theyre-not-handled-yet","title":"Events are sent, but they're not handled (yet)","text":"<p>Once you try to start <code>test_workflow_for_eventHandler</code> workflow, you would notice that the event is sent successfully, but the second worflow <code>test_workflow_startedBy_eventHandler</code> is not started. We have sent the Events, but we also need to define <code>Event Handlers</code> for Conductor to take any <code>actions</code> based on the Event. Let's create <code>Event Handlers</code>.</p>"},{"location":"devguide/labs/eventhandlers.html#create-event-handlers","title":"Create Event Handlers","text":"<p>Event Handler definitions are pretty much like Task or Workflow definitions. We start by name:</p> <pre><code>{\n  \"name\": \"test_start_workflow\"\n}\n</code></pre> <p>Event Handler should know the Queue it has to listen to. This should be defined in <code>event</code> parameter.</p> <p>When using Conductor queues, define <code>event</code> with format: </p> <p><code>conductor:{workflow_name}:{taskReferenceName}</code></p> <p>And when using SQS, define with format: </p> <p><code>sqs:{my_sqs_queue_name}</code></p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\"\n}\n</code></pre> <p>Event Handler can perform a list of actions defined in <code>actions</code> array parameter, for this particular <code>event</code> queue.</p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\",\n  \"actions\": [\n      \"&lt;insert-actions-here&gt;\"\n  ],\n  \"active\": true\n}\n</code></pre> <p>Let's define <code>start_workflow</code> action. We shall pass the name of workflow we would like to start. The <code>start_workflow</code> parameter can use any of the values from the general Start Workflow Request. Here we are passing in the workflowId, so that the Complete Task Event Handler can use it.</p> <pre><code>{\n    \"action\": \"start_workflow\",\n    \"start_workflow\": {\n        \"name\": \"test_workflow_startedBy_eventHandler\",\n        \"input\": {\n            \"sourceWorkflowId\": \"${workflowInstanceId}\"\n        }\n    }\n}\n</code></pre> <p>Send a <code>POST</code> request to <code>/event</code> endpoint:</p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\",\n  \"actions\": [\n    {\n      \"action\": \"start_workflow\",\n      \"start_workflow\": {\n        \"name\": \"test_workflow_startedBy_eventHandler\",\n        \"input\": {\n          \"sourceWorkflowId\": \"${workflowInstanceId}\"\n        }\n      }\n    }\n  ],\n  \"active\": true\n}\n</code></pre> <p>Similarly, create another Event Handler to complete task.</p> <pre><code>{\n  \"name\": \"test_complete_task_event\",\n  \"event\": \"conductor:test_workflow_startedBy_eventHandler:complete_task_with_event\",\n  \"actions\": [\n    {\n        \"action\": \"complete_task\",\n        \"complete_task\": {\n            \"workflowId\": \"${sourceWorkflowId}\",\n            \"taskRefName\": \"test_task_tobe_completed_by_eventHandler\"\n         }\n    }\n  ],\n  \"active\": true\n}\n</code></pre>"},{"location":"devguide/labs/eventhandlers.html#summary","title":"Summary","text":"<p>After wiring all of the above, starting the <code>test_workflow_for_eventHandler</code> should:</p> <ol> <li>Start <code>test_workflow_startedBy_eventHandler</code> workflow.</li> <li>Sets <code>test_task_tobe_completed_by_eventHandler</code> WAIT task <code>IN_PROGRESS</code>.</li> <li><code>test_workflow_startedBy_eventHandler</code> event task would publish an Event to complete the WAIT task above.</li> <li>Both the workflows would move to <code>COMPLETED</code> state.</li> </ol>"},{"location":"devguide/labs/first-workflow.html","title":"A First Workflow","text":"<p>In this article we will explore how we can run a really simple workflow that runs without deploying any new microservice. </p> <p>Conductor can orchestrate HTTP services out of the box without implementing any code.  We will use that to create and run the first workflow.</p> <p>See System Task for the list of such built-in tasks. Using system tasks is a great way to run a lot of our code in production.</p>"},{"location":"devguide/labs/first-workflow.html#configuring-our-first-workflow","title":"Configuring our First Workflow","text":"<p>This is a sample workflow that we can leverage for our test.</p> <pre><code>{\n  \"name\": \"first_sample_workflow\",\n  \"description\": \"First Sample Workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"get_population_data\",\n      \"taskReferenceName\": \"get_population_data\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&amp;measures=Population\",\n          \"method\": \"GET\"\n        }\n      },\n      \"type\": \"HTTP\"\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"data\": \"${get_population_data.output.response.body.data}\",\n    \"source\": \"${get_population_data.output.response.body.source}\"\n  },\n  \"schemaVersion\": 2,\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"example@email.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0\n}\n</code></pre> <p>This is an example workflow that queries a publicly available JSON API to retrieve some data. This workflow doesn\u2019t require any worker implementation as the tasks in this workflow are managed by the system itself. This is an awesome feature of Conductor. For a lot of typical work, we won\u2019t have to write any code at all.</p> <p>Let's talk about this workflow a little more so that we can gain some context.</p> <pre><code>\"name\" : \"first_sample_workflow\"\n</code></pre> <p>This line here is how we name our workflow. In this case our workflow name is <code>first_sample_workflow</code></p> <p>This workflow contains just one worker. The workers are defined under the key <code>tasks</code>. Here is the worker definition with the most important values:</p> <pre><code>{\n  \"name\": \"get_population_data\",\n  \"taskReferenceName\": \"get_population_data\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&amp;measures=Population\",\n      \"method\": \"GET\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre> <p>Here is a list of fields and what it does:</p> <ol> <li><code>\"name\"</code> : Name of our worker</li> <li><code>\"taskReferenceName\"</code> : This is a reference to this worker in this specific workflow implementation. We can have multiple    workers of the same name in our workflow, but we will need a unique task reference name for each of them. Task    reference name should be unique across our entire workflow.</li> <li><code>\"inputParameters\"</code> : These are the inputs into our worker. We can hard code inputs as we have done here. We can    also provide dynamic inputs such as from the workflow input or based on the output of another worker. We can find    examples of this in our documentation.</li> <li><code>\"type\"</code> : This is what defines what the type of worker is. In our example - this is <code>HTTP</code>. There are more task    types which we can find in the Conductor documentation.</li> <li><code>\"http_request\"</code> : This is an input that is required for tasks of type <code>HTTP</code>. In our example we have provided a well    known internet JSON API url and the type of HTTP method to invoke - <code>GET</code></li> </ol> <p>We haven't talked about the other fields that we can use in our definitions as these are either just metadata or more advanced concepts which we can learn more in the detailed documentation.</p> <p>Ok, now that we have walked through our workflow details, let's run this and see how it works.</p> <p>To configure the workflow, head over to the swagger API of conductor server and access the metadata workflow create API:</p> <p>http://{{ server_host }}/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/metadata-resource/create</p> <p>If the link doesn\u2019t open the right Swagger section, we can navigate to Metadata-Resource \u2192 <code>POST {{ api_prefix }}/metadata/workflow</code></p> <p></p> <p>Paste the workflow payload into the Swagger API and hit Execute.</p> <p>Now if we head over to the UI, we can see this workflow definition created:</p> <p></p> <p>If we click through we can see a visual representation of the workflow:</p> <p></p>"},{"location":"devguide/labs/first-workflow.html#running-our-first-workflow","title":"Running our First Workflow","text":"<p>Let\u2019s run this workflow. To do that we can use the swagger API under the workflow-resources</p> <p>http://{{ server_host }}/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/workflow-resource/startWorkflow_1</p> <p></p> <p>Hit Execute!</p> <p>Conductor will return a workflow id. We will need to use this id to load this up on the UI. If our UI installation has search enabled we wouldn't need to copy this. If we don't have search enabled (using Elasticsearch) copy it from the Swagger UI.</p> <p></p> <p>Ok, we should see this running and get completed soon. Let\u2019s go to the UI to see what happened.</p> <p>To load the workflow directly, use this URL format:</p> <pre><code>http://localhost:5000/execution/&lt;WORKFLOW_ID&gt;\n</code></pre> <p>Replace <code>&lt;WORKFLOW_ID&gt;</code> with our workflow id from the previous step. We should see a screen like below. Click on the different tabs to see all inputs and outputs and task list etc. Explore away!</p> <p></p>"},{"location":"devguide/labs/first-workflow.html#summary","title":"Summary","text":"<p>In this article \u2014 we learned how to run a sample workflow in our Conductor installation. Concepts we touched on:</p> <ol> <li>Workflow creation</li> <li>System tasks such as HTTP</li> <li>Running a workflow via API</li> </ol>"},{"location":"devguide/labs/kitchensink.html","title":"Kitchen Sink","text":"<p>An example kitchensink workflow that demonstrates the usage of all the schema constructs.</p>"},{"location":"devguide/labs/kitchensink.html#definition","title":"Definition","text":"<pre><code>{\n  \"name\": \"kitchensink\",\n  \"description\": \"kitchensink workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"task_1\",\n      \"taskReferenceName\": \"task_1\",\n      \"inputParameters\": {\n        \"mod\": \"${workflow.input.mod}\",\n        \"oddEven\": \"${workflow.input.oddEven}\"\n      },\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"event_task\",\n      \"taskReferenceName\": \"event_0\",\n      \"inputParameters\": {\n        \"mod\": \"${workflow.input.mod}\",\n        \"oddEven\": \"${workflow.input.oddEven}\"\n      },\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    },\n    {\n      \"name\": \"dyntask\",\n      \"taskReferenceName\": \"task_2\",\n      \"inputParameters\": {\n        \"taskToExecute\": \"${workflow.input.task2Name}\"\n      },\n      \"type\": \"DYNAMIC\",\n      \"dynamicTaskNameParam\": \"taskToExecute\"\n    },\n    {\n      \"name\": \"oddEvenDecision\",\n      \"taskReferenceName\": \"oddEvenDecision\",\n      \"inputParameters\": {\n        \"oddEven\": \"${task_2.output.oddEven}\"\n      },\n      \"type\": \"DECISION\",\n      \"caseValueParam\": \"oddEven\",\n      \"decisionCases\": {\n        \"0\": [\n          {\n            \"name\": \"task_4\",\n            \"taskReferenceName\": \"task_4\",\n            \"inputParameters\": {\n              \"mod\": \"${task_2.output.mod}\",\n              \"oddEven\": \"${task_2.output.oddEven}\"\n            },\n            \"type\": \"SIMPLE\"\n          },\n          {\n            \"name\": \"dynamic_fanout\",\n            \"taskReferenceName\": \"fanout1\",\n            \"inputParameters\": {\n              \"dynamicTasks\": \"${task_4.output.dynamicTasks}\",\n              \"input\": \"${task_4.output.inputs}\"\n            },\n            \"type\": \"FORK_JOIN_DYNAMIC\",\n            \"dynamicForkTasksParam\": \"dynamicTasks\",\n            \"dynamicForkTasksInputParamName\": \"input\"\n          },\n          {\n            \"name\": \"dynamic_join\",\n            \"taskReferenceName\": \"join1\",\n            \"type\": \"JOIN\"\n          }\n        ],\n        \"1\": [\n          {\n            \"name\": \"fork_join\",\n            \"taskReferenceName\": \"forkx\",\n            \"type\": \"FORK_JOIN\",\n            \"forkTasks\": [\n              [\n                {\n                  \"name\": \"task_10\",\n                  \"taskReferenceName\": \"task_10\",\n                  \"type\": \"SIMPLE\"\n                },\n                {\n                  \"name\": \"sub_workflow_x\",\n                  \"taskReferenceName\": \"wf3\",\n                  \"inputParameters\": {\n                    \"mod\": \"${task_1.output.mod}\",\n                    \"oddEven\": \"${task_1.output.oddEven}\"\n                  },\n                  \"type\": \"SUB_WORKFLOW\",\n                  \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\",\n                    \"version\": 1\n                  }\n                }\n              ],\n              [\n                {\n                  \"name\": \"task_11\",\n                  \"taskReferenceName\": \"task_11\",\n                  \"type\": \"SIMPLE\"\n                },\n                {\n                  \"name\": \"sub_workflow_x\",\n                  \"taskReferenceName\": \"wf4\",\n                  \"inputParameters\": {\n                    \"mod\": \"${task_1.output.mod}\",\n                    \"oddEven\": \"${task_1.output.oddEven}\"\n                  },\n                  \"type\": \"SUB_WORKFLOW\",\n                  \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\",\n                    \"version\": 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n            \"name\": \"join\",\n            \"taskReferenceName\": \"join2\",\n            \"type\": \"JOIN\",\n            \"joinOn\": [\n              \"wf3\",\n              \"wf4\"\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"search_elasticsearch\",\n      \"taskReferenceName\": \"get_es_1\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      },\n      \"type\": \"HTTP\"\n    },\n    {\n      \"name\": \"task_30\",\n      \"taskReferenceName\": \"task_30\",\n      \"inputParameters\": {\n        \"statuses\": \"${get_es_1.output..status}\",\n        \"workflowIds\": \"${get_es_1.output..workflowId}\"\n      },\n      \"type\": \"SIMPLE\"\n    }\n  ],\n  \"outputParameters\": {\n    \"statues\": \"${get_es_1.output..status}\",\n    \"workflowIds\": \"${get_es_1.output..workflowId}\"\n  },\n  \"ownerEmail\": \"example@email.com\",\n  \"schemaVersion\": 2\n}\n</code></pre>"},{"location":"devguide/labs/kitchensink.html#visual-flow","title":"Visual Flow","text":""},{"location":"devguide/labs/kitchensink.html#running-kitchensink-workflow","title":"Running Kitchensink Workflow","text":"<ol> <li>If you are running Conductor locally, use the <code>-DloadSample=true</code> Java system property when launching the server.  This will create a kitchensink workflow,  related task definitions and kick off an instance of kitchensink workflow. Otherwise, you can create a new Workflow Definition in the UI by copying the sample above.</li> <li>Once the workflow has started, the first task remains in the <code>SCHEDULED</code> state.  This is because no workers are currently polling for the task.</li> <li>We will use the REST endpoints directly to poll for tasks and updating the status.</li> </ol>"},{"location":"devguide/labs/kitchensink.html#start-workflow-execution","title":"Start workflow execution","text":"<p>Start the execution of the kitchensink workflow by posting the following:</p> <p><pre><code>curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' '{{ server_host }}{{ api_prefix }}/workflow/kitchensink' -d '\n{\n    \"task2Name\": \"task_5\" \n}\n'\n</code></pre> The response is a text string identifying the workflow instance id.</p>"},{"location":"devguide/labs/kitchensink.html#poll-for-the-first-task","title":"Poll for the first task:","text":"<pre><code>curl {{ server_host }}{{ api_prefix }}/tasks/poll/task_1\n</code></pre> <p>The response should look something like:</p> <pre><code>{\n    \"taskType\": \"task_1\",\n    \"status\": \"IN_PROGRESS\",\n    \"inputData\": {\n        \"mod\": null,\n        \"oddEven\": null\n    },\n    \"referenceTaskName\": \"task_1\",\n    \"retryCount\": 0,\n    \"seq\": 1,\n    \"pollCount\": 1,\n    \"taskDefName\": \"task_1\",\n    \"scheduledTime\": 1486580932471,\n    \"startTime\": 1486580933869,\n    \"endTime\": 0,\n    \"updateTime\": 1486580933902,\n    \"startDelayInSeconds\": 0,\n    \"retried\": false,\n    \"callbackFromWorker\": true,\n    \"responseTimeoutSeconds\": 3600,\n    \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\",\n    \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\",\n    \"callbackAfterSeconds\": 0,\n    \"polledTime\": 1486580933902,\n    \"queueWaitTime\": 1398\n}\n</code></pre>"},{"location":"devguide/labs/kitchensink.html#update-the-task-status","title":"Update the task status","text":"<ul> <li>Note the values for <code>taskId</code> and <code>workflowInstanceId</code> fields from the poll response</li> <li>Update the status of the task as <code>COMPLETED</code> as below:</li> </ul> <pre><code>curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST {{ server_host }}{{ api_prefix }}/tasks/ -d '\n{\n    \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\",\n    \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\",\n    \"status\": \"COMPLETED\",\n    \"outputData\": {\n        \"mod\": 5,\n        \"taskToExecute\": \"task_1\",\n        \"oddEven\": 0,\n        \"dynamicTasks\": [\n            {\n                \"name\": \"task_1\",\n                \"taskReferenceName\": \"task_1_1\",\n                \"type\": \"SIMPLE\"\n            },\n            {\n                \"name\": \"sub_workflow_4\",\n                \"taskReferenceName\": \"wf_dyn\",\n                \"type\": \"SUB_WORKFLOW\",\n                \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\"\n                }\n            }\n        ],\n        \"inputs\": {\n            \"task_1_1\": {},\n            \"wf_dyn\": {}\n        }\n    }\n}'\n</code></pre> <p>This will mark the task_1 as completed and schedule <code>task_5</code> as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.</p>"},{"location":"devguide/running/docker.html","title":"Running Conductor Using Docker","text":"<p>In this article we will explore how you can set up Conductor on your local machine using Docker compose. The docker compose will bring up the following:</p> <ol> <li>Conductor API Server</li> <li>Conductor UI</li> <li>Elasticsearch for searching workflows</li> </ol>"},{"location":"devguide/running/docker.html#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker: https://docs.docker.com/get-docker/</li> <li>Recommended host with CPU and RAM to be able to run multiple docker containers (at-least 16GB RAM)</li> </ol>"},{"location":"devguide/running/docker.html#steps","title":"Steps","text":""},{"location":"devguide/running/docker.html#1-clone-the-conductor-code","title":"1. Clone the Conductor Code","text":"<pre><code>$ git clone https://github.com/conductor-oss/conductor.git\n</code></pre>"},{"location":"devguide/running/docker.html#2-build-the-docker-compose","title":"2. Build the Docker Compose","text":"<pre><code>$ cd conductor\nconductor $ cd docker\ndocker $ docker-compose build\n</code></pre>"},{"location":"devguide/running/docker.html#3-run-docker-compose","title":"3. Run Docker Compose","text":"<pre><code>docker $ docker-compose up\n</code></pre> <p>Once up and running, you will see the following in your Docker dashboard:</p> <ol> <li>Elasticsearch</li> <li>Conductor UI</li> <li>Conductor Server</li> </ol> <p>You can access the UI &amp; Server on your browser to verify that they are running correctly:</p>"},{"location":"devguide/running/docker.html#conductor-server-url","title":"Conductor Server URL","text":"<p>{{ server_host }}</p> <p></p>"},{"location":"devguide/running/docker.html#conductor-ui-url","title":"Conductor UI URL","text":"<p>http://localhost:5000/</p> <p></p>"},{"location":"devguide/running/docker.html#4-exiting-compose","title":"4. Exiting Compose","text":"<p><code>Ctrl+c</code> will exit docker compose.</p> <p>To ensure images are stopped execute: <code>docker-compose down</code>.</p>"},{"location":"devguide/running/docker.html#alternative-persistence-engines","title":"Alternative Persistence Engines","text":"<p>By default <code>docker-compose.yaml</code> uses <code>config-local.properties</code>. This configures the <code>memory</code> database, where data is lost when the server terminates. This configuration is useful for testing or demo only.</p> <p>A selection of <code>docker-compose-*.yaml</code> and <code>config-*.properties</code> files are provided demonstrating the use of alternative persistence engines.</p> File Containers docker-compose.yaml <ol><li>In Memory Conductor Server</li><li>Elasticsearch</li><li>UI</li></ol> docker-compose-dynomite.yaml <ol><li>Conductor Server</li><li>Elasticsearch</li><li>UI</li><li>Dynomite Redis for persistence</li></ol> docker-compose-postgres.yaml <ol><li>Conductor Server</li><li>Elasticsearch</li><li>UI</li><li>Postgres persistence</li></ol> docker-compose-prometheus.yaml Brings up Prometheus server <p>For example this will start the server instance backed by a PostgreSQL DB. <pre><code>docker-compose -f docker-compose.yaml -f docker-compose-postgres.yaml up\n</code></pre></p>"},{"location":"devguide/running/docker.html#standalone-server-image","title":"Standalone Server Image","text":"<p>To build and run the server image, without using <code>docker-compose</code>, from the <code>docker</code> directory execute: <pre><code>docker build -t conductor:server -f server/Dockerfile ../\ndocker run -p 8080:8080 -d --name conductor_server conductor:server\n</code></pre> This builds the image <code>conductor:server</code> and runs it in a container named <code>conductor_server</code>. The API should now be accessible at <code>{{ server_host }}</code>.</p> <p>To 'login' to the running container, use the command: <pre><code>docker exec -it conductor_server /bin/sh\n</code></pre></p>"},{"location":"devguide/running/docker.html#standalone-ui-image","title":"Standalone UI Image","text":"<p>From the <code>docker</code> directory,  <pre><code>docker build -t conductor:ui -f ui/Dockerfile ../\ndocker run -p 5000:5000 -d --name conductor_ui conductor:ui\n</code></pre> This builds the image <code>conductor:ui</code> and runs it in a container named <code>conductor_ui</code>. The UI should now be accessible at <code>localhost:5000</code>.</p>"},{"location":"devguide/running/docker.html#note","title":"Note","text":"<ul> <li>In order for the UI to do anything useful the Conductor Server must already be running on port 8080, either in a Docker container (see above), or running directly in the local JRE.</li> <li>Additionally, significant parts of the UI will not be functional without Elastisearch being available. Using the <code>docker-compose</code> approach alleviates these considerations.</li> </ul>"},{"location":"devguide/running/docker.html#monitoring-with-prometheus","title":"Monitoring with Prometheus","text":"<p>Start Prometheus with: <code>docker-compose -f docker-compose-prometheus.yaml up -d</code></p> <p>Go to http://127.0.0.1:9090.</p>"},{"location":"devguide/running/docker.html#combined-server-ui-docker-image","title":"Combined Server &amp; UI Docker Image","text":"<p>This image at <code>/docker/serverAndUI</code> is provided to illustrate starting both the server &amp; UI within the same container. The UI is hosted using nginx.</p>"},{"location":"devguide/running/docker.html#building-the-combined-image","title":"Building the combined image","text":"<p>From the <code>docker</code> directory, <pre><code>docker build -t conductor:serverAndUI -f serverAndUI/Dockerfile ../\n</code></pre></p>"},{"location":"devguide/running/docker.html#running-the-combined-image","title":"Running the combined image","text":"<ul> <li>With interal DB: <code>docker run -p 8080:8080 -p 80:5000 -d -t conductor:serverAndUI</code></li> <li>With external DB: <code>docker run -p 8080:8080 -p 80:5000 -d -t -e \"CONFIG_PROP=config.properties\" conductor:serverAndUI</code></li> </ul>"},{"location":"devguide/running/docker.html#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch is optional, please be aware that disable it will make most of the conductor UI not functional.</p>"},{"location":"devguide/running/docker.html#how-to-enable-elasticsearch","title":"How to enable Elasticsearch","text":"<ul> <li>Set <code>conductor.indexing.enabled=true</code> in your_config.properties</li> <li>Add config related to elasticsearch   E.g.: <code>conductor.elasticsearch.url=http://es:9200</code></li> </ul>"},{"location":"devguide/running/docker.html#how-to-disable-elasticsearch","title":"How to disable Elasticsearch","text":"<ul> <li>Set <code>conductor.indexing.enabled=false</code> in your_config.properties</li> <li>Comment out all the config related to elasticsearch E.g.: <code>conductor.elasticsearch.url=http://es:9200</code></li> </ul>"},{"location":"devguide/running/docker.html#troubleshooting","title":"Troubleshooting","text":"<p>To troubleshoot a failed startup, check the server logss located at <code>/app/logs</code> (default directory in dockerfile)</p>"},{"location":"devguide/running/docker.html#potential-problem-when-using-docker-images","title":"Potential problem when using Docker Images","text":""},{"location":"devguide/running/docker.html#not-enough-memory","title":"Not enough memory","text":"<p>You will need at least 16 GB of memory to run everything. You can modify the docker compose to skip using Elasticsearch if you have no option to run this with your memory options.</p> <p>To disable Elasticsearch using Docker Compose - follow the steps above.</p>"},{"location":"devguide/running/docker.html#elasticsearch-fails-to-come-up-in-arm64-based-cpu-machines","title":"Elasticsearch fails to come up in arm64 based CPU machines","text":"<p>As of writing this article, Conductor relies on 6.8.x version of Elasticsearch. This version doesn't have an arm64 based Docker image. You will need to use Elasticsearch 7.x which requires a bit of customization to get up and running </p>"},{"location":"devguide/running/docker.html#elasticsearch-remains-in-yellow-health","title":"Elasticsearch remains in Yellow health","text":"<p>When you run Elasticsearch, sometimes the health remains in Yellow state. Conductor server by default requires Green state to run when indexing is enabled. To work around this, you can use the following property:  <code>conductor.elasticsearch.clusterHealthColor=yellow</code> </p> <p>See Github issue</p>"},{"location":"devguide/running/docker.html#elasticsearch-timeout","title":"Elasticsearch timeout","text":"<p>Symptom: Standalone (single node) Elasticsearch has a yellow status which will cause timeout for the Conductor server at startup (Required: Green).</p> <p>Solution: Spin up a cluster (more than one node) to prevent the timeout or use config option <code>conductor.elasticsearch.clusterHealthColor=yellow</code>.</p> <p>See Github issue</p>"},{"location":"devguide/running/docker.html#changes-in-config-properties-do-not-take-effect","title":"Changes in config-*.properties do not take effect","text":"<p>Config is copied into the image during the docker build. You have to rebuild the image or better, link a volume to it  to reflect new changes automatically.</p>"},{"location":"devguide/running/docker.html#unable-to-access-to-conductorserver-api-on-port-8080","title":"Unable to access to conductor:server API on port 8080","text":"<p>It may takes some time for conductor server to start. Please check server log for errors.</p>"},{"location":"devguide/running/hosted.html","title":"Hosted Solutions","text":""},{"location":"devguide/running/hosted.html#orkes","title":"Orkes","text":"<p>Orkes is a commercial vendor that offers a cloud hosted version of Conductor requiring minimal operational investment to get started.</p>"},{"location":"devguide/running/hosted.html#developer-playground","title":"Developer Playground","text":"<p>Orkes provides a developer playground for Conductor at https://play.orkes.io/.  The playground is self-service and is free to try. The playground allows you to create new workflow definitions, tasks and execute them using the UI or through APIs.</p> <p>Orkes also operates a Slack community featuring discussion and guidance for their product and Conductor in general.</p>"},{"location":"devguide/running/hosted.html#cloud-hosted-conductor","title":"Cloud Hosted Conductor","text":"<p>Orkes provides multiple options of hosted Conductor clusters in the cloud (AWS, Azure, and GCP, in addition to private clouds) with enterprise support provided by the Orkes team.</p> <p>Beyond full compatibility with the open source Netflix Conductor, the Orkes product provides additional features, such as in the area of security and analytics, not present in the open source release.</p>"},{"location":"devguide/running/source.html","title":"Building Conductor From Source","text":""},{"location":"devguide/running/source.html#build-and-run","title":"Build and Run","text":"<p>In this article we will explore how you can set up Conductor on your local machine for trying out some of its features.</p>"},{"location":"devguide/running/source.html#prerequisites","title":"Prerequisites","text":"<ol> <li>JDK 11 or greater</li> <li>(Optional) Docker if you want to run tests.  You can install docker from here.</li> <li>Node for building and running UI.  Instructions at https://nodejs.org.</li> <li>Yarn for building and running UI.  Instructions at https://classic.yarnpkg.com/en/docs/install.</li> </ol>"},{"location":"devguide/running/source.html#steps-to-build-conductor-server","title":"Steps to build Conductor Server","text":""},{"location":"devguide/running/source.html#1-checkout-the-code","title":"1. Checkout the code","text":"<p>Clone conductor code from the repo: https://github.com/conductor-oss/conductor</p> <pre><code>$ git clone https://github.com/conductor-oss/conductor.git\n</code></pre>"},{"location":"devguide/running/source.html#2-build-and-run-server","title":"2. Build and run Server","text":"<p>NOTE for Mac users: If you are using a new Mac with an Apple Silicon Chip, you must make a small change to <code>conductor/grpc/build.gradle</code> - adding \"osx-x86_64\" to two lines: <pre><code>protobuf {\n    protoc {\n        artifact = \"com.google.protobuf:protoc:${revProtoBuf}:osx-x86_64\"\n    }\n    plugins {\n        grpc {\n            artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\"\n        }\n    }\n...\n} \n</code></pre></p> <p>You may also need to install rosetta:  </p> <pre><code>softwareupdate --install-rosetta\n</code></pre> <pre><code>$ cd conductor\nconductor $ cd server\nserver $ ../gradlew bootRun\n</code></pre> <p>Navigate to the swagger API docs: http://{{ server_host }}/swagger-ui/index.html?configUrl=/api-docs/swagger-config</p> <p></p>"},{"location":"devguide/running/source.html#download-and-run","title":"Download and Run","text":"<p>As an alternative to building from source, you can download and run the pre-compiled JAR.</p> <p><pre><code>export CONDUCTOR_VER=3.3.4\nexport REPO_URL=https://repo1.maven.org/maven2/com/netflix/conductor/conductor-server\ncurl $REPO_URL/$CONDUCTOR_VER/conductor-server-$CONDUCTOR_VER-boot.jar \\\n--output conductor-server-$CONDUCTOR_VER-boot.jar; java -jar conductor-server-$CONDUCTOR_VER-boot.jar \n</code></pre> Navigate to the swagger URL: http://{{ server_host }}/swagger-ui/index.html?configUrl=/api-docs/swagger-config</p>"},{"location":"devguide/running/source.html#build-and-run-ui","title":"Build and Run UI","text":""},{"location":"devguide/running/source.html#conductor-ui-from-source","title":"Conductor UI from Source","text":"<p>The UI is a standard <code>create-react-app</code> React Single Page Application (SPA). To get started, with Node 14 and <code>yarn</code> installed, first run <code>yarn install</code> from within the <code>/ui</code> directory to retrieve package dependencies.</p> <pre><code>$ cd conductor/ui\nui $ yarn install\n</code></pre> <p>There is no need to \"build\" the project unless you require compiled assets to host on a production web server. If the latter is true, the project can be built with the command <code>yarn build</code>.</p> <p>To run the UI on the bundled development server, run <code>yarn run start</code>. Navigate your browser to <code>http://localhost:5000</code>. The server must already be running on port 8080. </p> <pre><code>ui $ yarn run start\n</code></pre> <p>Launch UI http://localhost:5000</p> <p></p>"},{"location":"devguide/running/source.html#summary","title":"Summary","text":"<ol> <li>By default in-memory persistence is used, so any workflows created or excuted will be wiped out once the server is terminated.</li> <li>Without indexing configured, the search functionality in UI will not work and will result an empty set.</li> <li>See how to install Conductor using Docker with persistence and indexing.</li> </ol>"},{"location":"documentation/advanced/annotation-processor.html","title":"Annotation Processor","text":"<p>This module is strictly for code generation tasks during builds based on annotations. Currently supports <code>protogen</code></p>"},{"location":"documentation/advanced/annotation-processor.html#usage","title":"Usage","text":"<p>This is an actual example of this module which is implemented in common/build.gradle</p> <pre><code>task protogen(dependsOn: jar, type: JavaExec) {\n    classpath configurations.annotationsProcessorCodegen\n    main = 'com.netflix.conductor.annotationsprocessor.protogen.ProtoGenTask'\n    args(\n            \"conductor.proto\",\n            \"com.netflix.conductor.proto\",\n            \"github.com/netflix/conductor/client/gogrpc/conductor/model\",\n            \"${rootDir}/grpc/src/main/proto\",\n            \"${rootDir}/grpc/src/main/java/com/netflix/conductor/grpc\",\n            \"com.netflix.conductor.grpc\",\n            jar.archivePath,\n            \"com.netflix.conductor.common\",\n    )\n}\n</code></pre>"},{"location":"documentation/advanced/archival-of-workflows.html","title":"Archiving Workflows","text":"<p>Conductor has support for archiving workflow upon termination or completion. Enabling this will delete the workflow from the configured database, but leave the associated data in Elasticsearch so it is still searchable. </p> <p>To enable, set the <code>conductor.workflow-status-listener.type</code> property to <code>archive</code>.</p> <p>A number of additional properties are available to control archival.</p> Property Default Value Description conductor.workflow-status-listener.archival.ttlDuration 0s The time to live in seconds for workflow archiving module. Currently, only RedisExecutionDAO supports this conductor.workflow-status-listener.archival.delayQueueWorkerThreadCount 5 The number of threads to process the delay queue in workflow archival conductor.workflow-status-listener.archival.delaySeconds 60 The time to delay the archival of workflow"},{"location":"documentation/advanced/azureblob-storage.html","title":"Azure Blob Storage","text":"<p>The AzureBlob storage module uses azure blob to store and retrieve workflows/tasks input/output payload that went over the thresholds defined in properties named <code>conductor.[workflow|task].[input|output].payload.threshold.kb</code>.</p> <p>Warning Azure Java SDK use libs already present inside <code>conductor</code> like <code>jackson</code> and <code>netty</code>. You may encounter deprecated issues, or conflicts and need to adapt the code if the module is not maintained along with <code>conductor</code>. It has only been tested with v12.2.0.</p>"},{"location":"documentation/advanced/azureblob-storage.html#configuration","title":"Configuration","text":""},{"location":"documentation/advanced/azureblob-storage.html#example","title":"Example","text":"<pre><code>conductor.additional.modules=com.netflix.conductor.azureblob.AzureBlobModule\nes.set.netty.runtime.available.processors=false\n\nworkflow.external.payload.storage=AZURE_BLOB\nworkflow.external.payload.storage.azure_blob.connection_string=DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;EndpointSuffix=localhost\nworkflow.external.payload.storage.azure_blob.signedurlexpirationseconds=360\n</code></pre>"},{"location":"documentation/advanced/azureblob-storage.html#testing","title":"Testing","text":"<p>You can use Azurite to simulate an Azure Storage.</p>"},{"location":"documentation/advanced/azureblob-storage.html#troubleshoots","title":"Troubleshoots","text":"<ul> <li>When using es5 persistence you will receive an <code>java.lang.IllegalStateException</code> because the Netty lib will call <code>setAvailableProcessors</code> two times. To resolve this issue you need to set the following system property</li> </ul> <pre><code>es.set.netty.runtime.available.processors=false\n</code></pre> <p>If you want to change the default HTTP client of azure sdk, you can use <code>okhttp</code> instead of <code>netty</code>. For that you need to add the following dependency.</p> <pre><code>com.azure:azure-core-http-okhttp:${compatible version}\n</code></pre>"},{"location":"documentation/advanced/extend.html","title":"Extending Conductor","text":""},{"location":"documentation/advanced/extend.html#backend","title":"Backend","text":"<p>Conductor provides a pluggable backend.  The current implementation uses Dynomite.</p> <p>There are 4 interfaces that need to be implemented for each backend:</p> <pre><code>//Store for workflow and task definitions\ncom.netflix.conductor.dao.MetadataDAO\n</code></pre> <pre><code>//Store for workflow executions\ncom.netflix.conductor.dao.ExecutionDAO\n</code></pre> <pre><code>//Index for workflow executions\ncom.netflix.conductor.dao.IndexDAO\n</code></pre> <pre><code>//Queue provider for tasks\ncom.netflix.conductor.dao.QueueDAO\n</code></pre> <p>It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others.</p>"},{"location":"documentation/advanced/extend.html#system-tasks","title":"System Tasks","text":"<p>To create system tasks follow the steps below:</p> <ul> <li>Extend <code>com.netflix.conductor.core.execution.tasks.WorkflowSystemTask</code></li> <li>Instantiate the new class as part of the startup (eager singleton)</li> <li>Implement the <code>TaskMapper</code> interface</li> <li>Add this implementation to the map identified by TaskMappers</li> </ul>"},{"location":"documentation/advanced/extend.html#external-payload-storage","title":"External Payload Storage","text":"<p>To configure conductor to externalize the storage of large payloads:</p> <ul> <li>Implement the <code>ExternalPayloadStorage</code> interface.</li> <li>Add the storage option to the enum here.</li> <li>Set this JVM system property <code>workflow.external.payload.storage</code> to the value of the enum element added above.</li> <li>Add a binding similar to this.</li> </ul>"},{"location":"documentation/advanced/extend.html#workflow-status-listener","title":"Workflow Status Listener","text":"<p>To provide a notification mechanism upon completion/termination of workflows:</p> <ul> <li>Implement the <code>WorkflowStatusListener</code> interface</li> <li>This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state.</li> </ul>"},{"location":"documentation/advanced/extend.html#locking-service","title":"Locking Service","text":"<p>By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module,  for eg., with Dynomite and Redlock:</p> <ul> <li>Implement <code>Lock</code> interface.</li> <li>Add a binding similar to this</li> <li>Enable locking service: <code>conductor.app.workflowExecutionLockEnabled: true</code></li> </ul>"},{"location":"documentation/advanced/extend.html#event-handling","title":"Event Handling","text":"<p>Provide the implementation of EventQueueProvider.</p> <p>E.g. SQS Queue Provider:  SQSEventQueueProvider.java </p>"},{"location":"documentation/advanced/externalpayloadstorage.html","title":"External Payload Storage","text":"<p>Warning</p> <p>The external payload storage is currently only implemented to be used to by the Java client. Client libraries in other languages need to be modified to enable this. Contributions are welcomed.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#context","title":"Context","text":"<p>Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#barriers","title":"Barriers","text":"<p>Conductor typically applies two kinds of barriers:</p> <ul> <li>Soft Barrier</li> <li>Hard Barrier</li> </ul>"},{"location":"documentation/advanced/externalpayloadstorage.html#soft-barrier","title":"Soft Barrier","text":"<p>The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process.  </p>"},{"location":"documentation/advanced/externalpayloadstorage.html#hard-barrier","title":"Hard Barrier","text":"<p>The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#usage","title":"Usage","text":""},{"location":"documentation/advanced/externalpayloadstorage.html#barriers-setup","title":"Barriers setup","text":"<p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value conductor.app.workflowInputPayloadSizeThreshold Soft barrier for workflow input payload in KB 5120 conductor.app.maxWorkflowInputPayloadSizeThreshold Hard barrier for workflow input payload in KB 10240 conductor.app.workflowOutputPayloadSizeThreshold Soft barrier for workflow output payload in KB 5120 conductor.app.maxWorkflowOutputPayloadSizeThreshold Hard barrier for workflow output payload in KB 10240 conductor.app.taskInputPayloadSizeThreshold Soft barrier for task input payload in KB 3072 conductor.app.maxTaskInputPayloadSizeThreshold Hard barrier for task input payload in KB 10240 conductor.app.taskOutputPayloadSizeThreshold Soft barrier for task output payload in KB 3072 conductor.app.maxTaskOutputPayloadSizeThreshold Hard barrier for task output payload in KB 10240"},{"location":"documentation/advanced/externalpayloadstorage.html#amazon-s3","title":"Amazon S3","text":"<p>Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: <pre><code>conductor.external-payload-storage.type=S3\n</code></pre></p> <p>Note</p> <p>This implementation assumes that S3 access is configured on the instance.</p> <p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value conductor.external-payload-storage.s3.bucketName S3 bucket where the payloads will be stored conductor.external-payload-storage.s3.signedUrlExpirationDuration The expiration time in seconds of the signed url for the payload 5 <p>The payloads will be stored in the bucket configured above in a <code>UUID.json</code> file at locations determined by the type of the payload. See here for information about how the object key is determined.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#azure-blob-storage","title":"Azure Blob Storage","text":"<p>ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage.  </p> <p>To build conductor with azure blob feature read the README.md in <code>azureblob-storage</code> module </p> <p>Note</p> <p>This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token. If you want signed url to expired you must specify a Connection String. </p> <p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions <code>Read</code> and <code>Write</code> on Resource <code>Object</code> on Service <code>Blob</code>. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored <code>conductor-payloads</code> workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ <p>The payloads will be stored as done in Amazon S3.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#postgresql-storage","title":"PostgreSQL Storage","text":"<p>Frinx provides an implementation of PostgreSQL Storage used to externalize large payload storage.</p> <p>Note</p> <p>This implementation assumes that you have an PostgreSQL database server with all required credentials.</p> <p>Set the following properties to your application.properties:</p> Property Description default value conductor.external-payload-storage.postgres.conductor-url URL, that can be used to pull the json configurations, that will be downloaded from PostgreSQL to the conductor server. For example: for local development it is <code>{{ server_host }}</code> <code>\"\"</code> conductor.external-payload-storage.postgres.url PostgreSQL database connection URL. Required to connect to database. conductor.external-payload-storage.postgres.username Username for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.password Password for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.table-name The PostgreSQL schema and table name where the payloads will be stored <code>external.external_payload</code> conductor.external-payload-storage.postgres.max-data-rows Maximum count of data rows in PostgreSQL database. After overcoming this limit, the oldest data will be deleted. Long.MAX_VALUE (9223372036854775807L) conductor.external-payload-storage.postgres.max-data-days Maximum count of days of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-months Maximum count of months of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-years Maximum count of years of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 1 <p>The maximum date age for fields in the database will be: <code>years + months + days</code> The payloads will be stored in PostgreSQL database with key (externalPayloadPath) <code>UUID.json</code> and you can generate URI for this data using <code>external-postgres-payload-resource</code> rest controller.  To make this URI work correctly, you must correctly set the conductor-url property.</p>"},{"location":"documentation/advanced/isolationgroups.html","title":"Isolation Groups","text":"<p>Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency.</p> <p>We can isolate the execution of such tasks to have predictable performance using <code>isolationgroupId</code>, a property of task definition.</p> <p>When we set isolationGroupId,  the executor <code>SystemTaskWorkerCoordinator</code> will allocate an isolated queue and an isolated thread pool for execution of those tasks.</p> <p>If no <code>isolationgroupId</code> is specified in task definition, then fallback is default behaviour where the executor executes the task in shared thread-pool for all tasks. </p>"},{"location":"documentation/advanced/isolationgroups.html#example","title":"Example","text":"<p>** Task Definition ** <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"isolationgroupId\": \"myIsolationGroupId\"\n}\n</code></pre> ** Workflow Definition ** <pre><code>{\n  \"name\": \"encode_and_deploy\",\n  \"description\": \"Encodes a file and deploys to CDN\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"encode\",\n      \"taskReferenceName\": \"encode\",\n      \"type\": \"HTTP\", \n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      }\n    }\n  ],\n  \"outputParameters\": {\n    \"cdn_url\": \"${d1.output.location}\"\n  },\n  \"failureWorkflow\": \"cleanup_encode_resources\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre></p> <ul> <li>puts <code>encode</code> in <code>HTTP-myIsolationGroupId</code> queue, and allocates a new thread pool for this for execution.</li> </ul> <p>Note:   To enable this feature, the <code>workflow.isolated.system.task.enable</code> property needs to be made <code>true</code>,its default value is <code>false</code></p> <p>The property <code>workflow.isolated.system.task.worker.thread.count</code>  sets the thread pool size for isolated tasks; default is <code>1</code>.</p> <p>isolationGroupId is currently supported only in HTTP and kafka Task. </p>"},{"location":"documentation/advanced/isolationgroups.html#execution-name-space","title":"Execution Name Space","text":"<p><code>executionNameSpace</code> A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally.</p> <p>Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per <code>isolationGroupId</code>.  Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. </p> <p>To support JVM isolation, and also allow the executors to scale horizontally, we can use <code>executionNameSpace</code> property in taskdef.</p> <p>Executor consumes tasks whose executionNameSpace matches with the configuration property <code>workflow.system.task.worker.executionNameSpace</code></p> <p>If the property is not set, the executor executes tasks without any executionNameSpace set. </p> <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"executionNameSpace\": \"myExecutionNameSpace\"\n}\n</code></pre>"},{"location":"documentation/advanced/isolationgroups.html#example-workflow-task","title":"Example Workflow task","text":"<pre><code>{ \n  \"name\": \"encode_and_deploy\",\n  \"description\": \"Encodes a file and deploys to CDN\",\n  \"version\": 1,\n  \"tasks\": [\n    { \n      \"name\": \"encode\",\n      \"taskReferenceName\": \"encode\",\n      \"type\": \"HTTP\", \n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      }\n    }\n  ],\n  \"outputParameters\": {\n    \"cdn_url\": \"${d1.output.location}\"\n  },\n  \"failureWorkflow\": \"cleanup_encode_resources\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <ul> <li><code>encode</code> task is executed by the executor deployment whose <code>workflow.system.task.worker.executionNameSpace</code> property is <code>myExecutionNameSpace</code> </li> </ul> <p><code>executionNameSpace</code> can be used along with <code>isolationGroupId</code></p> <p>If the above task contains a isolationGroupId <code>myIsolationGroupId</code>, the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace</p>"},{"location":"documentation/advanced/redis.html","title":"Redis","text":"<p>By default conductor runs with an in-memory Redis mock. However, you can change the configuration by setting the properties <code>conductor.db.type</code> and <code>conductor.redis.hosts</code>.</p>"},{"location":"documentation/advanced/redis.html#conductordbtype","title":"<code>conductor.db.type</code>","text":"Value Description dynomite Dynomite Cluster. Dynomite is a proxy layer that provides sharding and replication. memory Uses an in-memory Redis mock. Should be used only for development and testing purposes. redis_cluster Redis Cluster configuration. redis_sentinel Redis Sentinel configuration. redis_standalone Redis Standalone configuration."},{"location":"documentation/advanced/redis.html#conductorredishosts","title":"<code>conductor.redis.hosts</code>","text":"<p>Expected format is <code>host:port:rack</code> separated by semicolon, e.g.: </p> <pre><code>conductor.redis.hosts=host0:6379:us-east-1c;host1:6379:us-east-1c;host2:6379:us-east-1c\n</code></pre>"},{"location":"documentation/advanced/redis.html#auth-support","title":"Auth Support","text":"<p>Password authentication is supported. The password should be set as the 4th param of the first host <code>host:port:rack:password</code>, e.g.:</p> <pre><code>conductor.redis.hosts=host0:6379:us-east-1c:my_str0ng_pazz;host1:6379:us-east-1c;host2:6379:us-east-1c\n</code></pre> <p>Notes</p> <ul> <li>In a cluster, all nodes use the same password.</li> <li>In a sentinel configuration, sentinels and redis nodes use the same password.</li> </ul>"},{"location":"documentation/api/index.html","title":"API Specification","text":"<p>See the following sections for API endpoint documentation. </p> <ul> <li>Metadata API</li> <li>Start Workflow API</li> <li>Workflow API</li> <li>Task API</li> </ul> <p>Task Domains can be used to address tasks to specific pools of workers at runtime.</p>"},{"location":"documentation/api/index.html#swagger-ui","title":"Swagger UI","text":"<p>As an alternative resource, the Swagger UI will always have the definitive interface description.</p>"},{"location":"documentation/api/metadata.html","title":"Metadata API","text":""},{"location":"documentation/api/metadata.html#workflow-metadata","title":"Workflow Metadata","text":"Endpoint Description Input <code>GET {{ api_prefix }}/metadata/workflow</code> Get all the workflow definitions n/a <code>POST {{ api_prefix }}/metadata/workflow</code> Register new workflow Workflow Definition <code>PUT {{ api_prefix }}/metadata/workflow</code> Register/Update new workflows List of Workflow Definition <code>GET {{ api_prefix }}/metadata/workflow/{name}?version=</code> Get the workflow definitions workflow name, version (optional)"},{"location":"documentation/api/metadata.html#task-metadata","title":"Task Metadata","text":"Endpoint Description Input <code>GET {{ api_prefix }}/metadata/taskdefs</code> Get all the task definitions n/a <code>GET {{ api_prefix }}/metadata/taskdefs/{taskType}</code> Retrieve task definition Task Name <code>POST {{ api_prefix }}/metadata/taskdefs</code> Register new task definitions List of Task Definitions <code>PUT {{ api_prefix }}/metadata/taskdefs</code> Update a task definition A Task Definition <code>DELETE {{ api_prefix }}/metadata/taskdefs/{taskType}</code> Delete a task definition Task Name"},{"location":"documentation/api/startworkflow.html","title":"Start Workflow API","text":""},{"location":"documentation/api/startworkflow.html#api-parameters","title":"API Parameters","text":"<p>When starting a Workflow execution with a registered definition, <code>{{ api_prefix }}/workflow</code> accepts following parameters in the <code>POST</code> payload:</p> Field Description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef An adhoc Workflow Definition to run, without registering. See Dynamic Workflows. optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional"},{"location":"documentation/api/startworkflow.html#output","title":"Output","text":"<p>On success, this API returns the ID of the workflow.</p>"},{"location":"documentation/api/startworkflow.html#basic-example","title":"Basic Example","text":"<p><code>POST {{ server_host }}{{ api_prefix }}/workflow</code> with payload body:</p> <pre><code>{\n  \"name\": \"myWorkflow\", // Name of the workflow\n  \"version\": 1, // Version\n  \"correlationId\": \"corr1\", // Correlation Id\n  \"priority\": 1, // Priority\n    \"input\": { // Input Value Map\n      \"param1\": \"value1\",\n      \"param2\": \"value2\"\n    },\n  \"taskToDomain\": {\n    // Task to domain map\n  }\n}\n</code></pre>"},{"location":"documentation/api/startworkflow.html#dynamic-workflows","title":"Dynamic Workflows","text":"<p>If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used.</p> <p>This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the <code>workflowDef</code> parameter, avoiding the need to register the blueprints before execution.</p> <p>Example:</p> <p>Send a <code>POST</code> request to <code>{{ api_prefix }}/workflow</code> with payload like: <pre><code>{\n  \"name\": \"my_adhoc_unregistered_workflow\",\n  \"workflowDef\": {\n    \"ownerApp\": \"my_owner_app\",\n    \"ownerEmail\": \"my_owner_email@test.com\",\n    \"createdBy\": \"my_username\",\n    \"name\": \"my_adhoc_unregistered_workflow\",\n    \"description\": \"Test Workflow setup\",\n    \"version\": 1,\n    \"tasks\": [\n        {\n            \"name\": \"fetch_data\",\n            \"type\": \"HTTP\",\n            \"taskReferenceName\": \"fetch_data\",\n            \"inputParameters\": {\n              \"http_request\": {\n                \"connectionTimeOut\": \"3600\",\n                \"readTimeOut\": \"3600\",\n                \"uri\": \"${workflow.input.uri}\",\n                \"method\": \"GET\",\n                \"accept\": \"application/json\",\n                \"content-Type\": \"application/json\",\n                \"headers\": {\n                }\n              }\n            },\n            \"taskDefinition\": {\n                \"name\": \"fetch_data\",\n                \"retryCount\": 0,\n                \"timeoutSeconds\": 3600,\n                \"timeoutPolicy\": \"TIME_OUT_WF\",\n                \"retryLogic\": \"FIXED\",\n                \"retryDelaySeconds\": 0,\n                \"responseTimeoutSeconds\": 3000\n            }\n        }\n    ],\n    \"outputParameters\": {\n    }\n  },\n  \"input\": {\n    \"uri\": \"http://www.google.com\"\n  }\n}\n</code></pre></p> <p>Note</p> <p>If the <code>taskDefinition</code> is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.</p>"},{"location":"documentation/api/task.html","title":"Task API","text":""},{"location":"documentation/api/task.html#manage-tasks","title":"Manage Tasks","text":"Endpoint Description <code>GET {{ api_prefix }}/tasks/{taskId}</code> Get task details. <code>GET {{ api_prefix }}/tasks/queue/all</code> List the pending task sizes. <code>GET {{ api_prefix }}/tasks/queue/all/verbose</code> Same as above, includes the size per shard <code>GET {{ api_prefix }}/tasks/queue/sizes?taskType=&amp;taskType=&amp;taskType</code> Return the size of pending tasks for given task types"},{"location":"documentation/api/task.html#polling-ack-and-update-task","title":"Polling, Ack and Update Task","text":"<p>These endpoints are used by the worker to poll for task, send ack (after polling) and finally updating the task result. They typically should not be invoked manually.</p> Endpoint Description <code>GET {{ api_prefix }}/tasks/poll/{taskType}?workerid=&amp;domain=</code> Poll for a task. <code>workerid</code> identifies the worker that polled for the job and <code>domain</code> allows the poller to poll for a task in a specific domain <code>GET {{ api_prefix }}/tasks/poll/batch/{taskType}?count=&amp;timeout=&amp;workerid=&amp;domain</code> Poll for a task in a batch specified by <code>count</code>.  This is a long poll and the connection will wait until <code>timeout</code> or if there is at-least 1 item available, whichever comes first.<code>workerid</code> identifies the worker that polled for the job and <code>domain</code> allows the poller to poll for a task in a specific domain <code>POST {{ api_prefix }}/tasks</code> Update the result of task execution.  See the schema below."},{"location":"documentation/api/task.html#schema-for-updating-task-result","title":"Schema for updating Task Result","text":"<pre><code>{\n    \"workflowInstanceId\": \"Workflow Instance Id\",\n    \"taskId\": \"ID of the task to be updated\",\n    \"reasonForIncompletion\" : \"If failed, reason for failure\",\n    \"callbackAfterSeconds\": 0,\n    \"status\": \"IN_PROGRESS|FAILED|COMPLETED\",\n    \"outputData\": {\n        //JSON document representing Task execution output     \n    }\n\n}\n</code></pre> <p>Acknowledging tasks after poll</p> <p>If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.</p>"},{"location":"documentation/api/taskdomains.html","title":"Task Domains","text":"<p>Task domains helps support task development. The idea is same \"task definition\" can be implemented in different \"domains\". A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it.  </p> <p>As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \"Task Domain\" feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task.</p> <p>When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers (workers polled at least once in a 10 second window) for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on.</p> <p>If no workers are active for the domains provided:</p> <ul> <li>If <code>NO_DOMAIN</code> is provided as last token in list of domains, then no domain is set.</li> <li>Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain.</li> </ul> <p>Also, a <code>*</code> token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with <code>*</code>. </p> <p>For example, the below configuration:</p> <pre><code>\"taskToDomain\": {\n  \"*\": \"mydomain\",\n  \"some_task_x\":\"NO_DOMAIN\",\n  \"some_task_y\": \"someDomain, NO_DOMAIN\",\n  \"some_task_z\": \"someInactiveDomain1, someInactiveDomain2\"\n}\n</code></pre> <ul> <li>puts <code>some_task_x</code> in default queue (no domain).</li> <li>puts <code>some_task_y</code> in <code>someDomain</code> domain, if available or in default otherwise.</li> <li>puts <code>some_task_z</code> in <code>someInactiveDomain2</code>, even though workers are not available yet.</li> <li>and puts all other tasks in <code>mydomain</code> (even if workers are not available).</li> </ul> <p>Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, <code>NO_DOMAIN</code> token should be used last.</p>"},{"location":"documentation/api/taskdomains.html#how-to-use-task-domains","title":"How to use Task Domains","text":""},{"location":"documentation/api/taskdomains.html#change-the-poll-call","title":"Change the poll call","text":"<p>The poll call must now specify the domain. </p>"},{"location":"documentation/api/taskdomains.html#java-client","title":"Java Client","text":"<p>If you are using the java client then a simple property change will force  TaskRunnerConfigurer to pass the domain to the poller. <pre><code>    conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\"\n</code></pre></p>"},{"location":"documentation/api/taskdomains.html#rest-call","title":"REST call","text":"<p><code>GET {{ api_prefix }}/tasks/poll/batch/T2?workerid=myworker&amp;domain=mydomain</code> <code>GET {{ api_prefix }}/tasks/poll/T2?workerid=myworker&amp;domain=mydomain</code></p>"},{"location":"documentation/api/taskdomains.html#change-the-start-workflow-call","title":"Change the start workflow call","text":"<p>When starting the workflow, make sure the task to domain mapping is passes</p>"},{"location":"documentation/api/taskdomains.html#java-client_1","title":"Java Client","text":"<pre><code>{Map&lt;String, Object&gt; input = new HashMap&lt;&gt;();\ninput.put(\"wf_input1\", \"one\");\n\nMap&lt;String, String&gt; taskToDomain = new HashMap&lt;&gt;();\ntaskToDomain.put(\"T2\", \"mydomain\");\n\n// Other options ...\n// taskToDomain.put(\"*\", \"mydomain, NO_DOMAIN\")\n// taskToDomain.put(\"T2\", \"mydomain, fallbackDomain1, fallbackDomain2\")\n\nStartWorkflowRequest swr = new StartWorkflowRequest();\nswr.withName(\"myWorkflow\")\n    .withCorrelationId(\"corr1\")\n    .withVersion(1)\n    .withInput(input)\n    .withTaskToDomain(taskToDomain);\n\nwfclient.startWorkflow(swr);\n</code></pre>"},{"location":"documentation/api/taskdomains.html#rest-call_1","title":"REST call","text":"<p><code>POST {{ api_prefix }}/workflow</code></p> <pre><code>{\n  \"name\": \"myWorkflow\",\n  \"version\": 1,\n  \"correlatonId\": \"corr1\"\n  \"input\": {\n    \"wf_input1\": \"one\"\n  },\n  \"taskToDomain\": {\n    \"*\": \"mydomain\",\n    \"some_task_x\":\"NO_DOMAIN\",\n    \"some_task_y\": \"someDomain, NO_DOMAIN\"\n  }\n}\n</code></pre>"},{"location":"documentation/api/workflow.html","title":"Workflow API","text":""},{"location":"documentation/api/workflow.html#retrieve-workflows","title":"Retrieve Workflows","text":"Endpoint Description <code>GET {{ api_prefix }}/workflow/{workflowId}?includeTasks=true                               | false</code> Get Workflow State by workflow Id.  If includeTasks is set, then also includes all the tasks executed and scheduled. <code>GET {{ api_prefix }}/workflow/running/{name}</code> Get all the running workflows of a given type <code>GET {{ api_prefix }}/workflow/running/{name}/correlated/{correlationId}?includeClosed=true | false&amp;includeTasks=true                       |false</code> Get all the running workflows filtered by correlation Id.  If includeClosed is set, also includes workflows that have completed running. <code>GET {{ api_prefix }}/workflow/search</code> Search for workflows.  See Below."},{"location":"documentation/api/workflow.html#workflow-search","title":"Workflow Search","text":"<p>Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs.</p> <p><code>GET {{ api_prefix }}/workflow/search?start=&amp;size=&amp;sort=&amp;freeText=&amp;query=</code></p> Parameter Description start Page number.  Defaults to 0 size Number of results to return sort Sorting.  Format is: <code>ASC:&lt;fieldname&gt;</code> or <code>DESC:&lt;fieldname&gt;</code> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause.  e.g. workflowType = 'name_of_workflow'.  Optional if freeText is provided."},{"location":"documentation/api/workflow.html#output","title":"Output","text":"<p>Search result as described below: <pre><code>{\n  \"totalHits\": 0,\n  \"results\": [\n    {\n      \"workflowType\": \"string\",\n      \"version\": 0,\n      \"workflowId\": \"string\",\n      \"correlationId\": \"string\",\n      \"startTime\": \"string\",\n      \"updateTime\": \"string\",\n      \"endTime\": \"string\",\n      \"status\": \"RUNNING\",\n      \"input\": \"string\",\n      \"output\": \"string\",\n      \"reasonForIncompletion\": \"string\",\n      \"executionTime\": 0,\n      \"event\": \"string\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"documentation/api/workflow.html#manage-workflows","title":"Manage Workflows","text":"Endpoint Description <code>PUT {{ api_prefix }}/workflow/{workflowId}/pause</code> Pause.  No further tasks will be scheduled until resumed.  Currently running tasks are not paused. <code>PUT {{ api_prefix }}/workflow/{workflowId}/resume</code> Resume normal operations after a pause. <code>POST {{ api_prefix }}/workflow/{workflowId}/rerun</code> See Below. <code>POST {{ api_prefix }}/workflow/{workflowId}/restart</code> Restart workflow execution from the start.  Current execution history is wiped out. <code>POST {{ api_prefix }}/workflow/{workflowId}/retry</code> Retry the last failed task. <code>PUT {{ api_prefix }}/workflow/{workflowId}/skiptask/{taskReferenceName}</code> See below. <code>DELETE {{ api_prefix }}/workflow/{workflowId}</code> Terminates the running workflow. <code>DELETE {{ api_prefix }}/workflow/{workflowId}/remove</code> Deletes the workflow from system.  Use with caution."},{"location":"documentation/api/workflow.html#rerun","title":"Rerun","text":"<p>Re-runs a completed workflow from a specific task. </p> <p><code>POST {{ api_prefix }}/workflow/{workflowId}/rerun</code></p> <pre><code>{\n  \"reRunFromWorkflowId\": \"string\",\n  \"workflowInput\": {},\n  \"reRunFromTaskId\": \"string\",\n  \"taskInput\": {}\n}\n</code></pre>"},{"location":"documentation/api/workflow.html#skip-task","title":"Skip Task","text":"<p>Skips a task execution (specified as <code>taskReferenceName</code> parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. <code>PUT {{ api_prefix }}/workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&amp;taskReferenceName=</code> <pre><code>{\n  \"taskInput\": {},\n  \"taskOutput\": {}\n}\n</code></pre></p>"},{"location":"documentation/clientsdks/index.html","title":"Client SDKs","text":"<p>Conductor tasks that are executed by remote workers communicate over HTTP endpoints/gRPC to poll for the task and update the status of the execution. The follow SDKs are provided for implementing Conductor workers.</p> <ul> <li>Java</li> <li>Clojure</li> <li>C#</li> <li>Go</li> <li>Python</li> <li>Javascript/Typescript</li> </ul> <p>The non-Java Conductor SDKs are hosted on a separate GitHub repository: github.com/conductor-sdk.  Contributions from the community are encouraged!</p>"},{"location":"documentation/clientsdks/clojure-sdk.html","title":"Clojure SDK","text":"<p>Software Development Kit for Conductor, written on and providing support for Clojure.</p> <p>The code for the Clojure SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#get-the-sdk","title":"Get the SDK","text":"<p>https://clojars.org/io.orkes/conductor-clojure</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#quick-guide","title":"Quick Guide","text":"<ol> <li>Create connection options</li> </ol> <p><pre><code>(def options {\n                  :url  \"{{ server_host }}{{ api_prefix }}/\" ;; Conductor Server Path\n                  :app-key \"THIS-IS-SOME-APP-KEY\" ;; Optional if using Orkes Conductor\n                  :app-secret \"THIS-IS-SOME-APP-SECRET\" ;; Optional if using Orkes Conductor\n              } )\n</code></pre> 1. Creating a task using above options </p> <pre><code>(ns some.namespace \n    (:require [io.orkes.metadata :as metadata])\n\n    ;; Will Create a task. returns nil\n    (metadata/register-tasks options [{\n                         :name \"cool_clj_task\"\n                         :description \"some description\"\n                         :ownerEmail \"somemail@mail.com\"\n                         :retryCount 3\n                         :timeoutSeconds 300\n                         :responseTimeoutSeconds 180 }])\n)\n</code></pre> <ol> <li>Creating a Workflow that uses the task </li> </ol> <p><pre><code>(ns some.namespace \n    (:require [io.orkes.metadata :as metadata])\n\n;; Will Register a workflow that uses the above task returns nil\n(metadata/register-workflow-def options {\n                                              :name \"cool_clj_workflow\"\n                                              :description \"created programatically from clj\"\n                                              :version 1\n                                              :tasks [ {\n                                                       :name \"cool_clj_task\"\n                                                       :taskReferenceName \"cool_clj_task_ref\"\n                                                       :inputParameters {}\n                                                       :type \"SIMPLE\" \n                                                       } ]\n                                              :inputParameters []\n                                              :outputParameters {:message \"${clj_prog_task_ref.output.:message}\"}\n                                              :schemaVersion 2\n                                              :restartable true\n                                              :ownerEmail \"owner@yahoo.com\"\n                                              :timeoutSeconds 0\n                                         }))\n</code></pre> 3. Create and run a list of workers</p> <pre><code>;; Creates a worker and starts polling for work. will return an instance of Runner which can then be used to shutdown\n(def instance (runner-executor-for-workers\n               (list {\n                      :name \"cool_clj_task\"\n                      :execute (fn [someData]\n                                 [:completed {:message \"Hi From Clj i was created programatically\"}])\n                      })\n               options ))\n\n;; Shutsdown the polling for the workers defined above\n(.shutdown instance)\n</code></pre>"},{"location":"documentation/clientsdks/clojure-sdk.html#options","title":"Options","text":"<p>Options are a map with optional paremeters <pre><code>(def options {\n                  :url  \"{{ server_host }}{{ api_prefix }}/\" ;; Api url (Optional will default to \"{{ server_host }}\")\n                  :app-key \"THIS-IS-SOME-APP-KEY\" ;; Application Key (This is only relevant if you are using Orkes Conductor)\n                  :app-secret \"THIS-IS-SOME-APP-SECRET\" ;; Application Secret (This is only relevant if you are using Orkes Conductor)\n              } )\n</code></pre></p>"},{"location":"documentation/clientsdks/clojure-sdk.html#metadata-namespace","title":"Metadata namespace","text":"<p>Holds the functions to register workflows and tasks.</p> <p><code>(:require [conductor.metadata :as metadata])</code></p>"},{"location":"documentation/clientsdks/clojure-sdk.html#registering-tasks","title":"Registering tasks","text":"<p>Takes the option map and a list/vector of tasks to register. on success it will return nil</p> <pre><code>(metadata/register-tasks options [{\n                                                  :name \"cool_clj_task_b\"\n                                                  :description \"some description\"\n                                                  :ownerEmail \"mail@gmail.com\"\n                                                  :retryCount 3\n                                                  :timeoutSeconds 300\n                                                  :responseTimeoutSeconds 180 },\n                                                 {\n                                                  :name \"cool_clj_task_z\"\n                                                  :description \"some description\"\n                                                  :ownerEmail \"mail@gmail.com\"\n                                                  :retryCount 3\n                                                  :timeoutSeconds 300\n                                                  :responseTimeoutSeconds 180 }\n                                                 {\n                                                  :name \"cool_clj_task_x\"\n                                                  :description \"some description\"\n                                                  :ownerEmail \"mail@gmail.com\"\n                                                  :retryCount 3\n                                                  :timeoutSeconds 300\n                                                  :responseTimeoutSeconds 180 }\n                                                 ])\n</code></pre>"},{"location":"documentation/clientsdks/clojure-sdk.html#registering-a-workspace","title":"Registering a workspace","text":"<pre><code>(metadata/register-workflow-def options {\n                                                        :name \"cool_clj_workflow_2\"\n                                                        :description \"created programatically from clj\"\n                                                        :version 1\n                                                        :tasks [ {\n                                                                  :name \"cool_clj_task_b\"\n                                                                  :taskReferenceName \"cool_clj_task_ref\"\n                                                                  :inputParameters {}\n                                                                  :type \"SIMPLE\"\n                                                                  },\n                                                                {\n                                                                 :name \"someting\",\n                                                                 :taskReferenceName \"other\"\n                                                                 :inputParameters {}\n                                                                 :type \"FORK_JOIN\"\n                                                                 :forkTasks [[\n                                                                               {\n                                                                                :name \"cool_clj_task_z\"\n                                                                                :taskReferenceName \"cool_clj_task_z_ref\"\n                                                                                :inputParameters {}\n                                                                                :type \"SIMPLE\"\n                                                                                }\n                                                                               ]\n                                                                              [\n                                                                               {\n                                                                                :name \"cool_clj_task_x\"\n                                                                                :taskReferenceName \"cool_clj_task_x_ref\"\n                                                                                :inputParameters {}\n                                                                                :type \"SIMPLE\"\n                                                                                }\n                                                                               ]\n                                                                              ]\n                                                                 }\n                                                                {\n                                                                 :name \"join\"\n                                                                 :type \"JOIN\"\n                                                                 :taskReferenceName \"join_ref\"\n                                                                 :joinOn [ \"cool_clj_task_z\", \"cool_clj_task_x\"]\n                                                                 }\n                                                                ]\n                                                        :inputParameters []\n                                                        :outputParameters {\"message\" \"${clj_prog_task_ref.output.:message}\"}\n                                                        :schemaVersion 2\n                                                        :restartable true\n                                                        :ownerEmail \"mail@yahoo.com\"\n                                                        :timeoutSeconds 0\n                                                        :timeoutPolicy \"ALERT_ONLY\"\n                                                        })\n</code></pre>"},{"location":"documentation/clientsdks/clojure-sdk.html#client-namespace","title":"Client namespace","text":"<p>The client namespace holds the function to start a workflow and running a worker</p> <p><code>[io.orkes.client :as conductor]</code></p> <p><pre><code>;; Creates a worker and starts polling for work. will return an instance of Runner which can then be used to shutdown\n(def instance (runner-executor-for-workers\n               (list {\n                      :name \"cool_clj_task\"\n                      :execute (fn [someData]\n                                 [:completed {:message \"Hi From Clj i was created programatically\"}])\n                      })\n               options ))\n\n;; Shutsdown the polling for the workers defined above\n(.shutdown instance)\n</code></pre> The (runner-executor-for-workers) function will take a list of worker implementations map, and options and start pooling for work it will return a TaskRunnerConfigurer instance, which you can shutdown by calling the .shutdown() java method</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#mapper-utils-namespace","title":"Mapper-Utils namespace","text":"<p>The  <code>[io.orkes.mapper-utils :as mapper-utils]</code> namespace holds the functions to map to java object which are mostly not necesary.</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#the-mapper-utilsjava-map-clj-map-protocol","title":"The mapper-utils/java-map-&gt;clj-map protocol","text":"<p>Will map a java map to a clojure map which may come in handy for workers implementation. for example consider a worker that sums two input parameters. For a workflow defined like this :</p> <pre><code>(metadata/register-workflow-def options {:name \"simple_wf\"\n                                         :description \"created programatically from clj\"\n                                         :version 1\n                                         :tasks [{:name \"simplest_task\"\n                                                  :taskReferenceName \"repl_task_ref\"\n                                                  :inputParameters {\"firstNumber\" \"${workflow.input.firstNumber}\"\n                                                                     \"secondNumber\" \"${workflow.input.secondNumber}\"}\n                                                  :type \"SIMPLE\"}]\n                                         :inputParameters [\"firstNumber\" \"secondNumber\"]\n                                         :outputParameters {\"result\" \"${repl_task_ref.output.:result}\"}\n                                         :schema-version 2\n                                         :restartable true\n                                         :ownerEmail \"mail@yahoo.com\"\n                                         :timeoutSeconds 0\n                                         :timeoutPolicy \"ALERT_ONLY\"})\n</code></pre> <p>To be able to use the input params you would need to use the string names like this:</p> <pre><code>(def instance (conductor/runner-executor-for-workers\n               (list {:name \"simplest_task\"\n                      :execute (fn [someData]\n\n                                 [:completed {\"result\" (+ (get someData \"firstNumber\") (get someData \"secondNumber\"))}])})\n               options))\n</code></pre> <p>A more clojure friendly way would be to convert to clojure our map :</p> <pre><code>(def instance (conductor/runner-executor-for-workers\n               (list {:name \"simplest_task\"\n                      :execute (fn [someData]\n                      (let [convertedToClj (-&gt; someData mapper-utils/java-map-&gt;clj-map)]\n                        [:completed {\"result\" (+ (:firstNumber convertedToClj) (:secondNumber convertedToClj))}]\n                      ))})\n               options))\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html","title":"C# SDK","text":"<p><code>conductor-csharp</code> repository provides the client SDKs to build Task Workers and Clients in C#</p> <p>The code for the C# SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/csharp-sdk.html#quick-start","title":"Quick Start","text":"<ol> <li>Get Secrets</li> <li>Write workers</li> <li>Run workers</li> <li>Worker Configurations</li> </ol>"},{"location":"documentation/clientsdks/csharp-sdk.html#dependencies","title":"Dependencies","text":"<p><code>conductor-csharp</code> packages are published to nugget package manager.  You can find the latest releases here.</p>"},{"location":"documentation/clientsdks/csharp-sdk.html#write-workers","title":"Write workers","text":"<pre><code> internal class MyWorkflowTask : IWorkflowTask\n    {\n        public MyWorkflowTask(){}\n\n        public string TaskType =&gt; \"test_ctask\";\n        public int? Priority =&gt; null;\n\n        public async Task&lt;TaskResult&gt; Execute(Conductor.Client.Models.Task task, CancellationToken token)\n        {\n           Dictionary&lt;string, object&gt; newOutput = new Dictionary&lt;string, object&gt;();\n           newOutput.Add(\"output\", \"1\");\n           return task.Completed(task.OutputData.MergeValues(newOutput));\n        }\n    }\n\n internal class MyWorkflowTask2 : IWorkflowTask\n    {\n        public MyWorkflowTask2(){}\n\n        public string TaskType =&gt; \"test_ctask2\";\n        public int? Priority =&gt; null;\n\n        public async Task&lt;TaskResult&gt; Execute(Conductor.Client.Models.Task task, CancellationToken token)\n        {\n           Dictionary&lt;string, object&gt; newOutput = new Dictionary&lt;string, object&gt;();\n           //Reuse the existing code written in C#\n           newOutput.Add(\"output\", \"success\");\n           return task.Completed(task.OutputData.MergeValues(newOutput));\n        }\n    }\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html#run-workers","title":"Run workers","text":"<pre><code>using System;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Collections.Generic;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\nusing Microsoft.Extensions.Logging;\nusing Conductor.Client.Models;\nusing Conductor.Client.Extensions;\nusing Conductor.Client.Interfaces;\n\nusing Task = System.Threading.Tasks.Task;\nusing Conductor.Client;\nusing System.Collections.Concurrent;\n\nnamespace TestOrkesSDK\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            new HostBuilder()\n                 .ConfigureServices((ctx, services) =&gt;\n                 {\n                    // First argument is optional headers which client wasnt to pass.\n                     Configuration configuration = new Configuration(new ConcurrentDictionary&lt;string, string&gt;(), \n                         \"KEY\",\n                         \"SECRET\");\n                     services.AddConductorWorker(configuration);\n                     services.AddConductorWorkflowTask&lt;MyWorkflowTask&gt;();\n                     services.AddHostedService&lt;WorkflowsWorkerService&gt;();\n                 })\n                 .ConfigureLogging(logging =&gt;\n                 {\n                     logging.SetMinimumLevel(LogLevel.Debug);\n                     logging.AddConsole();\n                 })\n                 .RunConsoleAsync();\n            Console.ReadLine();\n        }\n    }\n\n    internal class MyWorkflowTask : IWorkflowTask\n    {\n        public MyWorkflowTask() { }\n\n        public string TaskType =&gt; \"my_ctask\";\n        public int? Priority =&gt; null;\n\n        public async Task&lt;TaskResult&gt; Execute(Conductor.Client.Models.Task task, CancellationToken token)\n        {\n            Dictionary&lt;string, object&gt; newOutput = new Dictionary&lt;string, object&gt;();\n            newOutput.Add(\"output\", 1);\n            return task.Completed(task.OutputData.MergeValues(newOutput));\n        }\n    }\n}\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html#note","title":"Note:","text":"<p>Replace KEY and SECRET by obtaining a new key and secret from Orkes Playground</p> <p>See Generating Access Keys for Programmatic Access for details./</p> <pre><code>    internal class WorkflowsWorkerService : BackgroundService\n    {\n        private readonly IWorkflowTaskCoordinator workflowTaskCoordinator;\n        private readonly IEnumerable&lt;IWorkflowTask&gt; workflowTasks;\n\n        public WorkflowsWorkerService(\n            IWorkflowTaskCoordinator workflowTaskCoordinator,\n            IEnumerable&lt;IWorkflowTask&gt; workflowTasks\n        )\n        {\n            this.workflowTaskCoordinator = workflowTaskCoordinator;\n            this.workflowTasks = workflowTasks;\n        }\n\n        protected override async Task ExecuteAsync(CancellationToken stoppingToken)\n        {\n            foreach (var worker in workflowTasks)\n            {\n                workflowTaskCoordinator.RegisterWorker(worker);\n            }\n            // start all the workers so that it can poll for the tasks\n            await workflowTaskCoordinator.Start();\n        }\n    }\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html#worker-configurations","title":"Worker Configurations","text":"<p>Worker configuration is handled via Configuration object passed when initializing TaskHandler. <pre><code>Configuration configuration = \n    new Configuration(new ConcurrentDictionary&lt;string, string&gt;(), \"KEY\", \"SECRET\", \"https://play.orkes.io/\");\n</code></pre></p>"},{"location":"documentation/clientsdks/csharp-sdk.html#registering-and-starting-the-workflow-using-sdk","title":"Registering and starting the workflow using SDK.","text":"<p>Below is the code snippet that shows how to register a simple workflow and start execution:</p> <p><pre><code>IDictionary&lt;string, string&gt; optionalHeaders = new ConcurrentDictionary&lt;string, string&gt;();\nConfiguration configuration = new Configuration(optionalHeaders, \"keyId\", \"keySecret\");\n\n//Create task definition\nMetadataResourceApi metadataResourceApi = new MetadataResourceApi(configuration);\nTaskDef taskDef = new TaskDef(name: \"test_task\");\ntaskDef.OwnerEmail = \"test@test.com\";\nmetadataResourceApi.RegisterTaskDef(new List&lt;TaskDef&gt;() { taskDef});\n\n//Create workflow definition\nWorkflowDef workflowDef = new WorkflowDef();\nworkflowDef.Name = \"test_workflow\";\nworkflowDef.OwnerEmail = \"test@test.com\";\nworkflowDef.SchemaVersion = 2;\n\nWorkflowTask workflowTask = new WorkflowTask();\nworkflowTask.Type = \"HTTP\";\nworkflowTask.Name = \"test_\"; //Same as registered task definition.\nIDictionary&lt;string, string&gt; requestParams = new Dictionary&lt;string, string&gt;();\nrequestParams.Add(\"uri\", \"https://www.google.com\"); //adding a key/value using the Add() method\nrequestParams.Add(\"method\", \"GET\");\nDictionary&lt;string, object&gt; request = new Dictionary&lt;string, object&gt;();\nrequest.Add(\"http_request\", requestParams);\nworkflowTask.InputParameters = request;\nworkflowDef.Tasks = new List&lt;WorkflowTask&gt;() { workflowTask };\n//Run a workflow\nWorkflowResourceApi workflowResourceApi = new WorkflowResourceApi(configuration);\nDictionary&lt;string, Object&gt; input = new Dictionary&lt;string, Object&gt;();\n//Fill the input map which workflow consumes.\nworkflowResourceApi.StartWorkflow(\"test_workflow\", input, 1);\nConsole.ReadLine();\n</code></pre> Please see Conductor.Api for the APIs.</p>"},{"location":"documentation/clientsdks/go-sdk.html","title":"Go SDK","text":"<p>The code for the Golang SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/go-sdk.html#quick-start","title":"Quick Start","text":"<ol> <li>Setup conductor-go package</li> <li>Create and run Task Workers</li> <li>Create workflows using Code</li> <li>API Documentation</li> </ol>"},{"location":"documentation/clientsdks/go-sdk.html#setup-conductor-go-package","title":"Setup conductor go package","text":"<p>Create a folder to build your package: <pre><code>mkdir quickstart/\ncd quickstart/\ngo mod init quickstart\n</code></pre></p> <p>Get Conductor Go SDK</p> <pre><code>go get github.com/conductor-sdk/conductor-go\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#configuration","title":"Configuration","text":""},{"location":"documentation/clientsdks/go-sdk.html#authentication-settings-optional","title":"Authentication settings (optional)","text":"<p>Use if your conductor server requires authentication * keyId: Key * keySecret: Secret for the Key</p> <pre><code>authenticationSettings := settings.NewAuthenticationSettings(\n  \"keyId\",\n  \"keySecret\",\n)\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#access-control-setup","title":"Access Control Setup","text":"<p>See Access Control for more details on role based access control with Conductor and generating API keys for your environment.</p>"},{"location":"documentation/clientsdks/go-sdk.html#configure-api-client","title":"Configure API Client","text":"<pre><code>apiClient := client.NewAPIClient(\n    settings.NewAuthenticationSettings(\n        KEY,\n        SECRET,\n    ),\n    settings.NewHttpSettings(\n        \"https://play.orkes.io\",\n    ),\n)\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#setup-logging","title":"Setup Logging","text":"<p>SDK uses logrus for the logging.</p> <pre><code>func init() {\n    log.SetFormatter(&amp;log.TextFormatter{})\n    log.SetOutput(os.Stdout)\n    log.SetLevel(log.DebugLevel)\n}\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#next-create-and-run-task-workers","title":"Next: Create and run Task Workers","text":""},{"location":"documentation/clientsdks/java-sdk.html","title":"Java SDK","text":"<p>Conductor provides the following java clients to interact with the various APIs</p> Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task"},{"location":"documentation/clientsdks/java-sdk.html#worker","title":"Worker","text":"<p>Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server.</p> <p>Implement the Worker interface to execute the task.</p>"},{"location":"documentation/clientsdks/java-sdk.html#taskrunnerconfigurer","title":"TaskRunnerConfigurer","text":"<p>The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update).  </p> <p>Use the Builder to create an instance of the TaskRunnerConfigurer. The Builder constructor takes the following parameters.</p> Parameter Description TaskClient TaskClient used to communicate to the Conductor server Workers Workers that will be used for polling work and task execution. <p>The builder accepts the following parameters:</p> Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not.  When the server goes out of discovery, the polling is stopped unless <code>pollOutOfDiscovery</code> is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- withShutdownGracePeriodSeconds Waiting seconds before forcing shutdown of your worker 10 <p>Once an instance is created, call <code>init()</code> method to initialize the TaskPollExecutor and begin the polling and execution of tasks.</p> <p>Note</p> <p>To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided <code>shutdown()</code> hook in a <code>PreDestroy</code> block.</p>"},{"location":"documentation/clientsdks/java-sdk.html#properties","title":"Properties","text":"<p>The worker behavior can be further controlled by using these properties:</p> Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery   status. This is useful while running on a dev machine. false <p>Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM:</p> Name Description <code>conductor.worker.&lt;property&gt;</code> Applies to ALL the workers in the JVM. <code>conductor.worker.&lt;taskDefName&gt;.&lt;property&gt;</code> Applies to the specified worker.  Overrides the global property."},{"location":"documentation/clientsdks/java-sdk.html#examples","title":"Examples","text":"<ul> <li>Sample Worker Implementation</li> <li>Example</li> </ul>"},{"location":"documentation/clientsdks/js-sdk.html","title":"Javascript/TypeScript SDK","text":"<p>See conductor-sdk/conductor-javascript</p>"},{"location":"documentation/clientsdks/python-sdk.html","title":"Python SDK","text":"<p>Software Development Kit for Conductor, written on and providing support for Python.</p> <p>The code for the Python SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/python-sdk.html#quick-guide","title":"Quick Guide","text":"<ol> <li> <p>Create a virtual environment</p> <pre><code>$ virtualenv conductor\n$ source conductor/bin/activate\n$ python3 -m pip list\nPackage    Version\n---------- -------\npip        22.0.3\nsetuptools 60.6.0\nwheel      0.37.1\n</code></pre> </li> <li> <p>Install latest version of <code>conductor-python</code> from pypi</p> <pre><code>$ python3 -m pip install conductor-python\nCollecting conductor-python\nCollecting certifi&gt;=14.05.14\nCollecting urllib3&gt;=1.15.1\nRequirement already satisfied: setuptools&gt;=21.0.0 in ./conductor/lib/python3.8/site-packages (from conductor-python) (60.6.0)\nCollecting six&gt;=1.10\nInstalling collected packages: certifi, urllib3, six, conductor-python\nSuccessfully installed certifi-2021.10.8 conductor-python-1.0.7 six-1.16.0 urllib3-1.26.8\n</code></pre> </li> <li> <p>Create a worker capable of executing a <code>Task</code>. Example:</p> <pre><code>from conductor.client.worker.worker_interface import WorkerInterface\n\nclass SimplePythonWorker(WorkerInterface):\n    def execute(self, task):\n        task_result = self.get_task_result_from_task(task)\n        task_result.add_output_data('key', 'value')\n        task_result.status = 'COMPLETED'\n        return task_result\n</code></pre> <ul> <li>The <code>add_output_data</code> is the most relevant part, since you can store information in a dictionary, which will be sent within <code>TaskResult</code> as your execution response to Conductor</li> </ul> </li> <li> <p>Create a main method to start polling tasks to execute with your worker. Example:</p> <pre><code>from conductor.client.automator.task_handler import TaskHandler\nfrom conductor.client.configuration.configuration import Configuration\nfrom conductor.client.worker.sample.faulty_execution_worker import FaultyExecutionWorker\nfrom conductor.client.worker.sample.simple_python_worker import SimplePythonWorker\n\ndef main():\n    configuration = Configuration(debug=True)\n    task_definition_name = 'python_example_task'\n    workers = [\n        SimplePythonWorker(task_definition_name),\n        FaultyExecutionWorker(task_definition_name)\n    ]\n    with TaskHandler(workers, configuration) as task_handler:\n        task_handler.start()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ul> <li>This example contains two workers, each with a different execution method, capable of running the same <code>task_definition_name</code></li> </ul> </li> <li> <p>Now that you have implemented the example, you can start the Conductor server locally:</p> <ol> <li> <p>Clone Conductor repository:</p> <pre><code>$ git clone https://github.com/conductor-oss/conductor.git\n$ cd conductor/\n</code></pre> </li> <li> <p>Start the Conductor server:</p> <pre><code>/conductor$ ./gradlew bootRun\n</code></pre> </li> <li> <p>Start Conductor UI:</p> <pre><code>/conductor$ cd ui/\n/conductor/ui$ yarn install\n/conductor/ui$ yarn run start\n</code></pre> </li> </ol> <p>You should be able to access:   * Conductor API:     * {{ server_host }}/swagger-ui/index.html   * Conductor UI:     * http://localhost:5000</p> </li> <li> <p>Create a <code>Task</code> within <code>Conductor</code>. Example:</p> <pre><code>$ curl -X 'POST' \\\n    '{{ server_host }}{{ api_prefix }}/metadata/taskdefs' \\\n    -H 'accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -d '[\n    {\n        \"name\": \"python_task_example\",\n        \"description\": \"Python task example\",\n        \"retryCount\": 3,\n        \"retryLogic\": \"FIXED\",\n        \"retryDelaySeconds\": 10,\n        \"timeoutSeconds\": 300,\n        \"timeoutPolicy\": \"TIME_OUT_WF\",\n        \"responseTimeoutSeconds\": 180,\n        \"ownerEmail\": \"example@example.com\"\n    }\n    ]'\n</code></pre> </li> <li> <p>Create a <code>Workflow</code> within <code>Conductor</code>. Example:</p> <pre><code>$ curl -X 'POST' \\\n    '{{ server_host }}{{ api_prefix }}/metadata/workflow' \\\n    -H 'accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n    \"createTime\": 1634021619147,\n    \"updateTime\": 1630694890267,\n    \"name\": \"workflow_with_python_task_example\",\n    \"description\": \"Workflow with Python Task example\",\n    \"version\": 1,\n    \"tasks\": [\n        {\n        \"name\": \"python_task_example\",\n        \"taskReferenceName\": \"python_task_example_ref_1\",\n        \"inputParameters\": {},\n        \"type\": \"SIMPLE\"\n        }\n    ],\n    \"inputParameters\": [],\n    \"outputParameters\": {\n        \"workerOutput\": \"${python_task_example_ref_1.output}\"\n    },\n    \"schemaVersion\": 2,\n    \"restartable\": true,\n    \"ownerEmail\": \"example@example.com\",\n    \"timeoutPolicy\": \"ALERT_ONLY\",\n    \"timeoutSeconds\": 0\n    }'\n</code></pre> </li> <li> <p>Start a new workflow:</p> <pre><code>$ curl -X 'POST' \\\n    '{{ server_host }}{{ api_prefix }}/workflow/workflow_with_python_task_example' \\\n    -H 'accept: text/plain' \\\n    -H 'Content-Type: application/json' \\\n    -d '{}'\n</code></pre> <p>You should receive a Workflow ID at the Response body * Workflow ID example: <code>8ff0bc06-4413-4c94-b27a-b3210412a914</code></p> <p>Now you must be able to see its execution through the UI. * Example: <code>http://localhost:5000/execution/8ff0bc06-4413-4c94-b27a-b3210412a914</code></p> </li> <li> <p>Run your Python file with the <code>main</code> method</p> </li> </ol>"},{"location":"documentation/clientsdks/python-sdk.html#unit-tests","title":"Unit Tests","text":""},{"location":"documentation/clientsdks/python-sdk.html#simple-validation","title":"Simple validation","text":"<pre><code>/conductor-python/src$ python3 -m unittest -v\ntest_execute_task (tst.automator.test_task_runner.TestTaskRunner) ... ok\ntest_execute_task_with_faulty_execution_worker (tst.automator.test_task_runner.TestTaskRunner) ... ok\ntest_execute_task_with_invalid_task (tst.automator.test_task_runner.TestTaskRunner) ... ok\n\n----------------------------------------------------------------------\nRan 3 tests in 0.001s\n\nOK\n</code></pre>"},{"location":"documentation/clientsdks/python-sdk.html#run-with-code-coverage","title":"Run with code coverage","text":"<pre><code>/conductor-python/src$ python3 -m coverage run --source=conductor/ -m unittest\n</code></pre> <p>Report:</p> <pre><code>/conductor-python/src$ python3 -m coverage report\n</code></pre> <p>Visual coverage results:</p> <pre><code>/conductor-python/src$ python3 -m coverage html\n</code></pre>"},{"location":"documentation/configuration/eventhandlers.html","title":"Event Handlers","text":"<p>Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems.</p> <p>This includes:</p> <ol> <li>Being able to produce an event (message) in an external system like SQS or internal to Conductor. </li> <li>Start a workflow when a specific event occurs that matches the provided criteria.</li> </ol> <p>Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow.  Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations.</p>"},{"location":"documentation/configuration/eventhandlers.html#event-task","title":"Event Task","text":"<p>Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.</p> <p>See Event Task for documentation.</p>"},{"location":"documentation/configuration/eventhandlers.html#event-handler","title":"Event Handler","text":"<p>Event handlers are listeners registered that executes an action when a matching event occurs.  The supported actions are:</p> <ol> <li>Start a Workflow</li> <li>Fail a Task</li> <li>Complete a Task</li> </ol> <p>Event Handlers can be configured to listen to Conductor Events or an external event like SQS.</p>"},{"location":"documentation/configuration/eventhandlers.html#configuration","title":"Configuration","text":"<p>Event Handlers are configured via <code>/event/</code> APIs.</p>"},{"location":"documentation/configuration/eventhandlers.html#structure","title":"Structure","text":"<p><pre><code>{\n  \"name\" : \"descriptive unique name\",\n  \"event\": \"event_type:event_location\",\n  \"condition\": \"boolean condition\",\n  \"actions\": [\"see examples below\"]\n}\n</code></pre> <code>condition</code> is an expression that MUST evaluate to a boolean value.  A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to <code>true</code>.</p>"},{"location":"documentation/configuration/eventhandlers.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/eventhandlers.html#condition","title":"Condition","text":"<p>Given the following payload in the message:</p> <pre><code>{\n    \"fileType\": \"AUDIO\",\n    \"version\": 3,\n    \"metadata\": {\n       \"length\": 300,\n       \"codec\": \"aac\"\n    }\n}\n</code></pre> <p>The following expressions can be used in <code>condition</code> with the indicated results:</p> Expression Result <code>$.version &gt; 1</code> true <code>$.version &gt; 10</code> false <code>$.metadata.length == 300</code> true"},{"location":"documentation/configuration/eventhandlers.html#actions","title":"Actions","text":"<p>Examples of actions that can be configured in the <code>actions</code> array:</p> <p>To start a workflow</p> <pre><code>{\n    \"action\": \"start_workflow\",\n    \"start_workflow\": {\n        \"name\": \"WORKFLOW_NAME\",\n        \"version\": \"&lt;optional_param&gt;\",\n        \"input\": {\n            \"param1\": \"${param1}\" \n        }\n    }\n}\n</code></pre> <p>To complete a task</p> <pre><code>{\n    \"action\": \"complete_task\",\n    \"complete_task\": {\n      \"workflowId\": \"${workflowId}\",\n      \"taskRefName\": \"task_1\",\n      \"output\": {\n        \"response\": \"${result}\"\n      }\n    },\n    \"expandInlineJSON\": true\n}\n</code></pre> <p>To fail a task*</p> <p><pre><code>{\n    \"action\": \"fail_task\",\n    \"fail_task\": {\n      \"workflowId\": \"${workflowId}\",\n      \"taskRefName\": \"task_1\",\n      \"output\": {\n        \"response\": \"${result}\"\n      }\n    },\n    \"expandInlineJSON\": true\n}\n</code></pre> Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring task inputs.</p> <p>Expanding stringified JSON elements in payload</p> <p><code>expandInlineJSON</code> property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions. </p>"},{"location":"documentation/configuration/taskdef.html","title":"Task Definition","text":"<p>Task Definitions are used to register SIMPLE tasks (workers). Conductor maintains a registry of user task types. A task type MUST be registered before being used in a workflow.</p> <p>This should not be confused with Task Configurations which are part of the Workflow Definition, and are iterated in the <code>tasks</code> property in the definition.</p>"},{"location":"documentation/configuration/taskdef.html#schema","title":"Schema","text":"Field Type Description Notes name string Task Name. Unique name of the Task that resonates with its function. Must be unique description string Description of the task Optional retryCount number Number of retries to attempt when a Task is marked as failure Defaults to 3 with maximum allowed capped at 10 retryLogic string (enum) Mechanism for the retries See Retry Logic retryDelaySeconds number Time to wait before retries Defaults to 60 seconds timeoutPolicy string (enum) Task's timeout policy Defaults to <code>TIME_OUT_WF</code>; See Timeout Policy timeoutSeconds number Time in seconds, after which the task is marked as <code>TIMED_OUT</code> if not completed after transitioning to <code>IN_PROGRESS</code> status for the first time No timeouts if set to 0 responseTimeoutSeconds number If greater than 0, the task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. Defaults to 3600 pollTimeoutSeconds number Time in seconds, after which the task is marked as <code>TIMED_OUT</code> if not polled by a worker No timeouts if set to 0 inputKeys array of string(s) Array of keys of task's expected input. Used for documenting task's input. Optional. See Using inputKeys and outputKeys. outputKeys array of string(s) Array of keys of task's expected output. Used for documenting task's output. Optional. See Using inputKeys and outputKeys. inputTemplate object Define default input values. Optional. See Using inputTemplate concurrentExecLimit number Number of tasks that can be executed at any given time Optional rateLimitFrequencyInSeconds number Sets the rate limit frequency window. Optional. See Task Rate limits rateLimitPerFrequency number Sets the max number of tasks that can be given to workers within window. Optional. See Task Rate limits below ownerEmail string Email address of the team that owns the task Required"},{"location":"documentation/configuration/taskdef.html#retry-logic","title":"Retry Logic","text":"<ul> <li>FIXED: Reschedule the task after <code>retryDelaySeconds</code></li> <li>EXPONENTIAL_BACKOFF: Reschedule the task after <code>retryDelaySeconds * (2 ^ attemptNumber)</code></li> <li>LINEAR_BACKOFF: Reschedule after <code>retryDelaySeconds * backoffRate * attemptNumber</code></li> </ul>"},{"location":"documentation/configuration/taskdef.html#timeout-policy","title":"Timeout Policy","text":"<ul> <li>RETRY: Retries the task again</li> <li>TIME_OUT_WF: Workflow is marked as TIMED_OUT and terminated. This is the default value.</li> <li>ALERT_ONLY: Registers a counter (task_timeout)</li> </ul>"},{"location":"documentation/configuration/taskdef.html#task-concurrent-execution-limits","title":"Task Concurrent Execution Limits","text":"<p><code>concurrentExecLimit</code> limits the number of simultaneous Task executions at any point.</p> <p>Example  You have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set <code>concurrentExecLimit</code> to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10.</p>"},{"location":"documentation/configuration/taskdef.html#task-rate-limits","title":"Task Rate Limits","text":"<p>Rate Limiting</p> <p>Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers.</p> <ul> <li><code>rateLimitFrequencyInSeconds</code> and <code>rateLimitPerFrequency</code> should be used together.</li> <li><code>rateLimitFrequencyInSeconds</code> sets the \"frequency window\", i.e the <code>duration</code> to be used in <code>events per duration</code>. Eg: 1s, 5s, 60s, 300s etc.</li> <li><code>rateLimitPerFrequency</code>defines the number of Tasks that can be given to Workers per given \"frequency window\". No rate limit if set to 0.</li> </ul> <p>Example Let's set <code>rateLimitFrequencyInSeconds = 5</code>, and <code>rateLimitPerFrequency = 12</code>. This means our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task.  </p> <p>Note that unlike <code>concurrentExecLimit</code>, rate limiting doesn't take into account tasks already in progress/completed. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example.   </p>"},{"location":"documentation/configuration/taskdef.html#using-inputkeys-and-outputkeys","title":"Using <code>inputKeys</code> and <code>outputKeys</code>","text":"<ul> <li><code>inputKeys</code> and <code>outputKeys</code> can be considered as parameters and return values for the Task.</li> <li>Consider the task Definition as being represented by an interface: <code>(value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN);</code></li> <li>However, these parameters are not strictly enforced at the moment. Both <code>inputKeys</code> and <code>outputKeys</code> act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition.</li> <li>In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages.</li> </ul>"},{"location":"documentation/configuration/taskdef.html#using-inputtemplate","title":"Using <code>inputTemplate</code>","text":"<ul> <li><code>inputTemplate</code> allows to define default values, which can be overridden by values provided in Workflow.</li> <li>Eg: In your Task Definition, you can define your inputTemplate as:</li> </ul> <pre><code>\"inputTemplate\": {\n    \"url\": \"https://some_url:7004\"\n}\n</code></pre> <ul> <li>Now, in your workflow Definition, when using above task, you can use the default <code>url</code> or override with something else in the task's <code>inputParameters</code>.</li> </ul> <pre><code>\"inputParameters\": {\n    \"url\": \"${workflow.input.some_new_url}\"\n}\n</code></pre>"},{"location":"documentation/configuration/taskdef.html#complete-example","title":"Complete Example","text":"<p>This is an example of a Task Definition for a worker implementation named <code>encode_task</code>.</p> <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"pollTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"ownerEmail\": \"foo@bar.com\",\n  \"description\": \"Sample Encoding task\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/index.html","title":"Workflow Definition","text":"<p>The Workflow Definition contains all the information necessary to define the behavior of a workflow. The most important part of this definition is the <code>tasks</code> property, which is an array of Task Configurations. </p>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-properties","title":"Workflow Properties","text":"Field Type Description Notes name string Name of the workflow description string Description of the workflow Optional version number Numeric field used to identify the version of the schema. Use incrementing numbers. When starting a workflow execution, if not specified, the definition with highest version is used tasks array of object(s) An array of task configurations. Details inputParameters array of string(s) List of input parameters. Used for documenting the required inputs to workflow Optional. outputParameters object JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task inputTemplate object Default input values. See Using inputTemplate Optional. failureWorkflow string Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. Explanation Optional. schemaVersion number Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable boolean Flag to allow Workflow restarts Defaults to true workflowStatusListenerEnabled boolean Enable status callback. Explanation Defaults to false ownerEmail string Email address of the team that owns the workflow Required timeoutSeconds number The timeout in seconds after which the workflow will be marked as <code>TIMED_OUT</code> if it hasn't been moved to a terminal state No timeouts if set to 0 timeoutPolicy string (enum) Workflow's timeout policy Defaults to <code>TIME_OUT_WF</code>"},{"location":"documentation/configuration/workflowdef/index.html#failure-workflow","title":"Failure Workflow","text":"<p>The failure workflow gets the original failed workflow\u2019s input along with 3 additional items,</p> <ul> <li><code>workflowId</code> - The id of the failed workflow which triggered the failure workflow.</li> <li><code>reason</code> - A string containing the reason for workflow failure.</li> <li><code>failureStatus</code> - A string status representation of the failed workflow.</li> <li><code>failureTaskId</code> - The id of the failed task of the workflow that triggered the failure workflow.</li> </ul>"},{"location":"documentation/configuration/workflowdef/index.html#timeout-policy","title":"Timeout Policy","text":"<ul> <li>TIME_OUT_WF: Workflow is marked as TIMED_OUT and terminated</li> <li>ALERT_ONLY: Registers a counter (workflow_failure with status tag set to <code>TIMED_OUT</code>)</li> </ul>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-status-listener","title":"Workflow Status Listener","text":"<p>Setting the <code>workflowStatusListenerEnabled</code> field in your Workflow Definition to <code>true</code> enables notifications.</p> <p>To add a custom implementation of the Workflow Status Listener. Refer to this .</p> <p>The listener can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here.</p>"},{"location":"documentation/configuration/workflowdef/index.html#default-input-with-inputtemplate","title":"Default Input with <code>inputTemplate</code>","text":"<ul> <li><code>inputTemplate</code> allows you to define default input values, which can optionally be overridden at runtime (when the workflow is invoked).</li> <li>Eg: In your Workflow Definition, you can define your inputTemplate as:</li> </ul> <pre><code>\"inputTemplate\": {\n    \"url\": \"https://some_url:7004\"\n}\n</code></pre> <p>And <code>url</code> would be <code>https://some_url:7004</code> if no <code>url</code> was provided as input to your workflow.</p>"},{"location":"documentation/configuration/workflowdef/index.html#task-configurations","title":"Task Configurations","text":"<p>The <code>tasks</code> property in a Workflow Definition defines an array of Task Configurations. This is the blueprint for the workflow. Task Configurations can reference different types of Tasks.</p> <ul> <li>Simple Tasks</li> <li>System Tasks</li> <li>Operators</li> </ul> <p>Note: Task Configuration should not be confused with Task Definitions, which are used to register SIMPLE (worker based) tasks.</p> Field Type Description Notes name string Name of the task. MUST be registered as a Task Type with Conductor before starting workflow taskReferenceName string Alias used to refer the task within the workflow.  MUST be unique within workflow. type string Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description string Description of the task optional optional boolean true  or false.  When set to true - workflow continues even if the task fails.  The status of the task is reflected as <code>COMPLETED_WITH_ERRORS</code> Defaults to <code>false</code> inputParameters object JSON template that defines the input given to the task. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task. See Using Expressions for details inputExpression object JSONPath expression that defines the input given to the task. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task. See Using Expressions for details asyncComplete boolean <code>false</code> to mark status COMPLETED upon execution; <code>true</code> to keep the task IN_PROGRESS and wait for an external event to complete it. Defaults to <code>false</code> startDelay number Time in seconds to wait before making the task available to be polled by a worker. Defaults to 0. <p>In addition to these parameters, System Tasks have their own parameters. Check out System Tasks for more information.</p>"},{"location":"documentation/configuration/workflowdef/index.html#using-expressions","title":"Using Expressions","text":"<p>Each executed task is given an input based on the <code>inputParameters</code> template or the <code>inputExpression</code> configured in the task configuration. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task.</p>"},{"location":"documentation/configuration/workflowdef/index.html#inputparameters","title":"inputParameters","text":"<p><code>inputParameters</code> can use JSONPath expressions to extract values out of the workflow input and other tasks in the workflow.</p> <p>For example, workflows are supplied an <code>input</code> by the client/caller when a new execution is triggered. The workflow <code>input</code> is available via an expression of the form <code>${workflow.input...}</code>. Likewise, the <code>input</code> and <code>output</code> data of a previously executed task can also be extracted using an expression for use in the <code>inputParameters</code> of a subsequent task.</p> <p>Generally, <code>inputParameters</code> can use expressions of the following syntax:</p> <p><code>${SOURCE.input/output.JSONPath}</code></p> Field Description SOURCE Can be either <code>\"workflow\"</code> or the reference name of any task input/output Refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output <p>JSON Path Support</p> <p>Conductor supports JSONPath specification and uses Java implementation from here.</p> <p>Escaping expressions</p> <p>To escape an expression, prefix it with an extra $ character (ex.: <code>$${workflow.input...}</code>).</p>"},{"location":"documentation/configuration/workflowdef/index.html#inputexpression","title":"inputExpression","text":"<p><code>inputExpression</code> can be used to select an entire object from the workflow input, or the output of another task. The field supports all definite JSONPath expressions.</p> <p>The syntax for mapping values in <code>inputExpression</code> follows the pattern,</p> <p><code>SOURCE.input/output.JSONPath</code></p> <p>NOTE: The <code>inputExpression</code> field does not require the expression to be wrapped in <code>${}</code>.</p> <p>See example below.</p>"},{"location":"documentation/configuration/workflowdef/index.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/index.html#example-1-a-basic-workflow-definition","title":"Example 1 - A Basic Workflow Definition","text":"<p>Assume your business logic is to simply to get some shipping information and then do the shipping. You start by logically partitioning them into two tasks:</p> <ol> <li>shipping_info - The first task takes the provided account number, and outputs an address.  </li> <li>shipping_task - The 2nd task takes the address info and generates a shipping label.</li> </ol> <p>We can configure these two tasks in the <code>tasks</code> array of our Workflow Definition. Let's assume that <code>shipping info</code> takes an account number, and returns a name and address.</p> <pre><code>{\n  \"name\": \"mail_a_box\",\n  \"description\": \"shipping Workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"shipping_info\",\n      \"taskReferenceName\": \"shipping_info_ref\",\n      \"inputParameters\": {\n        \"account\": \"${workflow.input.accountNumber}\"\n      },\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"shipping_task\",\n      \"taskReferenceName\": \"shipping_task_ref\",\n      \"inputParameters\": {\n        \"name\": \"${shipping_info_ref.output.name}\",\n        \"streetAddress\": \"${shipping_info_ref.output.streetAddress}\",\n        \"city\": \"${shipping_info_ref.output.city}\",\n        \"state\": \"${shipping_info_ref.output.state}\",\n        \"zipcode\": \"${shipping_info_ref.output.zipcode}\",\n      },\n      \"type\": \"SIMPLE\"\n    }\n  ],\n  \"outputParameters\": {\n    \"trackingNumber\": \"${shipping_task_ref.output.trackinNumber}\"\n  },\n  \"failureWorkflow\": \"shipping_issues\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0,\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>Upon completion of the 2 tasks, the workflow outputs the tracking number generated in the 2nd task.  If the workflow fails, a second workflow named <code>shipping_issues</code> is run.</p>"},{"location":"documentation/configuration/workflowdef/index.html#example-2-task-configuration","title":"Example 2 - Task Configuration","text":"<p>Consider a task <code>http_task</code> with input configured to use input/output parameters from workflow and a task named <code>loc_task</code>.</p> <pre><code>{\n  \"name\": \"encode_workflow\",\n  \"description\": \"Encode movie.\",\n  \"version\": 1,\n  \"inputParameters\": [\n    \"movieId\", \"fileLocation\", \"recipe\"\n  ],\n  \"tasks\": [\n    {\n      \"name\": \"loc_task\",\n      \"taskReferenceName\": \"loc_task_ref\",\n      \"taskType\": \"SIMPLE\",\n      ...      \n    },    \n    {\n      \"name\": \"http_task\",\n      \"taskReferenceName\": \"http_task_ref\",\n      \"taskType\": \"HTTP\",\n      \"inputParameters\": {\n        \"movieId\": \"${workflow.input.movieId}\",\n        \"url\": \"${workflow.input.fileLocation}\",\n        \"lang\": \"${loc_task.output.languages[0]}\",\n        \"http_request\": {\n          \"method\": \"POST\",\n          \"url\": \"http://example.com/${loc_task.output.fileId}/encode\",\n          \"body\": {\n            \"recipe\": \"${workflow.input.recipe}\",\n            \"params\": {\n              \"width\": 100,\n              \"height\": 100\n            }\n          },\n          \"headers\": {\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\"\n          }\n        }\n      }\n    }\n  ],\n  \"ownerEmail\": \"conductor@example.com\",\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>Consider the following as the workflow input</p> <p><pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\":\"s3://moviebucket/file123\",\n  \"recipe\":\"png\"\n}\n</code></pre> And the output of the loc_task as the following;</p> <pre><code>{\n  \"fileId\": \"file_xxx_yyy_zzz\",\n  \"languages\": [\"en\",\"ja\",\"es\"]\n}\n</code></pre> <p>When scheduling the task, Conductor will merge the values from workflow input and <code>loc_task</code>'s output and create the input to the <code>http_task</code> as follows:</p> <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"url\": \"s3://moviebucket/file123\",\n  \"lang\": \"en\",\n  \"http_request\": {\n    \"method\": \"POST\",\n    \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\",\n    \"body\": {\n      \"recipe\": \"png\",\n      \"params\": {\n        \"width\": 100,\n        \"height\": 100\n      }\n    },\n    \"headers\": {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/json\"\n    }\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/index.html#example-3-inputexpression","title":"Example 3 - inputExpression","text":"<p>Given the following task configuration: <pre><code>{\n  \"name\": \"loc_task\",\n  \"taskReferenceName\": \"loc_task_ref\",\n  \"taskType\": \"SIMPLE\",\n  \"inputExpression\": {\n    \"expression\": \"workflow.input\",\n    \"type\": \"JSON_PATH\"\n  }  \n}\n</code></pre></p> <p>When the workflow is invoked with the following workflow input <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\":\"s3://moviebucket/file123\",\n  \"recipe\":\"png\"\n}\n</code></pre></p> <p>When the task <code>loc_task</code> is scheduled, the entire workflow input object will be passed in as the task input: <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\":\"s3://moviebucket/file123\",\n  \"recipe\":\"png\"\n}\n</code></pre></p>"},{"location":"documentation/configuration/workflowdef/operators/index.html","title":"Operators","text":"<p>Operators are built-in primitives in Conductor that allow you to define the control flow in the workflow. Operators are similar to programming constructs such as for loops, decisions, etc. Conductor has support for most of the programing primitives allowing you to define the most advanced workflows.</p> <p>Conductor supports the following programming language constructs: </p> Language Construct Conductor Operator Do-While or For Loops Do While Task Function Pointer Dynamic Task Dynamic Parallel execution Dynamic Fork Task Static Parallel execution Fork Task Map Join Task Subroutine / Fork Process Sub Workflow Task Switch/if..then...else Switch Task Exit Terminate Task Global Variables Variable Task"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html","title":"Do-While","text":"<pre><code>\"type\" : \"DO_WHILE\"\n</code></pre> <p>The <code>DO_WHILE</code> task sequentially executes a list of tasks as long as a given condition is true.  The list of tasks is executed first, before the condition is checked (the first iteration will always execute).</p> <p>When scheduled, each task of this loop will see its <code>taskReferenceName</code> concatenated with __i, with i being the iteration number, starting at 1. Warning: taskReferenceName containing arithmetic operators must not be used.</p> <p>Each task output is stored as part of the DO_WHILE task, indexed by the iteration value (see example below), allowing the condition to reference the output of a task for a specific iteration (eg. $.LoopTask['iteration]['first_task'])</p> <p>The DO_WHILE task is set to <code>FAILED</code> as soon as one of the loopOver fails. In such case retry, iteration starts from 1.</p>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#limitations","title":"Limitations","text":"<ul> <li>Domain or isolation group execution is unsupported; </li> <li>Nested DO_WHILE is unsupported, however, DO_WHILE task supports SUB_WORKFLOW as loopOver task, so we can achieve similar functionality.</li> <li>Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task is not respected.</li> </ul> <p>Branching inside loopOver task is supported.</p>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#configuration","title":"Configuration","text":"<p>The following fields must be specified at the top level of the task configuration.</p> name type description loopCondition String Condition to be evaluated after every iteration. This is a Javascript expression, evaluated using the Nashorn engine. If an exception occurs during evaluation, the DO_WHILE task is set to FAILED_WITH_TERMINAL_ERROR. loopOver List[Task] List of tasks that needs to be executed as long as the condition is true."},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#output","title":"Output","text":"name type description iteration Integer Iteration number: the current one while executing; the final one once the loop is finished <code>i</code> Map[String, Any] Iteration number as a string, mapped to the task references names and their output. * Any Any state can be stored here if the <code>loopCondition</code> does so. For example <code>storage</code> will exist if <code>loopCondition</code> is <code>if ($.LoopTask['iteration'] &lt;= 10) {$.LoopTask.storage = 3; true } else {false}</code>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#basic-example","title":"Basic Example","text":"<p>The following definition: <pre><code>{\n    \"name\": \"Loop Task\",\n    \"taskReferenceName\": \"LoopTask\",\n    \"type\": \"DO_WHILE\",\n    \"inputParameters\": {\n      \"value\": \"${workflow.input.value}\"\n    },\n    \"loopCondition\": \"if ( ($.LoopTask['iteration'] &lt; $.value ) || ( $.first_task['response']['body'] &gt; 10)) { false; } else { true; }\",\n    \"loopOver\": [\n        {\n            \"name\": \"first task\",\n            \"taskReferenceName\": \"first_task\",\n            \"inputParameters\": {\n                \"http_request\": {\n                    \"uri\": \"http://localhost:8082\",\n                    \"method\": \"POST\"\n                }\n            },\n            \"type\": \"HTTP\"\n        },{\n            \"name\": \"second task\",\n            \"taskReferenceName\": \"second_task\",\n            \"inputParameters\": {\n                \"http_request\": {\n                    \"uri\": \"http://localhost:8082\",\n                    \"method\": \"POST\"\n                }\n            },\n            \"type\": \"HTTP\"\n        }\n    ],\n    \"startDelay\": 0,\n    \"optional\": false\n}\n</code></pre></p> <p>will produce the following execution, assuming 3 executions occurred (alongside <code>first_task__1</code>, <code>first_task__2</code>, <code>first_task__3</code>, <code>second_task__1</code>, <code>second_task__2</code> and <code>second_task__3</code>):</p> <pre><code>{\n    \"taskType\": \"DO_WHILE\",\n    \"outputData\": {\n        \"iteration\": 3,\n        \"1\": {\n            \"first_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            },\n            \"second_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            }\n        },\n        \"2\": {\n            \"first_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            },\n            \"second_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            }\n        },\n        \"3\": {\n            \"first_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            },\n            \"second_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#example-using-iteration-key","title":"Example using iteration key","text":"<p>Sometimes, you may want to use the iteration value/counter in the tasks used in the loop.  In this example, an API call is made to GitHub (to the Conductor repository), but each loop increases the pagination.</p> <p>The Loop <code>taskReferenceName</code> is \"get_all_stars_loop_ref\".</p> <p>In the <code>loopCondition</code> the term <code>$.get_all_stars_loop_ref['iteration']</code> is used.</p> <p>In tasks embedded in the loop, <code>${get_all_stars_loop_ref.output.iteration}</code> is used.  In this case, it is used to define which page of results the API should return.</p> <pre><code>{\n      \"name\": \"get_all_stars\",\n      \"taskReferenceName\": \"get_all_stars_loop_ref\",\n      \"inputParameters\": {\n        \"stargazers\": \"4000\"\n      },\n      \"type\": \"DO_WHILE\",\n      \"decisionCases\": {},\n      \"defaultCase\": [],\n      \"forkTasks\": [],\n      \"startDelay\": 0,\n      \"joinOn\": [],\n      \"optional\": false,\n      \"defaultExclusiveJoinTask\": [],\n      \"asyncComplete\": false,\n      \"loopCondition\": \"if ($.get_all_stars_loop_ref['iteration'] &lt; Math.ceil($.stargazers/100)) { true; } else { false; }\",\n      \"loopOver\": [\n        {\n          \"name\": \"100_stargazers\",\n          \"taskReferenceName\": \"hundred_stargazers_ref\",\n          \"inputParameters\": {\n            \"counter\": \"${get_all_stars_loop_ref.output.iteration}\",\n            \"http_request\": {\n              \"uri\": \"https://api.github.com/repos/ntflix/conductor/stargazers?page=${get_all_stars_loop_ref.output.iteration}&amp;per_page=100\",\n              \"method\": \"GET\",\n              \"headers\": {\n                \"Authorization\": \"token ${workflow.input.gh_token}\",\n                \"Accept\": \"application/vnd.github.v3.star+json\"\n              }\n            }\n          },\n          \"type\": \"HTTP\",\n          \"decisionCases\": {},\n          \"defaultCase\": [],\n          \"forkTasks\": [],\n          \"startDelay\": 0,\n          \"joinOn\": [],\n          \"optional\": false,\n          \"defaultExclusiveJoinTask\": [],\n          \"asyncComplete\": false,\n          \"loopOver\": [],\n          \"retryCount\": 3\n        }\n      ]\n    }\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html","title":"Dynamic Fork","text":"<pre><code>\"type\" : \"FORK_JOIN_DYNAMIC\"\n</code></pre> <p>The <code>DYNAMIC_FORK</code> operation in Conductor lets you execute a list of tasks or sub-workflows in parallel. This list will be determined at run-time and be of variable length.</p> <p>A <code>DYNAMIC_FORK</code> is typically followed by <code>JOIN</code> task that collects outputs from each of the forked tasks or sub workflows.</p> <p>There are three things that are needed to configure a <code>FORK_JOIN_DYNAMIC</code> task.</p> <ol> <li>A list of tasks or sub-workflows that needs to be forked and run in parallel.</li> <li>A list of inputs to each of these forked tasks or sub-workflows</li> <li>A task prior to the <code>FORK_JOIN_DYNAMIC</code> tasks outputs 1 and 2 above that can be wired in as in input to    the <code>FORK_JOIN_DYNAMIC</code> tasks</li> </ol>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#use-cases","title":"Use Cases","text":"<p>A <code>FORK_JOIN_DYNAMIC</code> is useful, when a set of tasks or sub-workflows needs to be executed and the number of tasks or sub-workflows are determined at run time. E.g. Let's say we have a task that resizes an image, and we need to create a workflow that will resize an image into multiple sizes. In this case, a task can be created prior to the <code>FORK_JOIN_DYNAMIC</code> task that will prepare the input that needs to be passed into the <code>FORK_JOIN_DYNAMIC</code> task. The single image resize task does one job. The <code>FORK_JOIN_DYNAMIC</code> and the following <code>JOIN</code> will manage the multiple invokes of the single image resize task. Here, the responsibilities are clearly broken out, where the single image resize task does the core job and <code>FORK_JOIN_DYNAMIC</code> manages the orchestration and fault tolerance aspects.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#configuration","title":"Configuration","text":"<p>To use the <code>DYNAMIC_FORK</code> task, you need to provide the following attributes at the top level of the task configuration, as well as corresponding values inside <code>inputParameters</code>.</p> Attribute Description dynamicForkTasksParam Name of attribute in <code>inputParameters</code> to read array of task or subworkflow names from. dynamicForkTasksInputParamName Name of attribute in <code>inputParameters</code> to read map of inputs to use for spawned tasks or subworkflows."},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#inputparameters","title":"inputParameters","text":"Attribute Description *dynamicForkTasksParam This is a JSON array of tasks or sub-workflow objects that needs to be forked and run in parallel (Note: This has a different format for <code>SUB_WORKFLOW</code> compared to <code>SIMPLE</code> tasks.) *dynamicForkTasksInputParamName A JSON map, where the keys are task or sub-workflow names, and the values are the <code>inputParameters</code> to be passed into the corresponding spawned tasks or sub-workflows. <p>Note: * means the de-referenced name.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#example-1","title":"Example 1","text":"<p>Here is an example of a <code>FORK_JOIN_DYNAMIC</code> task followed by a <code>JOIN</code> task</p> <pre><code>[\n  {\n    \"inputParameters\": {\n      \"dynamicTasks\": \"${fooBarTask.output.dynamicTasksJSON}\",\n      \"dynamicTasksInput\": \"${fooBarTask.output.dynamicTasksInputJSON}\"\n    },\n    \"type\": \"FORK_JOIN_DYNAMIC\",\n    \"dynamicForkTasksParam\": \"dynamicTasks\",\n    \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n  },\n  {\n    \"name\": \"image_multiple_convert_resize_join\",\n    \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\",\n    \"type\": \"JOIN\"\n  }\n]\n</code></pre> <p>Dissecting into this example above, let's look at the three things that are needed to configured for the <code>FORK_JOIN_DYNAMIC</code> task</p> <p><code>dynamicForkTasksParam</code> This is a JSON array of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel <code>dynamicForkTasksInputParamName</code> This is a JSON map of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel fooBarTask This is a task that is defined prior to the FORK_JOIN_DYNAMIC in the workflow definition. This task will need to output (outputParameters) 1 and 2 above so that it can be wired into inputParameters of the FORK_JOIN_DYNAMIC tasks. (dynamicTasks and dynamicTasksInput)</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#example-2","title":"Example 2","text":"<p>Let's say we have a task that resizes an image, and we need to create a workflow that will resize an image into multiple sizes. In this case, a task can be created prior to the <code>FORK_JOIN_DYNAMIC</code> task that will prepare the input that needs to be passed into the <code>FORK_JOIN_DYNAMIC</code> task. These will be:</p> <ul> <li><code>dynamicForkTasksParam</code> the JSON array of tasks/subworkflows to be run in parallel. Each JSON object will have: </li> <li>A unique <code>taskReferenceName</code>.</li> <li>The name of the Task/Subworkflow to be called (note - the location of this key:value is different for a subworkflow).</li> <li>The type of the task (This is optional for SIMPLE tasks).</li> <li><code>dynamicForkTasksInputParamName</code> a JSON map of input parameters for each task. The keys will be the unique <code>taskReferenceName</code> defined in the first JSON array, and the values will be the specific input parameters for the task/subworkflow.</li> </ul> <p>The <code>image_resize</code> task works to resize just one image. The <code>FORK_JOIN_DYNAMIC</code> and the following <code>JOIN</code> will manage the multiple invocations of the single <code>image_resize</code> task. The responsibilities are clearly broken out, where the individual  <code>image_resize</code> tasks do the core job and <code>FORK_JOIN_DYNAMIC</code> manages the orchestration and fault tolerance aspects of handling multiple invocations of the task.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#workflow-definition-task-configuration","title":"Workflow Definition - Task Configuration","text":"<p>Here is an example of a <code>FORK_JOIN_DYNAMIC</code> task followed by a <code>JOIN</code> task.  The fork is named and given a taskReferenceName, but all of the input parameters are JSON variables that we will discuss next:</p> <pre><code>[\n  {      \n    \"name\": \"image_multiple_convert_resize_fork\",\n    \"taskReferenceName\": \"image_multiple_convert_resize_fork_ref\",\n    \"inputParameters\": {\n      \"dynamicTasks\": \"${fooBarTask.output.dynamicTasksJSON}\",\n      \"dynamicTasksInput\": \"${fooBarTask.output.dynamicTasksInputJSON}\"\n    },\n    \"type\": \"FORK_JOIN_DYNAMIC\",\n    \"dynamicForkTasksParam\": \"dynamicTasks\",\n    \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n  },\n  {\n    \"name\": \"image_multiple_convert_resize_join\",\n    \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\",\n    \"type\": \"JOIN\"\n  }\n]\n</code></pre> <p>This appears in the UI as follows:</p> <p></p> <p>Let's assume this data is sent to the workflow:</p> <pre><code>{\n    \"fileLocation\": \"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\",\n    \"outputFormats\": [\"png\",\"jpg\"],\n\n    \"outputSizes\": [\n        {\"width\":300,\n        \"height\":300},\n        {\"width\":200,\n        \"height\":200}\n    ],\n    \"maintainAspectRatio\": \"true\"\n}\n</code></pre> <p>With 2 file formats and 2 sizes in the input, we'll be creating 4 images total.  The first task will generate the tasks and the parameters for these tasks:</p> <ul> <li><code>dynamicForkTasksParam</code> This is a JSON array of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel. This JSON varies depeding oon the type of task.  </li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#dynamicforktasksparam-simple-task","title":"<code>dynamicForkTasksParam</code> Simple task","text":"<p>In this case, our fork is running a SIMPLE task: <code>image_convert_resize</code>:</p> <pre><code>{ \"dynamicTasks\": [\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_png_300x300_0\",\n    ...\n  },\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_png_200x200_1\",\n    ...\n  },\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_jpg_300x300_2\",\n    ...\n  },\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_jpg_200x200_3\",\n    ...\n  }\n]}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#dynamicforktasksparam-subworkflow-task","title":"<code>dynamicForkTasksParam</code> SubWorkflow task","text":"<p>In this case, our Dynamic fork is running a SUB_WORKFLOW task: <code>image_convert_resize_subworkflow</code></p> <pre><code>{ \"dynamicTasks\": [\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_png_300x300_0\",\n    ...\n  },\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_png_200x200_1\",\n    ...\n  },\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_jpg_300x300_2\",\n    ...\n  },\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_jpg_200x200_3\",\n    ...\n  }\n]}\n</code></pre> <ul> <li><code>dynamicForkTasksInputParamName</code> This is a JSON map of task or sub-workflow objects and all the input parameters that these tasks will need to run.</li> </ul> <pre><code>\"dynamicTasksInput\":{\n\"image_convert_resize_jpg_300x300_2\":{\n\"outputWidth\":300\n\"outputHeight\":300\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"jpg\"\n\"maintainAspectRatio\":true\n}\n\"image_convert_resize_jpg_200x200_3\":{\n\"outputWidth\":200\n\"outputHeight\":200\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"jpg\"\n\"maintainAspectRatio\":true\n}\n\"image_convert_resize_png_200x200_1\":{\n\"outputWidth\":200\n\"outputHeight\":200\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"png\"\n\"maintainAspectRatio\":true\n}\n\"image_convert_resize_png_300x300_0\":{\n\"outputWidth\":300\n\"outputHeight\":300\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"png\"\n\"maintainAspectRatio\":true\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#join-task","title":"Join Task","text":"<p>The JOIN task will run after all of the dynamic tasks, collecting the output for all of the tasks.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html","title":"Dynamic","text":"<p><pre><code>\"type\" : \"DYNAMIC\"\n</code></pre> The <code>DYNAMIC</code> task allows one to execute a task whose name is resolved dynamically at run-time. The task name to execute is specified as <code>taskToExecute</code> in <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#use-cases","title":"Use Cases","text":"<p>Consider a scenario, when we have to make decision of executing a task dynamically i.e. while the workflow is still running. In such cases, Dynamic Task would be useful.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#configuration","title":"Configuration","text":"<p>To use the <code>DYNAMIC</code> task, you need to provide <code>dynamicTaskNameParam</code> at the top level of the task configuration, as well as an attribute in <code>inputParameters</code> matching the value you selected for <code>dynamicTaskNameParam</code>.</p> name description dynamicTaskNameParam Name of the parameter from <code>inputParameters</code> whose value is used to schedule the task. e.g. <code>\"taskToExecute\"</code>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#inputparameters","title":"inputParameters","text":"name description *dynamicTaskNameParam e.g. <code>taskToExecute</code> Name of task to execute."},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#example","title":"Example","text":"<p>Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of Post Code.</p> <p>Consider the following 3 task definitions.</p> <p>The following task <code>shipping_info</code> generates an output on the basis of which decision would be taken to run the next task.</p> <pre><code>{\n  \"name\": \"shipping_info\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 600,\n  \"pollTimeoutSeconds\": 1200,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 300,\n  \"responseTimeoutSeconds\": 300,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"ownerEmail\":\"abc@example.com\",\n  \"rateLimitPerFrequency\": 1\n}\n</code></pre> <p>The following are the two worker tasks, one among them would execute on the basis of output generated by the <code>shipping_info</code> task :</p> <pre><code>{\n  \"name\": \"ship_via_fedex\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 600,\n  \"pollTimeoutSeconds\": 1200,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 300,\n  \"responseTimeoutSeconds\": 300,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"ownerEmail\":\"abc@example.com\",\n  \"rateLimitPerFrequency\": 2\n},\n{\n  \"name\": \"ship_via_ups\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 600,\n  \"pollTimeoutSeconds\": 1200,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 300,\n  \"responseTimeoutSeconds\": 300,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"ownerEmail\":\"abc@example.com\",\n  \"rateLimitPerFrequency\": 2\n}\n</code></pre> <p>We will create a workflow with the following definition :</p> <pre><code>{\n  \"name\": \"Shipping_Flow\",\n  \"description\": \"Ships smartly on the basis of Shipping info\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"shipping_info\",\n      \"taskReferenceName\": \"shipping_info\",\n      \"inputParameters\": {\n      },\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"shipping_task\",\n      \"taskReferenceName\": \"shipping_task\",\n      \"inputParameters\": {\n        \"taskToExecute\": \"${shipping_info.output.shipping_service}\"\n      },\n      \"type\": \"DYNAMIC\",\n      \"dynamicTaskNameParam\": \"taskToExecute\"\n    }\n\n  ],\n  \"restartable\": true,\n  \"ownerEmail\":\"abc@example.com\",\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <p>The workflow created is shown in the below diagram.</p> <p></p> <p>Note : <code>shipping_task</code> is a <code>DYNAMIC</code> task and the <code>taskToExecute</code> parameter can be set with input value provided while running the workflow or with the output of previous tasks. Here, it is set to the output provided by the previous task i.e. <code>${shipping_info.output.shipping_service}</code>.</p> <p>If the input value is provided while running the workflow it can be accessed by <code>${workflow.input.shipping_service}</code>.</p> <pre><code>{\n  \"shipping_service\": \"ship_via_fedex\"\n}\n</code></pre> <p>We can see in the below example that on the basis of Post Code the shipping service is being decided.</p> <p>Based on given set of inputs i.e. Post Code starts with '9' hence, <code>ship_via_fedex</code> is executed -</p> <p></p> <p>If the Post Code started with anything other than 9 <code>ship_via_ups</code> is executed -</p> <p></p> <p>If the incorrect task name or the task that doesn't exist is provided then the workflow fails and we get the error <code>\"Invalid task specified. Cannot find task by name in the task definitions.\"</code></p> <p>If the null reference is provided in the task name then also the workflow fails and we get the error <code>\"Cannot map a dynamic task based on the parameter and input. Parameter= taskToExecute, input= {taskToExecute=null}\"</code></p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html","title":"Fork","text":"<pre><code>\"type\" : \"FORK_JOIN\"\n</code></pre> <p>A <code>FORK_JOIN</code> operation lets you run a specified list of tasks or sub workflows in parallel. A <code>FORK_JOIN</code> task is followed by a <code>JOIN</code> operation that waits on the forked tasks or sub workflows to finish and collects their outputs.</p> <p>This is also known as a Static Fork to distinguish it from the DYNAMIC_FORK.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#use-cases","title":"Use Cases","text":"<p><code>FORK_JOIN</code> tasks are typically used when a list of tasks can be run in parallel. E.g In a notification workflow, there could be multiple ways of sending notifications, i,e e-mail, SMS, HTTP etc. These notifications are not dependent on each other, and so they can be run in parallel. In such cases, you can create 3 sub-lists of forked tasks for each of these operations.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#configuration","title":"Configuration","text":"<p>A <code>FORK_JOIN</code> task has a <code>forkTasks</code> attribute at the top level of the task configuration that is an array of arrays (<code>[[...], [...]]</code>);</p> Attribute Description forkTasks A list of lists of tasks. Each of the outer list will be invoked in parallel. The inner list can be a graph of other tasks and sub-workflows <p>Each element of <code>forkTasks</code> is itself a list of tasks. These sub-lists are invoked in parallel. The tasks defined within each sublist can be sequential or even more nested forks.</p> <p>A FORK_JOIN is typically followed by a JOIN operation. </p> <p>The behavior of a <code>FORK_JOIN</code> task is not affected by <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#output","title":"Output","text":"<p><code>FORK_JOIN</code> has no output. </p> <p>The <code>FORK_JOIN</code> task is used in conjunction with the JOIN task, which aggregates the output from the parallelized workflows.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#example","title":"Example","text":"<p>Imagine a workflow that sends 3 notifications: email, SMS and HTTP. Since none of these steps are dependant on the others, they can be run in parallel with a fork.</p> <p>The diagram will appear as:</p> <p></p> <p>Here's the JSON definition for the workflow:</p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_email\",\n          \"type\": \"SIMPLE\"\n        },\n        {\n          \"name\": \"email_notification\",\n          \"taskReferenceName\": \"email_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_sms\",\n          \"type\": \"SIMPLE\"\n        },\n        {\n          \"name\": \"sms_notification\",\n          \"taskReferenceName\": \"sms_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_http\",\n          \"type\": \"SIMPLE\"\n        },\n        {\n          \"name\": \"http_notification\",\n          \"taskReferenceName\": \"http_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"notification_join\",\n    \"taskReferenceName\": \"notification_join_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"email_notification_ref\",\n      \"sms_notification_ref\"\n    ]\n  }\n]\n</code></pre> <p>Note</p> <p>There are three parallel 'tines' to this fork, but only two of the outputs are required for the JOIN to continue, owing to the definition of <code>joinOn</code>. The diagram does draw an arrow from <code>http_notification_ref</code> to the <code>notification_join</code>, but it is not required for the workflow to continue. </p> <p>Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being <code>joinOn</code>. The corresponding values are the outputs of those tasks.</p> <pre><code>{\n  \"email_notification_ref\": {\n    \"email_sent_at\": \"2021-11-06T07:37:17+0000\",\n    \"email_sent_to\": \"test@example.com\"\n  },\n  \"sms_notification_ref\": {\n    \"smm_sent_at\": \"2021-11-06T07:37:17+0129\",\n    \"sms_sen\": \"+1-425-555-0189\"\n  }\n}\n</code></pre> <p>See JOIN for more details on the JOIN aspect of the FORK.</p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html","title":"Join","text":"<pre><code>\"type\" : \"JOIN\"\n</code></pre> <p>A <code>JOIN</code> task is used in conjunction with a <code>FORK_JOIN</code> or <code>FORK_JOIN_DYNAMIC</code> task. Each of the aggregated task outputs is given a corresponding key in the <code>JOIN</code> task output.</p> <ul> <li>When used with a <code>FORK_JOIN</code> task, it waits for a user provided list of zero or more of the forked tasks to be completed. </li> <li>When used with a <code>FORK_JOIN_DYNAMIC</code> task, it implicitly waits for all of the dynamically forked tasks to complete.</li> </ul> <p><code>JOIN</code> used in this context is loosely analogous to the Map phase of the Map-Reduce programming pattern. In Conductor, the reduce step could  be implemented as a subsequent task that references the output of the <code>JOIN</code> task.</p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#use-cases","title":"Use Cases","text":"<p>FORK_JOIN and FORK_JOIN_DYNAMIC task are used to execute a collection of other tasks or sub workflows in parallel. In such cases, there is a need to collect the output from the forked tasks before moving to the next stage in the workflow. </p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#configuration","title":"Configuration","text":"<p>When used with <code>FORK_JOIN</code> (Static Fork), The <code>joinOn</code> attribute is provided at the top level of the task configuration. </p> Attribute Description joinOn A list of task reference names that this <code>JOIN</code> task will wait for completion. Omitted for <code>DYNAMIC_FORK</code> <p>The <code>JOIN</code> task does not utilize <code>inputParameters</code>. </p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#output","title":"Output","text":"Attribute Description task_ref_name_1 A task reference name that was being <code>joinOn</code>. The value is the output of that task task_ref_name_2 A task reference name that was being <code>joinOn</code>. The value is the output of that task ... ... task_ref_name_N A task reference name that was being <code>joinOn</code>. The value is the output of that task"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/operators/join-task.html#simple-example","title":"Simple Example","text":"<p>Here is an example of a <code>JOIN</code> task. This task will wait for the completion of tasks <code>my_task_ref_1</code> and <code>my_task_ref_2</code> as specified by the <code>joinOn</code> attribute.</p> <pre><code>{\n  \"name\": \"join_task\",\n  \"taskReferenceName\": \"my_join_task_ref\",\n  \"type\": \"JOIN\",\n  \"joinOn\": [\n    \"my_task_ref_1\",\n    \"my_task_ref_2\"\n  ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#example-ignoring-one-fork","title":"Example - Ignoring one fork","text":"<p>Here is an example of a <code>JOIN</code> task used in conjunction with a <code>FORK_JOIN</code> task. The 'FORK_JOIN' spawns 3 tasks. An <code>email_notification</code> task, a <code>sms_notification</code> task and a  <code>http_notification</code> task. Email and SMS are usually best effort delivery systems. However, in case of a http based notification you get a return code and you can retry until it succeeds or eventually give up. When you setup a notification workflow, you may decide to continue ,if you kicked off an email and sms notification. Im that case, you can decide to <code>joinOn</code> those specific tasks. However, the <code>http_notification</code> task will still continue to execute, but it will not block the rest of the workflow from proceeding.</p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"email_notification\",\n          \"taskReferenceName\": \"email_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"sms_notification\",\n          \"taskReferenceName\": \"sms_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"http_notification\",\n          \"taskReferenceName\": \"http_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"notification_join\",\n    \"taskReferenceName\": \"notification_join_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"email_notification_ref\",\n      \"sms_notification_ref\"\n    ]\n  }\n]\n</code></pre> <p>Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being <code>joinOn</code>. The corresponding values are the outputs of those tasks.</p> <pre><code>{\n  \"email_notification_ref\": {\n    \"email_sent_at\": \"2021-11-06T07:37:17+0000\",\n    \"email_sent_to\": \"test@example.com\"\n  },\n  \"sms_notification_ref\": {\n    \"smm_sent_at\": \"2021-11-06T07:37:17+0129\",\n    \"sms_sen\": \"+1-425-555-0189\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html","title":"Set Variable","text":"<pre><code>\"type\" : \"SET_VARIABLE\"\n</code></pre> <p>The <code>SET_VARIABLE</code> task allows users to create global workflow variables, and update them with new values.</p> <p>Variables can be initialized in the workflow definition as well as during the workflow run. Once a variable is initialized it can be read using the expression <code>${workflow.variables.NAME}</code> by any other task.</p> <p>It can be overwritten by a subsequent <code>SET_VARIABLE</code> task.</p> <p>Warning</p> <p>There is a hard barrier for variables payload size in KB defined in the JVM system properties (<code>conductor.max.workflow.variables.payload.threshold.kb</code>) the default value is <code>256</code>. Passing this barrier will fail the task and the workflow.</p>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#use-cases","title":"Use Cases","text":"<p>For example, you might want to track shared state at the workflow level, and have the state be accessible by any task executed as part of the workflow.</p>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#configuration","title":"Configuration","text":"<p>Global variables can be set in <code>inputParameters</code> using the desired variable names and their respective values.</p>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#example","title":"Example","text":"<p>Suppose in a workflow, we have to store a value in a variable and then later in workflow reuse the value stored in the variable just as we do in programming, in such scenarios <code>Set Variable</code> task can be used.</p> <p>Following is the workflow definition with <code>SET_VARIABLE</code> task.</p> <pre><code>{\n  \"name\": \"Set_Variable_Workflow\",\n  \"description\": \"Set a value to a variable and then reuse it later in the workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"Set_Name\",\n      \"taskReferenceName\": \"Set_Name\",\n      \"type\": \"SET_VARIABLE\",\n      \"inputParameters\": {\n        \"name\": \"Foo\"\n      }\n    },\n    {\n      \"name\": \"Read_Name\",\n      \"taskReferenceName\": \"Read_Name\",\n      \"inputParameters\": {\n        \"saved_name\" : \"${workflow.variables.name}\"\n      },\n      \"type\": \"SIMPLE\"\n    }\n  ],\n  \"restartable\": true,\n  \"ownerEmail\":\"abc@example.com\",\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <p>In the above example, it can be seen that the task <code>Set_Name</code> is a Set Variable Task and the variable <code>name</code> is set to <code>Foo</code> and later in the workflow it is referenced by <code>\"${workflow.variables.name}\"</code> in another task.</p>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html","title":"Start Workflow","text":"<pre><code>\"type\" : \"START_WORKFLOW\"\n</code></pre> <p>The <code>START_WORKFLOW</code> task starts another workflow. Unlike <code>SUB_WORKFLOW</code>, <code>START_WORKFLOW</code> does not create a relationship between starter and the started workflow. It also does not wait for the started workflow to complete. A <code>START_WORKFLOW</code> is  considered successful once the requested workflow is started successfully. In other words, <code>START_WORKFLOW</code> is marked as <code>COMPLETED</code> once the started  workflow is in <code>RUNNING</code> state.</p> <p>There is no ability to access the <code>output</code> of the started workflow.</p>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#use-cases","title":"Use Cases","text":"<p>When another workflow needs to be started from the current workflow, <code>START_WORKFLOW</code> can be used. </p>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#configuration","title":"Configuration","text":"<p>The workflow invocation payload is passed into <code>startWorkflow</code> under <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#inputparameters","title":"inputParameters","text":"name type description startWorkflow Map[String, Any] The value of this parameter is Start Workflow Request."},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#output","title":"Output","text":"name type description workflowId String The id of the started workflow <p>Note: <code>START_WORKFLOW</code> will neither wait for the completion of, nor pass back the <code>output</code> of the spawned workflow.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html","title":"Sub Workflow","text":"<p><pre><code>\"type\" : \"SUB_WORKFLOW\"\n</code></pre> Sub Workflow task allows for nesting a workflow within another workflow. Nested workflows contain a reference to their parent.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#use-cases","title":"Use Cases","text":"<p>Suppose we want to include another workflow inside our current workflow. In that case, Sub Workflow Task would be used.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#configuration","title":"Configuration","text":"<p><code>subWorkflowParam</code> is provided at the top level of the task configuration.</p> name type description subWorkflowParam Map[String, Any] See below <p><code>inputParameters</code> will be passed down to the invoked sub-workflow.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#subworkflowparam","title":"subWorkflowParam","text":"name type description name String Name of the workflow to execute version Integer Version of the workflow to execute taskToDomain Map[String, String] Allows scheduling the sub workflow's tasks per given mappings.  See Task Domains for instructions to configure taskDomains. workflowDefinition WorkflowDefinition Allows starting a subworkflow with a dynamic workflow definition."},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#output","title":"Output","text":"name type description subWorkflowId String Sub-workflow execution Id generated when running the sub-workflow"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#examples","title":"Examples","text":"<p>Imagine we have a workflow that has a fork in it. In the example below, we input one image, but using a fork to create 2 images simultaneously:</p> <p></p> <p>The left fork will create a JPG, and the right fork a WEBP image. Maintaining this workflow might be difficult, as changes made to one side of the fork do not automatically propagate the other.  Rather than using 2 tasks, we can define a <code>image_convert_resize</code> workflow that we can call for both forks as a sub-workflow:</p> <pre><code>{\n    \"name\": \"image_convert_resize_subworkflow1\",\n    \"description\": \"Image Processing Workflow\",\n    \"version\": 1,\n    \"tasks\": [{\n            \"name\": \"image_convert_resize_multipleformat_fork\",\n            \"taskReferenceName\": \"image_convert_resize_multipleformat_ref\",\n            \"inputParameters\": {},\n            \"type\": \"FORK_JOIN\",\n            \"decisionCases\": {},\n            \"defaultCase\": [],\n            \"forkTasks\": [\n                [{\n                    \"name\": \"image_convert_resize_sub\",\n                    \"taskReferenceName\": \"subworkflow_jpg_ref\",\n                    \"inputParameters\": {\n                        \"fileLocation\": \"${workflow.input.fileLocation}\",\n                        \"recipeParameters\": {\n                            \"outputSize\": {\n                                \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                                \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n                            },\n                            \"outputFormat\": \"jpg\"\n                        }\n                    },\n                    \"type\": \"SUB_WORKFLOW\",\n                    \"subWorkflowParam\": {\n                        \"name\": \"image_convert_resize\",\n                        \"version\": 1\n                    }\n                }],\n                [{\n                        \"name\": \"image_convert_resize_sub\",\n                        \"taskReferenceName\": \"subworkflow_webp_ref\",\n                        \"inputParameters\": {\n                            \"fileLocation\": \"${workflow.input.fileLocation}\",\n                            \"recipeParameters\": {\n                                \"outputSize\": {\n                                    \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                                    \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n                                },\n                                \"outputFormat\": \"webp\"\n                            }\n                        },\n                        \"type\": \"SUB_WORKFLOW\",\n                        \"subWorkflowParam\": {\n                            \"name\": \"image_convert_resize\",\n                            \"version\": 1\n                        }\n                    }\n\n                ]\n            ]\n        },\n        {\n            \"name\": \"image_convert_resize_multipleformat_join\",\n            \"taskReferenceName\": \"image_convert_resize_multipleformat_join_ref\",\n            \"inputParameters\": {},\n            \"type\": \"JOIN\",\n            \"decisionCases\": {},\n            \"defaultCase\": [],\n            \"forkTasks\": [],\n            \"startDelay\": 0,\n            \"joinOn\": [\n                \"subworkflow_jpg_ref\",\n                \"upload_toS3_webp_ref\"\n            ],\n            \"optional\": false,\n            \"defaultExclusiveJoinTask\": [],\n            \"asyncComplete\": false,\n            \"loopOver\": []\n        }\n    ],\n    \"inputParameters\": [],\n    \"outputParameters\": {\n        \"fileLocationJpg\": \"${subworkflow_jpg_ref.output.fileLocation}\",\n        \"fileLocationWebp\": \"${subworkflow_webp_ref.output.fileLocation}\"\n    },\n    \"schemaVersion\": 2,\n    \"restartable\": true,\n    \"workflowStatusListenerEnabled\": true,\n    \"ownerEmail\": \"conductor@example.com\",\n    \"timeoutPolicy\": \"ALERT_ONLY\",\n    \"timeoutSeconds\": 0,\n    \"variables\": {},\n    \"inputTemplate\": {}\n}\n</code></pre> <p>Now our diagram will appear as: </p> <p>The inputs to both sides of the workflow are identical before and after - but we've abstracted the tasks into the sub-workflow. Any change to the sub-workflow will automatically occur in bth sides of the fork.</p> <p>Looking at the subworkflow (the WEBP version):</p> <pre><code>{\n    \"name\": \"image_convert_resize_sub\",\n    \"taskReferenceName\": \"subworkflow_webp_ref\",\n    \"inputParameters\": {\n        \"fileLocation\": \"${workflow.input.fileLocation}\",\n        \"recipeParameters\": {\n            \"outputSize\": {\n                \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n            },\n            \"outputFormat\": \"webp\"\n        }\n    },\n    \"type\": \"SUB_WORKFLOW\",\n    \"subWorkflowParam\": {\n        \"name\": \"image_convert_resize\",\n        \"version\": 1\n    }\n}\n</code></pre> <p>The <code>subWorkflowParam</code> tells conductor which workflow to call. The task is marked as completed upon the completion of the spawned workflow.  If the sub-workflow is terminated or fails the task is marked as failure and retried if configured. </p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#optional-sub-workflow-task","title":"Optional Sub Workflow Task","text":"<p>If the Sub Workflow task is defined as optional in the parent workflow task definition, the parent workflow task will not be retried if sub-workflow is terminated or failed. In addition, even if the sub-workflow is retried/rerun/restarted after reaching to a terminal status, the parent workflow task status will remain as it is.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html","title":"Switch","text":"<pre><code>\"type\" : \"SWITCH\"\n</code></pre> <p>A <code>SWITCH</code> task is similar to the <code>switch...case</code> statement in a programming language. Two evaluators are supported. In either case, the <code>expression</code> is evaluated (this could be a simply a referefence to <code>inputParameters</code>, or a more complex Javascript expression), and the appropriate task is executed based on the output of the <code>expression</code>, and the <code>decisionCases</code> defined in the task configuration.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#use-cases","title":"Use Cases","text":"<p>Useful in any situation where we have to execute one of many task options.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#configuration","title":"Configuration","text":"<p>The following parameters are specified at the top level of the task configuration.</p> name type description evaluatorType String (enum) Type of the evaluator used. Supported types: <code>value-param</code>, <code>javascript</code>. expression String Depends on the evaluatorType decisionCases Map[String, List[task]] Map where the keys are the possible values that can result from <code>expression</code> being evaluated by <code>evaluatorType</code> with values being lists of tasks to be executed. defaultCase List[task] List of tasks to be executed when no matching value if found in <code>decisionCases</code> <p>Note: If the evaluation type is <code>value-param</code>, <code>inputParameters</code> must be populated with the key specified in <code>expression</code>. </p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#evaluator-types-and-expression","title":"Evaluator Types and Expression","text":"evaluatorType expression description value-param String (name of key) Reference to provided key in <code>inputParameters</code> javascript String (JavaScript expression) Evaluate JavaScript expressions and compute value"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#output","title":"Output","text":"name type description evaluationResult List[String] A List of string representing the list of cases that matched."},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#examples","title":"Examples","text":"<p>In this example workflow, we have to ship a package with the shipping service providers on the basis of input provided while running the workflow.</p> <p>Let's create a Workflow with the following switch task definition that uses <code>value-param</code> evaluatorType:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"inputParameters\": {\n    \"switchCaseValue\": \"${workflow.input.service}\"\n  },\n  \"type\": \"SWITCH\",\n  \"evaluatorType\": \"value-param\",\n  \"expression\": \"switchCaseValue\",\n  \"defaultCase\": [\n    {\n      ...\n    }\n  ],\n  \"decisionCases\": {\n    \"fedex\": [\n      {\n        ...\n      }\n    ],\n    \"ups\": [\n      {\n        ...\n      }\n    ]\n  }\n}\n</code></pre> <p>In the definition above the value of the parameter <code>switch_case_value</code> is used to determine the switch-case. The evaluator type is <code>value-param</code> and the expression is a direct reference to the name of an input parameter. If the value of <code>switch_case_value</code> is <code>fedex</code> then the decision case <code>ship_via_fedex</code>is executed as shown below.</p> <p></p> <p>In a similar way - if the input was <code>ups</code>, then <code>ship_via_ups</code> will be executed. If none of the cases match then the default option is executed.</p> <p>Here is an example using the <code>javascript</code> evaluator type:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"inputParameters\": {\n    \"inputValue\": \"${workflow.input.service}\"\n  },\n  \"type\": \"SWITCH\",\n  \"evaluatorType\": \"javascript\",\n  \"expression\": \"$.inputValue == 'fedex' ? 'fedex' : 'ups'\",\n  \"defaultCase\": [\n    {\n      ...\n    }\n  ],\n  \"decisionCases\": {\n    \"fedex\": [\n      {\n        ...\n      }\n    ],\n    \"ups\": [\n      {\n        ...\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html","title":"Terminate","text":"<pre><code>\"type\" : \"TERMINATE\"\n</code></pre> <p>The <code>TERMINATE</code> task can terminate the workflow with a given status and set the workflow's output with the provided value.  It can act as a <code>return</code> statement for conditions where you simply want to terminate your workflow. </p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#use-cases","title":"Use Cases","text":"<p>Use it when you want to terminate the workflow without continuing the execution. For example, if you have a decision where the first condition is met, you want to execute some tasks,  otherwise you want to finish your workflow.</p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#configuration","title":"Configuration","text":"<p>The <code>TERMINATE</code> task is configured with the following attributes inside <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#inputparameters","title":"inputParameters","text":"name type description notes terminationStatus String Either <code>COMPLETED</code> or <code>FAILED</code> required workflowOutput Any Workflow output to be set terminationReason String For failed tasks, this reason is recorded and passed to any configured <code>failureWorkflow</code>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#output","title":"Output","text":"name type description output Map The content of <code>workflowOutput</code> from the inputParameters. An empty object if <code>workflowOutput</code> is not set."},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#basic-example","title":"Basic Example","text":"<p>Terminate task is defined directly inside the workflow with type <code>TERMINATE</code>.</p> <pre><code>{\n  \"name\": \"terminate\",\n  \"taskReferenceName\": \"terminate0\",\n  \"inputParameters\": {\n      \"terminationStatus\": \"COMPLETED\",\n      \"workflowOutput\": \"${task0.output}\"\n  },\n  \"type\": \"TERMINATE\",\n  \"startDelay\": 0,\n  \"optional\": false\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#use-with-switch","title":"Use with SWITCH","text":"<p>Let's consider the same example we had in Switch Task.</p> <p>Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of input provided while running the workflow. If the input provided while running workflow does not match with the available shipping providers then the workflow will fail and return. If input provided  matches then it goes ahead.</p> <p>Here is a snippet that shows the default switch case terminating the workflow:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"type\": \"SWITCH\",\n  \"defaultCase\": [\n      {\n      \"name\": \"terminate\",\n      \"taskReferenceName\": \"terminate\",\n      \"type\": \"TERMINATE\",\n      \"inputParameters\": {\n          \"terminationStatus\": \"FAILED\",\n          \"terminationReason\":\"Shipping provider not found.\"\n      }      \n    }\n   ]\n}\n</code></pre> <p>Workflow gets created as shown in the diagram.</p> <p></p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#best-practices","title":"Best Practices","text":"<ol> <li>Include termination reason when terminating the workflow with failure status to make it easy to understand the cause.</li> <li>Include any additional details (e.g. output of the tasks, switch case etc) that helps understand the path taken to termination.</li> </ol>"},{"location":"documentation/configuration/workflowdef/systemtasks/index.html","title":"System Tasks","text":"<p>System Tasks (Workers) are built-in tasks that are general purpose and re-usable. They run on the Conductor servers. Such tasks allow you to get started without having to write custom workers.</p> Task Description Use Case Event Publishing Event Task External eventing system integration. e.g. amqp, sqs, nats HTTP HTTP Task Invoke any HTTP(S) endpoints Inline Code Execution Inline Task Execute arbitrary lightweight javascript code JQ Transform JQ Task Use JQ to transform task input/output Kafka Publish Kafka Task Publish messages to Kafka Wait Wait Task Block until resolved"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html","title":"Event Task","text":"<pre><code>\"type\" : \"EVENT\"\n</code></pre> <p>The <code>EVENT</code> task is a task used to publish an event into one of the supported eventing systems in Conductor.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#use-cases","title":"Use Cases","text":"<p>Consider a use case where at some point in the execution, an event is published to an external eventing system such as SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#supported-queuing-systems","title":"Supported Queuing Systems","text":"<p>Conductor supports the the following eventing models:</p> <ol> <li>Conductor internal events (prefix: <code>conductor</code>)</li> <li>SQS (prefix: <code>sqs</code>)</li> <li>NATS (prefix: <code>nats</code>)</li> <li>AMQP (prefix: <code>amqp_queue or amqp_exchange</code>)</li> </ol>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#configuration","title":"Configuration","text":"<p>The following parameters are specified at the top level of the task configuration.</p> Attribute Description sink External event queue in the format of <code>prefix:location</code>.  Prefix is either <code>sqs</code> or <code>conductor</code> and <code>location</code> specifies the actual queue name. e.g. <code>sqs:send_email_queue</code> asyncComplete Boolean. See below"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#asynccomplete","title":"asyncComplete","text":"<ul> <li><code>false</code> to mark status COMPLETED upon execution </li> <li><code>true</code> to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. </li> </ul>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#conductor-sink","title":"Conductor sink","text":"<p>When producing an event with Conductor as sink, the event name follows the structure: <code>conductor:&lt;workflow_name&gt;:&lt;task_reference_name&gt;</code></p> <p>When using Conductor as sink, you have two options: defining the sink as <code>conductor</code> in which case the queue name will default to the taskReferenceName of the Event Task, or specifying the queue name in the sink, as <code>conductor:&lt;queue_name&gt;</code>. The queue name is in the <code>event</code> value of the event Handler, as <code>conductor:&lt;workflow_name&gt;:&lt;queue_name&gt;</code>.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#sqs-sink","title":"SQS sink","text":"<p>For SQS, use the name of the queue and NOT the URI.  Conductor looks up the URI based on the name.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#output","title":"Output","text":"<p>Tasks's output are sent as a payload to the external event. In case of SQS the task's output is sent to the SQS message a a payload.</p> name type description workflowInstanceId String Workflow id workflowType String Workflow Name workflowVersion Integer Workflow Version correlationId String Workflow CorrelationId sink String Copy of the input data \"sink\" asyncComplete Boolean Copy of the input data \"asyncComplete event_produced String Name of the event produced <p>The published event's payload is identical to the output of the task (except \"event_produced\").</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#examples","title":"Examples","text":"<p>Consider an example where we want to publish an event into SQS to notify an external system. </p> <pre><code>{\n    \"type\": \"EVENT\",\n    \"sink\": \"sqs:sqs_queue_name\",\n    \"asyncComplete\": false\n}\n</code></pre> <p>An example where we want to publish a messase to conductor's internal queuing system. <pre><code>{\n    \"type\": \"EVENT\",\n    \"sink\": \"conductor:internal_event_name\",\n    \"asyncComplete\": false\n}\n</code></pre></p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html","title":"HTTP Task","text":"<pre><code>\"type\" : \"HTTP\"\n</code></pre> <p>The <code>HTTP</code> task is useful when you have a requirements such as:</p> <ol> <li>Making calls to another service that exposes an API via HTTP</li> <li>Fetch any resource or data present on an endpoint</li> </ol> <p>The <code>HTTP</code> task is moved to <code>COMPLETED</code> status once the remote service responds successfully.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#use-cases","title":"Use Cases","text":"<p>If we have a scenario where we need to make an HTTP call into another service, we can make use of HTTP tasks. You can use the data returned from the HTTP call in your subsequent tasks as inputs. Using HTTP tasks you can avoid having to write the code that talks to these services and instead let Conductor manage it directly. This can reduce the code you have to maintain and allows for a lot of flexibility.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#configuration","title":"Configuration","text":"<p>HTTP task is configured using the following key inside the <code>inputParameters</code>  of a task with type <code>HTTP</code>.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#inputparameters","title":"inputParameters","text":"name type description http_request HttpRequest JSON object (see below)"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#http_request","title":"http_request","text":"Name Type Description uri String URI for the service. Can be a partial when using vipAddress or includes the server address. method String HTTP method. GET, PUT, POST, DELETE, OPTIONS, HEAD accept String Accept header. Default:  <code>application/json</code> contentType String Content Type - supported types are <code>text/plain</code>, <code>text/html</code>, and <code>application/json</code> (Default) headers Map[String, Any] A map of additional http headers to be sent along with the request. body Map[] Request body asyncComplete Boolean <code>false</code> to mark status COMPLETED upon execution ; <code>true</code> to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. connectionTimeOut Integer Connection Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 100. readTimeOut Integer Read Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 150. <p>Tip</p> <p>In the case that remote service sends an asynchronous event to signal the completion of the request, consider setting the <code>asyncComplete</code> flag on the HTTP task to <code>true</code>. In this case, you will need to transition the HTTP task to COMPLETED manually.</p> <p>Tip</p> <p>If the remote address that you are connecting to is a secure location, add the Authorization header with <code>Bearer &lt;access_token&gt;</code> to headers.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#output","title":"Output","text":"name type description response Map JSON body containing the response if one is present headers Map[String, Any] Response Headers statusCode Integer Http Status Code reasonPhrase String Http Status Code's reason phrase"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#get-method","title":"GET Method","text":"<p>We can use variables in our URI as show in the example below. </p> <pre><code>{\n  \"name\": \"Get Example\",\n  \"taskReferenceName\": \"get_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/${workflow.input.queryid}\",\n      \"method\": \"GET\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#post-method","title":"POST Method","text":"<p>Here we are using variables for our POST body which happens to be data from a previous task. This is an example of how you can chain HTTP calls to make complex flows happen without writing any additional code.</p> <pre><code>{\n  \"name\": \"http_post_example\",\n  \"taskReferenceName\": \"post_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/\",\n      \"method\": \"POST\",\n      \"body\": {\n        \"title\": \"${get_example.output.response.body.title}\",\n        \"userId\": \"${get_example.output.response.body.userId}\",\n        \"action\": \"doSomething\"\n      }\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#put-method","title":"PUT Method","text":"<pre><code>{\n  \"name\": \"http_put_example\",\n  \"taskReferenceName\": \"put_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\",\n      \"method\": \"PUT\",\n      \"body\": {\n        \"title\": \"${get_example.output.response.body.title}\",\n        \"userId\": \"${get_example.output.response.body.userId}\",\n        \"action\": \"doSomethingDifferent\"\n      }\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#delete-method","title":"DELETE Method","text":"<pre><code>{\n  \"name\": \"DELETE Example\",\n  \"taskReferenceName\": \"delete_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\",\n      \"method\": \"DELETE\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#isolation-groups","title":"Isolation Groups","text":"<p>Why are my HTTP tasks not getting picked up?</p> <p>We might have too many HTTP tasks in the queue. There is a concept called Isolation Groups that you can rely on for prioritizing certain HTTP tasks over others. </p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html","title":"Human Task","text":"<pre><code>\"type\" : \"HUMAN\"\n</code></pre> <p>The <code>HUMAN</code> task is used when the workflow needs to be paused for an external signal to continue. It acts as a gate that  remains in the <code>IN_PROGRESS</code> state until marked as <code>COMPLETED</code> or <code>FAILED</code> by an external trigger.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#use-cases","title":"Use Cases","text":"<p>The HUMAN is can be used when the workflow needs to pause and wait for human intervention, such as manual approval. It can also be used with an event coming from external source such as Kafka, SQS or Conductor's internal queueing mechanism.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#configuration","title":"Configuration","text":"<p>No parameters are required</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#completing","title":"Completing","text":""},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#task-update-api","title":"Task Update API","text":"<p>To conclude a <code>HUMAN</code> task, the <code>POST {{ api_prefix }}/tasks</code> API can be used.</p> <p>You'll need to provide the<code>taskId</code>, the task status (generally <code>COMPLETED</code> or <code>FAILED</code>), and the desired task output.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#event-handler","title":"Event Handler","text":"<p>If SQS integration is enabled, the <code>HUMAN</code> task can also be resolved using the <code>{{ api_prefix }}/queue</code> API.</p> <p>You'll need the  <code>workflowId</code> and <code>taskRefName</code> or <code>taskId</code>.</p> <ol> <li>POST <code>{{ api_prefix }}/queue/update/{workflowId}/{taskRefName}/{status}</code> </li> <li>POST <code>{{ api_prefix }}/queue/update/{workflowId}/task/{taskId}/{status}</code> </li> </ol> <p>An event handler using the <code>complete_task</code> action can also be configured.</p> <p>Any parameter that is sent in the body of the POST message will be repeated as the output of the task.  For example, if we send a COMPLETED message as follows:</p> <pre><code>curl -X \"POST\" \"{{ server_host }}{{ api_prefix }}/queue/update/{workflowId}/waiting_around_ref/COMPLETED\" -H 'Content-Type: application/json' -d '{\"data_key\":\"somedatatoWait1\",\"data_key2\":\"somedatatoWAit2\"}'\n</code></pre> <p>The output of the task will be:</p> <pre><code>{\n  \"data_key\":\"somedatatoWait1\",\n  \"data_key2\":\"somedatatoWAit2\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html","title":"Inline Task","text":"<pre><code>\"type\": \"INLINE\"\n</code></pre> <p>The <code>INLINE</code> task helps execute necessary logic at workflow runtime, using an evaluator. There are two supported evaluators as of now:</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#configuration","title":"Configuration","text":"<p>The <code>INLINE</code> task is configured by specifying the following keys inside <code>inputParameters</code>, along side any other input values required for the evaluation.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#inputparameters","title":"inputParameters","text":"Name Type Description Notes evaluatorType String Type of the evaluator. Supported evaluators: <code>value-param</code>, <code>javascript</code> which evaluates javascript expression. Must be non-empty. expression String Expression associated with the type of evaluator. For <code>javascript</code> evaluator, Javascript evaluation engine is used to evaluate expression defined as a string. Must return a value. Must be non-empty. <p>Besides <code>expression</code>, any value is accessible as <code>$.value</code> for the <code>expression</code> to evaluate.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#outputs","title":"Outputs","text":"Name Type Description result Map Contains the output returned by the evaluator based on the <code>expression</code>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#example-1","title":"Example 1","text":"<pre><code>{\n  \"name\": \"INLINE_TASK\",\n  \"taskReferenceName\": \"inline_test\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n      \"inlineValue\": \"${workflow.input.inlineValue}\",\n      \"evaluatorType\": \"javascript\",\n      \"expression\": \"function scriptFun(){if ($.inlineValue == 1){ return {testvalue: true} } else { return\n      {testvalue: false} }} scriptFun();\"\n  }\n}\n</code></pre> <p>The task output can then be referenced in downstream tasks using an expression: <code>\"${inline_test.output.result.testvalue}\"</code></p> <p>Note</p> <p>The JavaScript evaluator accepts JS code written to the ECMAScript 5.1(ES5) standard</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#example-2","title":"Example 2","text":"<p>Perhaps a weather API sometimes returns Celcius, and sometimes returns Farenheit temperature values.  This task ensures that the downstream tasks ONLY receive Celcius values:</p> <pre><code>{\n  \"name\": \"INLINE_TASK\",\n  \"taskReferenceName\": \"inline_test\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n      \"scale\": \"${workflow.input.tempScale}\",\n        \"temperature\": \"${workflow.input.temperature}\",\n      \"evaluatorType\": \"javascript\",\n      \"expression\": \"function SIvaluesOnly(){if ($.scale === \"F\"){ centigrade = ($.temperature -32)*5/9; return {temperature: centigrade} } else { return \n      {temperature: $.temperature} }} SIvaluesOnly();\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html","title":"JSON JQ Transform Task","text":"<pre><code>\"type\" : \"JSON_JQ_TRANSFORM\"\n</code></pre> <p><code>JSON_JQ_TRANSFORM</code> is a System task that allows processing of JSON data that is supplied to the task, by using the popular JQ processing tool\u2019s query expression language.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#use-cases","title":"Use Cases","text":"<p>JSON is a popular format of choice for data-interchange. It is widely used in web and server applications, document storage, API I/O etc. It\u2019s also used within Conductor to define workflow and task definitions and passing data and state between tasks and workflows. This makes a tool like JQ a natural fit for processing task related data. Some common usages within Conductor includes, working with HTTP task, JOIN tasks or standalone tasks that try to transform data from the output of one task to the input of another.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#configuration","title":"Configuration","text":"<p><code>queryExpression</code> is appended to the <code>inputParameters</code> of <code>JSON_JQ_TRANSFORM</code>, along side any other input values needed for the evaluation.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#inputparameters","title":"inputParameters","text":"name description queryExpression JQ query expression"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#about-jq","title":"About JQ","text":"<p>Check out the JQ Manual.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#output","title":"Output","text":"Attribute Description result The first results returned by the JQ expression resultList A List of results returned by the JQ expression error An optional error message, indicating that the JQ query failed processing"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#example","title":"Example","text":""},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#example-1","title":"Example 1","text":"<p>Here is an example of a <code>JSON_JQ_TRANSFORM</code> task. The <code>inputParameters</code> attribute is expected to have a value object that has the following</p> <ol> <li> <p>A list of key value pair objects denoted key1/value1, key2/value2 in the example below. Note the key1/value1 are    arbitrary names used in this example.</p> </li> <li> <p>A key with the name <code>queryExpression</code>, whose value is a JQ expression. The expression will operate on the value of    the <code>inputParameters</code> attribute. In the example below, the <code>inputParameters</code> has 2 inner objects named by attributes    <code>key1</code> and <code>key2</code>, each of which has an object that is named <code>value1</code> and <code>value2</code>. They have an associated array of    strings as values, <code>\"a\", \"b\"</code> and <code>\"c\", \"d\"</code>. The expression <code>key3: (.key1.value1 + .key2.value2)</code> concat's the 2    string arrays into a single array against an attribute named <code>key3</code></p> </li> </ol> <pre><code>{\n  \"name\": \"jq_example_task\",\n  \"taskReferenceName\": \"my_jq_example_task\",\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"inputParameters\": {\n    \"key1\": {\n      \"value1\": [\n        \"a\",\n        \"b\"\n      ]\n    },\n    \"key2\": {\n      \"value2\": [\n        \"c\",\n        \"d\"\n      ]\n    },\n    \"queryExpression\": \"{ key3: (.key1.value1 + .key2.value2) }\"\n  }\n}\n</code></pre> <p>The execution of this example task above will provide the following output. The <code>resultList</code> attribute stores the full list of the <code>queryExpression</code> result. The <code>result</code> attribute stores the first element of the resultList. An optional <code>error</code> attribute along with a string message will be returned if there was an error processing the query expression.</p> <pre><code>{\n  \"result\": {\n    \"key3\": [\n      \"a\",\n      \"b\",\n      \"c\",\n      \"d\"\n    ]\n  },\n  \"resultList\": [\n    {\n      \"key3\": [\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#example-2","title":"Example 2","text":"<p>A HTTP Task makes an API call to GitHub to request a list of \"stargazers\" (users who have starred a repository).  The API response (for just one user) looks like:</p> <p>Snippet of <code>${hundred_stargazers_ref.output}</code></p> <pre><code>\"body\":[\n  {\n  \"starred_at\":\"2016-12-14T19:55:46Z\",\n  \"user\":{\n    \"login\":\"lzehrung\",\n    \"id\":924226,\n    \"node_id\":\"MDQ6VXNlcjkyNDIyNg==\",\n    \"avatar_url\":\"https://avatars.githubusercontent.com/u/924226?v=4\",\n    \"gravatar_id\":\"\",\n    \"url\":\"https://api.github.com/users/lzehrung\",\n    \"html_url\":\"https://github.com/lzehrung\",\n    \"followers_url\":\"https://api.github.com/users/lzehrung/followers\",\n    \"following_url\":\"https://api.github.com/users/lzehrung/following{/other_user}\",\n    \"gists_url\":\"https://api.github.com/users/lzehrung/gists{/gist_id}\",\n    \"starred_url\":\"https://api.github.com/users/lzehrung/starred{/owner}{/repo}\",\n    \"subscriptions_url\":\"https://api.github.com/users/lzehrung/subscriptions\",\n    \"organizations_url\":\"https://api.github.com/users/lzehrung/orgs\",\n    \"repos_url\":\"https://api.github.com/users/lzehrung/repos\",\n    \"events_url\":\"https://api.github.com/users/lzehrung/events{/privacy}\",\n    \"received_events_url\":\"https://api.github.com/users/lzehrung/received_events\",\n    \"type\":\"User\",\n    \"site_admin\":false\n  }\n}\n]\n</code></pre> <p>We only need the <code>starred_at</code> and <code>login</code> parameters for users who starred the repository AFTER a given date (provided as an input to the workflow <code>${workflow.input.cutoff_date}</code>).  We'll use the JQ Transform to simplify the output:</p> <pre><code>{\n  \"name\": \"jq_cleanup_stars\",\n  \"taskReferenceName\": \"jq_cleanup_stars_ref\",\n  \"inputParameters\": {\n    \"starlist\": \"${hundred_stargazers_ref.output.response.body}\",\n    \"queryExpression\": \"[.starlist[] | select (.starred_at &gt; \\\"${workflow.input.cutoff_date}\\\") |{occurred_at:.starred_at, member: {github:  .user.login}}]\"\n  },\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"decisionCases\": {},\n  \"defaultCase\": [],\n  \"forkTasks\": [],\n  \"startDelay\": 0,\n  \"joinOn\": [],\n  \"optional\": false,\n  \"defaultExclusiveJoinTask\": [],\n  \"asyncComplete\": false,\n  \"loopOver\": []\n}\n</code></pre> <p>The JSON is stored in <code>starlist</code>.  The <code>queryExpression</code> reads in the JSON, selects only entries where the <code>starred_at</code> value meets the date criteria, and generates output JSON of the form:</p> <pre><code>{\n  \"occurred_at\": \"date from JSON\",\n  \"member\":{\n    \"github\" : \"github Login from JSON\"\n  }\n}\n</code></pre> <p>The entire expression is wrapped in <code>[]</code> to indicate that the response should be an array.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html","title":"Kafka Publish Task","text":"<p><pre><code>\"type\" : \"KAFKA_PUBLISH\"\n</code></pre> The <code>KAFKA_PUBLISH</code> task is used to push messages to another microservice via Kafka.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#configuration","title":"Configuration","text":"<p>The task expects a field named <code>kafka_request</code> as part of the task's <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#inputparameters","title":"inputParameters","text":"name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka.  One of the following can be set : 1. <code>org.apache.kafka.common.serialization.IntegerSerializer</code>2. <code>org.apache.kafka.common.serialization.LongSerializer</code>3. <code>org.apache.kafka.common.serialization.StringSerializer</code>. Default is <code>StringSerializer</code> value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property <code>kafka.publish.request.timeout.ms</code>. If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property <code>kafka.publish.max.block.ms</code>. If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#task-output","title":"Task Output","text":"<p>Task status transitions to <code>COMPLETED</code> on success.</p> <p>The task is marked as <code>FAILED</code> if the message could not be published to the Kafka queue.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#example","title":"Example","text":"<pre><code>{\n  \"name\": \"call_kafka\",\n  \"taskReferenceName\": \"call_kafka\",\n  \"inputParameters\": {\n    \"kafka_request\": {\n      \"topic\": \"userTopic\",\n      \"value\": \"Message to publish\",\n      \"bootStrapServers\": \"localhost:9092\",\n      \"headers\": {\n    \"x-Auth\":\"Auth-key\"    \n      },\n      \"key\": \"123\",\n      \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\"\n    }\n  },\n  \"type\": \"KAFKA_PUBLISH\"\n}\n</code></pre> <p>The task expects an input parameter named <code>\"kafka_request\"</code> as part of the task's input with the following details:</p> <ol> <li><code>\"bootStrapServers\"</code> - bootStrapServers for connecting to given kafka.</li> <li><code>\"key\"</code> - Key to be published.</li> <li><code>\"keySerializer\"</code> - Serializer used for serializing the key published to kafka.  One of the following can be set : a. org.apache.kafka.common.serialization.IntegerSerializer b. org.apache.kafka.common.serialization.LongSerializer c. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer.</li> <li><code>\"value\"</code> - Value published to kafka</li> <li><code>\"requestTimeoutMs\"</code> - Request timeout while publishing to kafka.  If this value is not given the value is read from the property  kafka.publish.request.timeout.ms. If the property is not set the value defaults to 100 ms.</li> <li><code>\"maxBlockMs\"</code> - maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms. If the property is not set the value defaults to 500 ms.</li> <li><code>\"headers\"</code> - A map of additional kafka headers to be sent along with the request.</li> <li><code>\"topic\"</code> - Topic to publish.</li> </ol> <p>The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified  kafka.publish.producer.cache.size, kafka.publish.producer.cache.time.ms respectively.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html","title":"Wait Task","text":"<p>The WAIT task is a no-op task that will remain <code>IN_PROGRESS</code> until after a certain duration or timestamp, at which point it will be marked as <code>COMPLETED</code>.</p> <pre><code>\"type\" : \"WAIT\"\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#configuration","title":"Configuration","text":"<p>The <code>WAIT</code> task is configured using either <code>duration</code> or <code>until</code> in <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#inputparameters","title":"inputParameters","text":"name type description duration String Duration to wait for until String Timestamp to wait until"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#wait-for-time-duration","title":"Wait For time duration","text":"<p>Format duration as <code>XhYmZs</code>, using the <code>duration</code> key.</p> <pre><code>{\n    \"type\": \"WAIT\",\n    \"inputParameters\": {\n        \"duration\": \"10m20s\"\n    }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#wait-until-specific-datetime","title":"Wait until specific date/time","text":"<p>Specify the timestamp using one of the formats, using the <code>until</code> key.</p> <ol> <li><code>yyyy-MM-dd HH:mm</code></li> <li><code>yyyy-MM-dd HH:mm z</code></li> <li><code>yyyy-MM-dd</code></li> </ol> <pre><code>{\n    \"type\": \"WAIT\",\n    \"inputParameters\": {\n        \"until\": \"2022-12-31 11:59\"\n    }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#external-triggers","title":"External Triggers","text":"<p>The task endpoint <code>POST {{ api_prefix }}/tasks</code> can be used to update the status of a task to COMPLETED prior to the configured timeout. This is same technique as prescribed for the HUMAN task.</p> <p>For cases where no timeout is necessary it is recommended that you use the HUMAN task directly.</p>"},{"location":"documentation/metrics/client.html","title":"Client Metrics","text":"<p>When using the Java client, the following metrics are published:</p> Name Purpose Tags task_execution_queue_full Counter to record execution queue has saturated taskType task_poll_error Client error when polling for a task queue taskType, includeRetries, status task_paused Counter for number of times the task has been polled, when the worker has been paused taskType task_execute_error Execution error taskType task_ack_failed Task ack failed taskType task_ack_error Task ack has encountered an exception taskType task_update_error Task status cannot be updated back to server taskType task_poll_counter Incremented each time polling is done taskType task_poll_time Time to poll for a batch of tasks taskType task_execute_time Time to execute a task taskType task_result_size Records output payload size of a task taskType workflow_input_size Records input payload size of a workflow workflowType, workflowVersion external_payload_used Incremented each time external payload storage is used name, operation, payloadType <p>Metrics on client side supplements the one collected from server in identifying the network as well as client side issues.</p>"},{"location":"documentation/metrics/server.html","title":"Server Metrics","text":"<p>Conductor uses spectator to collect the metrics.</p> <ul> <li>To enable conductor serve to publish metrics, add this dependency to your build.gradle.</li> <li>Create your own AbstractModule that overides configure function and registers the Spectator metrics registry.</li> <li>Initialize the Registry and add it to the global registry via <code>((CompositeRegistry)Spectator.globalRegistry()).add(...)</code>.</li> </ul> <p>The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks.</p> Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType"},{"location":"documentation/metrics/server.html#collecting-metrics-with-log4j","title":"Collecting metrics with Log4j","text":"<p>One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel.</p> <p>Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties:</p> <pre><code>conductor.metrics-logger.enabled = true\nconductor.metrics-logger.reportPeriodSeconds = 15\n</code></pre> <p>This will push all available metrics into log4j every 15 seconds.</p> <p>By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file:</p> <pre><code>log4j.rootLogger=INFO,console,file\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=/app/logs/conductor.log\nlog4j.appender.file.MaxFileSize=10MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n# Dedicated file appender for metrics\nlog4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender\nlog4j.appender.fileMetrics.File=/app/logs/metrics.log\nlog4j.appender.fileMetrics.MaxFileSize=10MB\nlog4j.appender.fileMetrics.MaxBackupIndex=10\nlog4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout\nlog4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\nlog4j.logger.ConductorMetrics=INFO,console,fileMetrics\nlog4j.additivity.ConductorMetrics=false\n</code></pre> <p>This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var:</p> <pre><code>LOG4J_PROP=log4j-file-appender.properties\n</code></pre> <p>This variable is used by startup.sh script.</p>"},{"location":"documentation/metrics/server.html#integration-with-logstash-using-a-log-file","title":"Integration with logstash using a log file","text":"<p>The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender -&gt; logstash -&gt; ElasticSearch.</p> <p>Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch.</p> <p>Following configuration needs to be used in logstash to achieve it:</p> <p>pipeline.yml:</p> <pre><code>- pipeline.id: conductor_metrics\n  path.config: \"/usr/share/logstash/pipeline/logstash_metrics.conf\"\n  pipeline.workers: 2\n</code></pre> <p>logstash_metrics.conf</p> <pre><code>input {\n\n file {\n  path =&gt; [\"/conductor-server-logs/metrics.log\"]\n  codec =&gt; multiline {\n      pattern =&gt; \"^%{TIMESTAMP_ISO8601} \"\n      negate =&gt; true\n      what =&gt; previous\n  }\n }\n}\n\nfilter {\n    kv {\n        field_split =&gt; \", \"\n        include_keys =&gt; [ \"name\", \"type\", \"count\", \"value\" ]\n    }\n    mutate {\n        convert =&gt; {\n          \"count\" =&gt; \"integer\"\n          \"value\" =&gt; \"float\"\n        }\n      }\n}\n\noutput {\n elasticsearch {\n  hosts =&gt; [\"elasticsearch:9200\"]\n }\n}\n</code></pre> <p>Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types</p>"},{"location":"documentation/metrics/server.html#integration-with-fluentd-using-a-syslog-channel","title":"Integration with fluentd using a syslog channel","text":"<p>Another example of metrics collection uses: log4j syslog appender -&gt; fluentd -&gt; prometheus.</p> <p>In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel:</p> <pre><code>    log4j.rootLogger=INFO,console,file\n\n    log4j.appender.console=org.apache.log4j.ConsoleAppender\n    log4j.appender.console.layout=org.apache.log4j.PatternLayout\n    log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    log4j.appender.file=org.apache.log4j.RollingFileAppender\n    log4j.appender.file.File=/app/logs/conductor.log\n    log4j.appender.file.MaxFileSize=10MB\n    log4j.appender.file.MaxBackupIndex=10\n    log4j.appender.file.layout=org.apache.log4j.PatternLayout\n    log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    # Syslog based appender streaming metrics into fluentd\n    log4j.appender.server=org.apache.log4j.net.SyslogAppender\n    log4j.appender.server.syslogHost=fluentd:5170\n    log4j.appender.server.facility=LOCAL1\n    log4j.appender.server.layout=org.apache.log4j.PatternLayout\n    log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    log4j.logger.ConductorMetrics=INFO,console,server\n    log4j.additivity.ConductorMetrics=false\n</code></pre> <p>And on the fluentd side you need following configuration:</p> <pre><code>    &lt;source&gt;\n      @type prometheus\n    &lt;/source&gt;\n\n    &lt;source&gt;\n      @type syslog\n      port 5170\n      bind 0.0.0.0\n      tag conductor\n        &lt;parse&gt;\n         ; only allow TIMER metrics of workflow execution and extract tenant ID\n          @type regexp\n          expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(?&lt;tenant&gt;.*)_(?&lt;workflow&gt;.+), count=(?&lt;count&gt;\\d+), min=(?&lt;min&gt;[\\d.]+), max=(?&lt;max&gt;[\\d.]+), mean=(?&lt;mean&gt;[\\d.]+).*$/\n          types count:integer,min:float,max:float,mean:float\n        &lt;/parse&gt;\n    &lt;/source&gt;\n\n    &lt;filter conductor.local1.info&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name conductor_workflow_count\n          type gauge\n          desc The total number of executed workflows\n          key count\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n        &lt;metric&gt;\n          name conductor_workflow_max_duration\n          type gauge\n          desc Max duration in millis for a workflow\n          key max\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n        &lt;metric&gt;\n          name conductor_workflow_mean_duration\n          type gauge\n          desc Mean duration in millis for a workflow\n          key mean\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n    &lt;/filter&gt;\n\n    &lt;match **&gt;\n      @type stdout\n    &lt;/match&gt;\n</code></pre> <p>With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics </p>"},{"location":"documentation/metrics/server.html#collecting-metrics-with-prometheus","title":"Collecting metrics with Prometheus","text":"<p>Another way to collect metrics is using Prometheus client to push them to Prometheus server.</p> <p>Conductor provides optional modules that connect metrics registry with Prometheus. To enable these modules, configure following additional module property in config.properties:</p> <pre><code>conductor.metrics-prometheus.enabled = true\n</code></pre> <p>This will simply push these metrics via Prometheus collector. However, you need to configure your own Prometheus collector and expose the metrics via an endpoint.</p>"},{"location":"resources/contributing.html","title":"Contributing","text":"<p>Thanks for your interest in Conductor! This guide helps to find the most efficient way to contribute, ask questions, and report issues.</p>"},{"location":"resources/contributing.html#code-of-conduct","title":"Code of conduct","text":"<p>Please review our code of conduct.</p>"},{"location":"resources/contributing.html#i-have-a-question","title":"I have a question!","text":"<p>We have a dedicated discussion forum for asking \"how to\" questions and to discuss ideas. The discussion forum is a great place to start if you're considering creating a feature request or work on a Pull Request. Please do not create issues to ask questions.</p>"},{"location":"resources/contributing.html#i-want-to-contribute","title":"I want to contribute!","text":"<p>We welcome Pull Requests and already had many outstanding community contributions! Creating and reviewing Pull Requests take considerable time. This section helps you to set up a smooth Pull Request experience.</p> <p>The stable branch is main.</p> <p>Please create pull requests for your contributions against main only.</p> <p>It's a great idea to discuss the new feature you're considering on the discussion forum before writing any code. There are often different ways you can implement a feature. Getting some discussion about different options helps shape the best solution. When starting directly with a Pull Request, there is the risk of having to make considerable changes. Sometimes that is the best approach, though! Showing an idea with code can be very helpful; be aware that it might be throw-away work. Some of our best Pull Requests came out of multiple competing implementations, which helped shape it to perfection.</p> <p>Also, consider that not every feature is a good fit for Conductor. A few things to consider are:</p> <ul> <li>Is it increasing complexity for the user, or might it be confusing?</li> <li>Does it, in any way, break backward compatibility (this is seldom acceptable)</li> <li>Does it require new dependencies (this is rarely acceptable for core modules)</li> <li>Should the feature be opt-in or enabled by default. For integration with a new Queuing recipe or persistence module, a separate module which can be optionally enabled is the right choice.  </li> <li>Should the feature be implemented in the main Conductor repository, or would it be better to set up a separate repository? Especially for integration with other systems, a separate repository is often the right choice because the life-cycle of it will be different.</li> </ul> <p>Of course, for more minor bug fixes and improvements, the process can be more light-weight.</p> <p>We'll try to be responsive to Pull Requests. Do keep in mind that because of the inherently distributed nature of open source projects, responses to a PR might take some time because of time zones, weekends, and other things we may be working on.</p>"},{"location":"resources/contributing.html#i-want-to-report-an-issue","title":"I want to report an issue","text":"<p>If you found a bug, it is much appreciated if you create an issue. Please include clear instructions on how to reproduce the issue, or even better, include a test case on a branch. Make sure to come up with a descriptive title for the issue because this helps while organizing issues.</p>"},{"location":"resources/contributing.html#i-have-a-great-idea-for-a-new-feature","title":"I have a great idea for a new feature","text":"<p>Many features in Conductor have come from ideas from the community. If you think something is missing or certain use cases could be supported better, let us know! You can do so by opening a discussion on the discussion forum. Provide as much relevant context to why and when the feature would be helpful. Providing context is especially important for \"Support XYZ\" issues since we might not be familiar with what \"XYZ\" is and why it's useful. If you have an idea of how to implement the feature, include that as well.</p> <p>Once we have decided on a direction, it's time to summarize the idea by creating a new issue.</p>"},{"location":"resources/contributing.html#code-style","title":"Code Style","text":"<p>We use spotless to enforce consistent code style for the project, so make sure to run <code>gradlew spotlessApply</code> to fix any violations after code changes.</p>"},{"location":"resources/contributing.html#license","title":"License","text":"<p>By contributing your code, you agree to license your contribution under the terms of the APLv2: https://github.com/conductor-oss/conductor/blob/main/LICENSE</p> <p>All files are released with the Apache 2.0 license, and the following license header will be automatically added to your new file if none present:</p> <pre><code>/**\n * Copyright $YEAR Conductor authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n */\n</code></pre>"},{"location":"resources/license.html","title":"License","text":"<p>Copyright 2023 Conductor authors.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"resources/related.html","title":"Community projects related to Conductor","text":""},{"location":"resources/related.html#client-sdks","title":"Client SDKs","text":"<p>Further, all of the (non-Java) SDKs have a new GitHub home: the Conductor SDK repository is your new source for Conductor SDKs:</p> <ul> <li>Golang</li> <li>Python</li> <li>C#</li> <li>Clojure</li> </ul> <p>All contributions on the above client sdks can be made on Conductor SDK repository.</p>"},{"location":"resources/related.html#microservices-operations","title":"Microservices operations","text":"<ul> <li> <p>https://github.com/flaviostutz/schellar - Schellar is a scheduler tool for instantiating Conductor workflows from time to time, mostly like a cron job, but with transport of input/output variables between calls.</p> </li> <li> <p>https://github.com/flaviostutz/backtor - Backtor is a backup scheduler tool that uses Conductor workers to handle backup operations and decide when to expire backups (ex.: keep backup 3 days, 2 weeks, 2 months, 1 semester)</p> </li> <li> <p>https://github.com/cquon/conductor-tools - Conductor CLI for launching workflows, polling tasks, listing running tasks etc</p> </li> </ul>"},{"location":"resources/related.html#conductor-deployment","title":"Conductor deployment","text":"<ul> <li> <p>https://github.com/flaviostutz/conductor-server - Docker container for running Conductor with  Prometheus metrics plugin installed and some tweaks to ease provisioning of workflows from json files embedded to the container</p> </li> <li> <p>https://github.com/flaviostutz/conductor-ui - Docker container for running Conductor UI so that you can easily scale UI independently</p> </li> <li> <p>https://github.com/flaviostutz/elasticblast - \"Elasticsearch to Bleve\" bridge tailored for running Conductor on top of Bleve indexer. The footprint of Elasticsearch may cost too much for small deployments on Cloud environment.</p> </li> <li> <p>https://github.com/mohelsaka/conductor-prometheus-metrics - Conductor plugin for exposing Prometheus metrics over path '/metrics'</p> </li> </ul>"},{"location":"resources/related.html#oauth20-security-configuration","title":"OAuth2.0 Security Configuration","text":"<p>Forked Repository - Conductor (Secure)</p> <p>OAuth2.0 Role Based Security! - Spring Security with easy configuration to secure the Conductor server APIs.</p> <p>Docker image published to Docker Hub</p>"},{"location":"resources/related.html#conductor-worker-utilities","title":"Conductor Worker utilities","text":"<ul> <li> <p>https://github.com/ggrcha/conductor-go-client - Conductor Golang client for writing Workers in Golang</p> </li> <li> <p>https://github.com/courosh12/conductor-dotnet-client - Conductor DOTNET client for writing Workers in DOTNET</p> </li> <li> <p>https://github.com/TwoUnderscorez/serilog-sinks-conductor-task-log - Serilog sink for sending worker log events to Conductor</p> </li> <li> <p>https://github.com/davidwadden/conductor-workers - Various ready made Conductor workers for common operations on some platforms (ex.: Jira, Github, Concourse)</p> </li> </ul>"},{"location":"resources/related.html#conductor-web-ui","title":"Conductor Web UI","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor-ng-ui - Angular based - Conductor Workflow Management UI</li> </ul>"},{"location":"resources/related.html#conductor-persistence","title":"Conductor Persistence","text":""},{"location":"resources/related.html#mongo-persistence","title":"Mongo Persistence","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor/tree/mongo_persistence - With option to use Mongo Database as persistence unit.</li> <li>Mongo Persistence / Option to use Mongo Database as persistence unit.</li> <li>Docker Compose example with MongoDB Container.</li> </ul>"},{"location":"resources/related.html#oracle-persistence","title":"Oracle Persistence","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor/tree/oracle_persistence - With option to use Oracle Database as persistence unit.</li> <li>Oracle Persistence / Option to use Oracle Database as persistence unit : version &gt; 12.2 - Tested well with 19C</li> <li>Docker Compose example with Oracle Container.</li> </ul>"},{"location":"resources/related.html#schedule-conductor-workflow","title":"Schedule Conductor Workflow","text":"<ul> <li>https://github.com/jas34/scheduledwf - It solves the following problem statements:<ul> <li>At times there are use cases in which we need to run some tasks/jobs only at a scheduled time.</li> <li>In microservice architecture maintaining schedulers in various microservices is a pain.</li> <li>We should have a central dedicate service that can do scheduling for us and provide a trigger to a microservices at expected time.</li> </ul> </li> <li>It offers an additional module <code>io.github.jas34.scheduledwf.config.ScheduledWfServerModule</code> built on the existing core  of conductor and does not require deployment of any additional service. For more details refer: Schedule Conductor Workflows and Capability In Conductor To Schedule Workflows</li> </ul>"}]}