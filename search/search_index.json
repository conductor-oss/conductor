{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"Scalable Workflow Orchestration                 Conductor is a platform originally created at Netflix to orchestrate workflows that span across microservices.        Get Started  Open Source                 Apache-2.0 license for commercial and non-commerical use. Freedom to deploy, modify and contribute back.         Modular                 A fully abstracted backend enables you choose your own database persistence layer and queueing service.         Proven                         Enterprise ready, Java Spring based platform that has been battle tested in production systems at Netflix and elsewhere.         Control                 Powerful flow control constructs including Decisions, Dynamic Fork-Joins and Subworkflows. Variables and templates are supported.         Polyglot                 Client libraries in multiple languages allows workers to be implemented in Java, Node JS, Python and C#.         Scalable                 Distributed architecture for both orchestrator and workers scalable from a single workflow to millions of concurrent processes.                 Developer Experience        <ul> <li>Discover and visualize the process flows from the bundled UI</li> <li>Integrated interface to create, refine and validate workflows</li> <li>JSON based workflow definition DSL</li> <li>Full featured API for custom automation</li>          Observability        <ul> <li>Understand, debug and iterate on task and workflow executions.</li> <li>Fine grain operational control over workflows with the ability to pause, resume, restart, retry and terminate</li> </ul> Why Conductor?  Service Orchestration          <p>Workflow definitions are decoupled from task implementations. This allows the creation of process flows in which each individual task can be implemented            by an encapsulated microservice.</p> <p>Designing a workflow orchestrator that is resilient and horizontally scalable is not a simple problem. Conductor was developed as a solution to that problem.</p>  Service Choreography                     Process flows are implicitly defined across multiple service implementations, often with           tight peer-to-peer coupling between services. Multiple event buses and complex           pub/sub models limit observability around process progress and capacity."},{"location":"devguide/bestpractices.html","title":"Best Practices","text":""},{"location":"devguide/bestpractices.html#response-timeout","title":"Response Timeout","text":"<ul> <li>Configure the responseTimeoutSeconds of each task to be &gt; 0.</li> <li>Should be less than or equal to timeoutSeconds.</li> </ul>"},{"location":"devguide/bestpractices.html#payload-sizes","title":"Payload sizes","text":"<ul> <li>Configure your workflows such that conductor is not used as a persistence store.</li> <li>Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result.</li> <li>In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large (&gt; 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.</li> </ul>"},{"location":"devguide/faq.html","title":"Frequently asked Questions","text":""},{"location":"devguide/faq.html#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc","title":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)","text":"<p>After polling for the task update the status of the task to <code>IN_PROGRESS</code> and set the <code>callbackAfterSeconds</code> value to the desired time.  The task will remain in the queue until the specified second before worker polling for it will receive it again.</p> <p>If there is a timeout set for the task, and the <code>callbackAfterSeconds</code> exceeds the timeout value, it will result in task being TIMED_OUT.</p>"},{"location":"devguide/faq.html#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months","title":"How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?","text":"<p>Yes.  As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.</p>"},{"location":"devguide/faq.html#my-workflow-fails-to-start-with-missing-task-error","title":"My workflow fails to start with missing task error","text":"<p>Ensure all the tasks are registered via <code>/metadata/taskdefs</code> APIs.  Add any missing task definition (as reported in the error) and try again.</p>"},{"location":"devguide/faq.html#where-does-my-worker-run-how-does-conductor-run-my-tasks","title":"Where does my worker run?  How does conductor run my tasks?","text":"<p>Conductor does not run the workers.  When a task is scheduled, it is put into the queue maintained by Conductor.  Workers are required to poll for tasks using <code>/tasks/poll</code> API at periodic interval, execute the business logic for the task and report back the results using <code>POST {{ api_prefix }}/tasks</code> API call.  Conductor, however will run system tasks on the Conductor server.</p>"},{"location":"devguide/faq.html#how-can-i-schedule-workflows-to-run-at-a-specific-time","title":"How can I schedule workflows to run at a specific time?","text":"<p>Conductor itself does not provide any scheduling mechanism.  But there is a community project Schedule Conductor Workflows which provides workflow scheduling capability as a pluggable module as well as workflow server. Other way is you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow.  Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing.</p>"},{"location":"devguide/faq.html#how-do-i-setup-dynomite-cluster","title":"How do I setup Dynomite cluster?","text":"<p>Visit Dynomite's Github page to find details on setup and support mechanism.</p>"},{"location":"devguide/faq.html#can-i-use-conductor-with-ruby-go-python","title":"Can I use conductor with Ruby / Go / Python?","text":"<p>Yes.  Workers can be written any language as long as they can poll and update the task results via HTTP endpoints.</p> <p>Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server.</p> <p>Note: Python and Go clients have been contributed by the community.</p>"},{"location":"devguide/faq.html#how-can-i-get-help-with-dynomite","title":"How can I get help with Dynomite?","text":"<p>Visit Dynomite's Github page to find details on setup and support mechanism.</p>"},{"location":"devguide/faq.html#my-workflow-is-running-and-the-task-is-scheduled-but-it-is-not-being-processed","title":"My workflow is running and the task is SCHEDULED but it is not being processed.","text":"<p>Make sure that the worker is actively polling for this task. Navigate to the <code>Task Queues</code> tab on the Conductor UI and select your task name in the search box. Ensure that <code>Last Poll Time</code> for this task is current.</p> <p>In Conductor 3.x, <code>conductor.redis.availabilityZone</code> defaults to <code>us-east-1c</code>.  Ensure that this matches where your workers are, and that it also matches<code>conductor.redis.hosts</code>.</p>"},{"location":"devguide/faq.html#how-do-i-configure-a-notification-when-my-workflow-completes-or-fails","title":"How do I configure a notification when my workflow completes or fails?","text":"<p>When a workflow fails, you can configure a \"failure workflow\" to run using the<code>failureWorkflow</code> parameter. By default, three parameters are passed:</p> <ul> <li>reason</li> <li>workflowId: use this to pull the details of the failed workflow.</li> <li>failureStatus</li> </ul> <p>You can also use the Workflow Status Listener: </p> <ul> <li>Set the workflowStatusListenerEnabled field in your workflow definition to true which enables notifications.</li> <li>Add a custom implementation of the Workflow Status Listener. Refer this.</li> <li>This notification can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here.</li> </ul> <p>Refer to this documentation to extend conductor to send out events/notifications upon workflow completion/failure. </p>"},{"location":"devguide/faq.html#i-want-my-worker-to-stop-polling-and-executing-tasks-when-the-process-is-being-terminated-java-client","title":"I want my worker to stop polling and executing tasks when the process is being terminated. (Java client)","text":"<p>In a <code>PreDestroy</code> block within your application, call the <code>shutdown()</code> method on the <code>TaskRunnerConfigurer</code> instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated.</p>"},{"location":"devguide/faq.html#can-i-exit-early-from-a-task-without-executing-the-configured-automatic-retries-in-the-task-definition","title":"Can I exit early from a task without executing the configured automatic retries in the task definition?","text":"<p>Set the status to <code>FAILED_WITH_TERMINAL_ERROR</code> in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.</p>"},{"location":"devguide/architecture/index.html","title":"Architecture Overview","text":"<p>The API and storage layers are pluggable and provide ability to work with different backends and queue service providers.</p>"},{"location":"devguide/architecture/index.html#runtime-model","title":"Runtime Model","text":"<p>Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.</p> <p></p>"},{"location":"devguide/architecture/index.html#notes","title":"Notes","text":"<ul> <li>Workers are remote systems that communicate over HTTP with the conductor servers.</li> <li>Task Queues are used to schedule tasks for workers.  We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism.</li> <li>conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend.</li> <li>See section under extending backend for implementing support for different databases for storage and indexing.</li> </ul>"},{"location":"devguide/architecture/directed-acyclic-graph.html","title":"Directed Acyclic Graph (DAG)","text":""},{"location":"devguide/architecture/directed-acyclic-graph.html#what-is-a-directed-acyclic-graph-dag","title":"What is a Directed Acyclic Graph (DAG)?","text":"<p>Conductor workflows are directed acyclic graphs (DAGs). But, what exactly is a DAG?</p> <p>To understand a DAG, we'll walk through each term (but not in order):</p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#graph","title":"Graph","text":"<p>A graph is \"a collection of vertices (or point) and edges (or lines) that indicate connections between the vertices.\"  </p> <p>By this definition, this is a graph - just not exactly correct in the context of DAGs:</p> <p></p> <p>But in the context of workflows, we're thinking of a graph more like this:</p> <p></p> <p>Imagine each vertex as a microservice, and the lines are how the microservices are connected together. However, this graph is not a directed graph - as there is no direction given to each connection.</p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#directed","title":"Directed","text":"<p>A directed graph means that there is a direction to each connection. For example, this graph is directed:</p> <p></p> <p>Each arrow has a direction, Point \"N\" can proceed directly to \"B\", but \"B\" cannot proceed to \"N\" in the opposite direction.  </p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#acyclic","title":"Acyclic","text":"<p>Acyclic means without circular or cyclic paths.  In the directed example above,  A -&gt; B -&gt; D -&gt; A is a cyclic loop.  </p> <p>So a Directed Acyclic Graph is a set of vertices where the connections are directed without any looping.  DAG charts can only \"move forward\" and cannot redo a step (or series of steps.)</p> <p>Since a Conductor workflow is a series of vertices that can connect in only a specific direction and cannot loop, a Conductor workflow is thus a directed acyclic graph:</p> <p></p>"},{"location":"devguide/architecture/directed-acyclic-graph.html#can-a-workflow-have-loops-and-still-be-a-dag","title":"Can a workflow have loops and still be a DAG?","text":"<p>Yes. For example, Conductor workflows have Do-While loops:</p> <p></p> <p>This is still a DAG, because the loop is just shorthand for running the tasks inside the loop over and over again.  For example, if the 2nd loop in the above image is run 3 times, the workflow path will be:</p> <ol> <li>zero_offset_fix_1</li> <li>post_to_orbit_ref_1</li> <li>zero_offset_fix_2</li> <li>post_to_orbit_ref_2</li> <li>zero_offset_fix_3</li> <li>post_to_orbit_ref_3</li> </ol> <p>The path is directed forward, and the loop just makes it easier to define the workflow.</p>"},{"location":"devguide/architecture/tasklifecycle.html","title":"Task Lifecycle","text":""},{"location":"devguide/architecture/tasklifecycle.html#task-state-transitions","title":"Task state transitions","text":"<p>The figure below depicts the state transitions that a task can go through within a workflow execution.</p> <p></p>"},{"location":"devguide/architecture/tasklifecycle.html#retries-and-failure-scenarios","title":"Retries and Failure Scenarios","text":""},{"location":"devguide/architecture/tasklifecycle.html#task-failure-and-retries","title":"Task failure and retries","text":"<p>Retries for failed task executions of each task can be configured independently. <code>retryCount</code>, <code>retryDelaySeconds</code> and <code>retryLogic</code> can be used to configure the retry mechanism.</p> <p></p> <ol> <li>Worker (W1) polls for task T1 from the Conductor server and receives the task.</li> <li>Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds.</li> <li>The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds.</li> </ol>"},{"location":"devguide/architecture/tasklifecycle.html#poll-timeout-seconds","title":"Poll Timeout Seconds","text":"<p>Poll timeout is the maximum amount of time by which a worker needs to poll a task, else the task will be marked as <code>TIMED_OUT</code>.</p> <p></p> <p>In the figure above, task T1 does not get polled by the worker within 60 seconds, so Conductor marks it as <code>TIMED_OUT</code>.</p>"},{"location":"devguide/architecture/tasklifecycle.html#timeout-seconds","title":"Timeout seconds","text":"<p>Timeout is the maximum amount of time that the task must reach a terminal state in, else it will be marked as <code>TIMED_OUT</code>.</p> <p></p> <p>0 seconds -&gt; Worker polls for task T1 from the Conductor server and receives the task. T1 is put into <code>IN_PROGRESS</code> status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to <code>IN_PROGRESS</code> status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds.  </p> <p>9,18 seconds -&gt; Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds.</p> <p>27 seconds -&gt; Worker polls and receives task T1 from the server and is now able to process this task.</p> <p>30 seconds (T1 timeout) -&gt; Server marks T1 as <code>TIMED_OUT</code> because it is not in a terminal state after first being moved to <code>IN_PROGRESS</code> status. Server schedules a new task based on the retry count.</p> <p>32 seconds -&gt; Worker completes processing of T1 and updates the server with <code>COMPLETED</code> status. Server will ignore this update since T1 has already been moved to a terminal status (<code>TIMED_OUT</code>).</p>"},{"location":"devguide/architecture/tasklifecycle.html#response-timeout-seconds","title":"Response timeout seconds","text":"<p>Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT.</p> <p></p> <p>0 seconds -&gt; Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into <code>IN_PROGRESS</code> status by the server.</p> <p>Worker starts processing the task but the worker instance dies during this execution.</p> <p>20 seconds (T1 responseTimeout) -&gt; Server marks T1 as <code>TIMED_OUT</code> since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration.</p> <p>25 seconds -&gt; The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.</p>"},{"location":"devguide/architecture/technicaldetails.html","title":"Technical Details","text":""},{"location":"devguide/architecture/technicaldetails.html#grpc-framework","title":"gRPC Framework","text":"<p>As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving  the Jetty server into a separate module and the conformance to Guice guidelines and best practices.  This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API. The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects.</p>"},{"location":"devguide/architecture/technicaldetails.html#cassandra-persistence","title":"Cassandra Persistence","text":"<p>The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \"workflows\" table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \"task_lookup\" table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId.  All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO, PollDataDAO and RateLimitingDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from.</p>"},{"location":"devguide/architecture/technicaldetails.html#external-payload-storage","title":"External Payload Storage","text":"<p>The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system. The externalization of payloads is enforced using two kinds of barriers. Soft barriers are used when the  payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed. The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location.</p>"},{"location":"devguide/architecture/technicaldetails.html#dynamic-workflow-executions","title":"Dynamic Workflow Executions","text":"<p>In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed:</p> <ul> <li>Registration of the 10 task definitions if they don't exist (assuming workflow task type SIMPLE for simplicity)</li> <li>Registration of the workflow definition</li> <li>Each time a definition needs to be retrieved, a call to the metadata store needed to be performed</li> <li>In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions</li> </ul> <p>To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest.</p> <p>Following benefits are seen as a result of these changes:</p> <ul> <li>Grants immutability of the definition stored within the execution data against modifications to the metadata store</li> <li>Better testability of workflows with faster experimental changes to definitions</li> <li>Reduced stress on the datastore due to prefetching the metadata only once at the start</li> </ul>"},{"location":"devguide/architecture/technicaldetails.html#decoupling-elasticsearch-from-persistence","title":"Decoupling Elasticsearch from Persistence","text":"<p>In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa. We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution.</p>"},{"location":"devguide/architecture/technicaldetails.html#elasticsearch-56-support","title":"Elasticsearch 5/6 Support","text":"<p>Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI.  In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 by following the simple instructions here.</p>"},{"location":"devguide/architecture/technicaldetails.html#maintaining-workflow-consistency-with-distributed-locking-and-fencing-tokens","title":"Maintaining workflow consistency with distributed locking and fencing tokens","text":""},{"location":"devguide/architecture/technicaldetails.html#problem","title":"Problem","text":"<p>Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow.</p> <p>In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent.</p>"},{"location":"devguide/architecture/technicaldetails.html#a-two-part-solution-to-maintain-workflow-consistency","title":"A two-part solution to maintain Workflow Consistency","text":"<p>Preventing concurrent decides with distributed locking: The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided.</p> <p>Preventing stale data updates with fencing tokens: While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct.</p> <p>Netflix uses Cassandra. Considering the tradeoffs of Cassandra\u2019s Lightweight Transactions (LWT) and the probability of this stale updates happening, and our testing results, we\u2019ve decided to first only rollout distributed locking with Zookeeper. We'll monitor our system and add C LWT if needed.</p>"},{"location":"devguide/architecture/technicaldetails.html#setting-up-desired-level-of-consistency","title":"Setting up desired level of consistency","text":"<p>Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations.</p>"},{"location":"devguide/architecture/technicaldetails.html#alternative-solution-to-distributed-decide-evaluation","title":"Alternative solution to distributed \"decide\" evaluation","text":"<p>As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service.</p> <p>Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out.</p> <p>In a single node deployment (single dynomite rack and single conductor server) this shouldn't be a problem. But when running multiple replicated dynomite racks and a conductor server on top of each rack, this might trigger the race condition described in previous section.</p> <p>Dynomite rack is a single or multiple instance dynomite setup that holds all the data.</p> <p>More on dynomite HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404)</p> <p>In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2.</p>"},{"location":"devguide/architecture/technicaldetails.html#more-on-dyno-queues","title":"More on dyno queues","text":"<p>Dyno queues are the default queuing mechanism of conductor.</p> <p>Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue)   * This queue is used by SweeperService</p> <p>Each conductor server instance gets its own set of queues. Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g.</p> <pre><code>conductor_queues.test.QUEUE._deciderQueue.c\nconductor_queues.test.QUEUE._deciderQueue.d\nconductor_queues.test.QUEUE.HTTP.c\nconductor_queues.test.QUEUE.HTTP.d\nconductor_queues.test.QUEUE.LAMBDA.c\nconductor_queues.test.QUEUE.LAMBDA.d\n</code></pre> <p>The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively.</p> <p>The shard names are extracted from dynomite rack name such as us-east-1c that is set in \"LOCAL_RACK\" or \"EC2_AVAILABILTY_ZONE\"</p> <p>Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows:</p> <pre><code>Workflow execution    -&gt; conductor_queues.test.QUEUE._deciderQueue.c\nHTTP taks execution   -&gt; conductor_queues.test.QUEUE.HTTP.d\nLAMBDA task execution -&gt; conductor_queues.test.QUEUE.LAMBDA.c\n</code></pre> <p>Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task.</p> <p>This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next.</p> <p>The round-robin strategy responsible for work distribution is defined here</p>"},{"location":"devguide/architecture/technicaldetails.html#back-to-alternative-solution","title":"Back to alternative solution","text":"<p>The alternative solution here is Switching round-robin queue allocation for a local-only strategy. Meaning that a workflow and its task executions are queued only for the conductor instance which started the workflow.</p> <p>This completely avoids the race condition for the price of removing task execution distribution.</p> <p>Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances.</p> <p>The downside here is that the workload is not distributed across all conductor servers. Which might be an advantage in active-standby deployments.</p> <p>Considering other downsides ...</p> <p>Considering a situation where a conductor instance goes down: * With local-only strategy, the workflow executions from failed conductor instance will not progress until:   * The conductor instance is restarted or   * The executions are manually terminated and restarted from a different node * With round-robin strategy, there is a chance the tasks will be rescheduled on a different conductor node   * This is nondeterministic though</p> <p>Enabling local only queue allocation strategy for dyno queues:</p> <p>Just enable following setting the config.properties:</p> <pre><code>workflow.dyno.queue.sharding.strategy=localOnly\n</code></pre> <p>The default is roundRobin</p>"},{"location":"devguide/concepts/index.html","title":"Basic Concepts","text":"<p>Conductor is an orchestration engine for building stateful, modular process flows. Here are the key concepts that underpin the Conductor system:</p> <ul> <li>Workflows\u2014The blueprint of a process flow.</li> <li>Tasks\u2014The basic building blocks of a Conductor workflow.</li> <li>Workers\u2014The code that executes tasks in a Conductor workflow.</li> </ul>"},{"location":"devguide/concepts/tasks.html","title":"Tasks","text":"<p>A task is the basic building block of a Conductor workflow. They are reusable and modular, representing steps in your application like processing data files, calling an AI model, or executing some logic.</p> <p>In Conductor, tasks can be defined, configured, and then executed. Learn more about the distinct but related concepts, task definition, task configuration, and task execution below.</p>"},{"location":"devguide/concepts/tasks.html#types-of-tasks","title":"Types of tasks","text":"<p>Tasks are categorized into three types, enabling you to flexibly build workflows using pre-built tasks, custom logic, or a combination of both:</p>"},{"location":"devguide/concepts/tasks.html#system-tasks","title":"System tasks","text":"<p>System tasks are built-in, general-purpose tasks designed for common uses like calling an HTTP endpoint or publishing events to an external system.</p> <p>System tasks are managed by Conductor and executed within its server's JVM, allowing you to get started without having to write custom workers.</p>"},{"location":"devguide/concepts/tasks.html#worker-tasks","title":"Worker tasks","text":"<p>Worker tasks (<code>SIMPLE</code>) can be used to implement custom logic outside the scope of Conductor\u2019s system tasks. Also known as Simple tasks, Worker tasks are implemented by your task workers that run in a separate environment from Conductor.</p>"},{"location":"devguide/concepts/tasks.html#operators","title":"Operators","text":"<p>Operators are built-in control flow primitives similar to programming language constructs like loops, switch cases, or fork/joins. Like system tasks, operators are also managed by Conductor.</p>"},{"location":"devguide/concepts/tasks.html#task-definition","title":"Task definition","text":"<p>Task definitions are used to define a task's default parameters, like inputs and output keys, timeouts, and retries. This provides reusability across workflows, as the registered task definition will be referenced when a task is configured in a workflow definition.</p> <p>When using Worker tasks (<code>SIMPLE</code>), its task definition must be registered to the Conductor server before it can execute in a workflow. Because system tasks are managed by Conductor, tt is not necessary to add a task definition for system tasks unless you wish to customize its default parameters.</p>"},{"location":"devguide/concepts/tasks.html#task-configuration","title":"Task configuration","text":"<p>Stored in the <code>tasks</code> array of a workflow definition, task configurations make up the workflow-specific blueprint that describes:</p> <ul> <li>The order and control flow of tasks.</li> <li>How data is passed from one task to another through task inputs and outputs.</li> <li>Other workflow-specific behavior, like optionality, caching, and schema enforcement.</li> </ul> <p>The specific configuration for each task differs depending on the task type. For system tasks and operators, the task configuration will contain important parameters that control the behavior of the task. For example, the task configuration of an HTTP task will specify an endpoint URL and its templatized payload that will be used when the task executes.</p> <p>For Worker tasks (<code>SIMPLE</code>), the configuration will simply contain its inputs/outputs and a reference to its task definition name, because the logic of its behavior will already be specified in the worker code of your application.</p> <p>There must be at least one task configured in each workflow definition.</p>"},{"location":"devguide/concepts/tasks.html#task-execution","title":"Task execution","text":"<p>A task execution object is created during runtime when an input is passed into a configured task. This object has a unique ID and represents the result of the task operation, including the task status, start time, and inputs/outputs.</p>"},{"location":"devguide/concepts/why.html","title":"Introducing Conductor","text":"<p>Conductor enables developers to build highly reliable applications and workflows. As an orchestration engine, Conductor:</p> <ul> <li>Keeps track of your application\u2019s execution flow and state;  </li> <li>Calls modular, stateless tasks in its defined order;  </li> <li>Handles failure scenarios and retries gracefully; and  </li> <li>Stores all final and intermediate results.</li> </ul> <p></p> <p>By using Conductor to build application flows, developers can focus on their core work: writing application code in the language of the choice, rather than orchestration or plumbing logic. Conductor does the heavy lifting associated with ensuring reliability, transactional consistency, and durability of the execution flow. No matter where your application code lives, you can build a workflow in Conductor to govern its execution.</p>"},{"location":"devguide/concepts/why.html#features","title":"Features","text":"<p>Here are the key features available in Conductor OSS:</p> <ul> <li>A distributed server ecosystem, which stores workflow state information efficiently.  </li> <li>DAG- (Directed Acyclic Graph) based JSON workflow definitions, decoupled from its service implementations.  </li> <li>Reusable, built-in tasks and operators for quick workflow creation.  </li> <li>Visual diagrams for depicting workflow definitions and their runtime execution paths.  </li> <li>Logs, timelines, and execution data that provide visibility and traceability into the workflows.  </li> <li>Simple interface to connect Conductor server to workers that execute the tasks in workflows.  </li> <li>Language-agnostic workers, enabling each task to be written in the language most suited for the service.  </li> <li>Configurable properties with sensible defaults for rate limits, retries, timeouts, and concurrent execution limits.  </li> <li>User interface to build, search, run, and visualize workflows.  </li> <li>Full operational control over workflow execution (pause, resume, restart, retry and terminate).  </li> <li>Event handlers to control workflows via external actions.  </li> <li>Client implementations in Java, Python and other languages.  </li> <li>Backed by a queuing service abstracted from the clients.  </li> <li>Ability to scale to millions of concurrently-running process flows.</li> </ul>"},{"location":"devguide/concepts/why.html#why-not-peer-to-peer-choreography","title":"Why not peer-to-peer choreography?","text":"<p>Using peer-to-peer task choreography, we found it harder to scale processes with evolving business needs and complexities. Even using a pub/sub model for the simplest flows, some of the issues associated with the approach quickly became evident:</p> <ul> <li>High-level process flows are \"embedded\" and implicit within the code of multiple applications.</li> <li>Often, there are many assumptions around inputs/outputs, SLAs, and so on, and the tight coupling makes it harder to adapt processes to changing needs.</li> <li>There is almost no way to systematically answer: \"How far along are with in Process X?\"</li> </ul>"},{"location":"devguide/concepts/workers.html","title":"Workers","text":"<p>A worker is responsible for executing a task in a workflow. Each type of worker implements the core functionality of each task, handling the logic as defined in its code.</p> <p>System task workers are managed by Conductor within its JVM, while <code>SIMPLE</code> task workers are to be implemented by yourself. These workers can be implemented in any programming language of your choice (Python, Java, JavaScript, C#, Go, and Clojure) and hosted anywhere outside the Conductor environment.</p> <p>Note</p> <p>Conductor provides a set of worker frameworks in its SDKs. These frameworks come with comes with features like polling threads, metrics, and server communication, making it easy to create custom workers.</p> <p>These workers communicate with the Conductor server via REST/gRPC, allowing them to poll for tasks and update the task status. Learn more in Architecture.</p>"},{"location":"devguide/concepts/workflows.html","title":"Workflows","text":"<p>A workflow is a sequence of tasks with a defined order and execution. Each workflow encapsulates a specific process, such as:</p> <ul> <li>Classifying documents</li> <li>Ordering from a self-checkout service</li> <li>Upgrading cloud infrastructure</li> <li>Transcoding videos</li> <li>Approving expenses</li> </ul> <p>In Conductor, workflows can be defined and then executed. Learn more about the two distinct but related concepts, workflow definition and workflow execution, below.</p>"},{"location":"devguide/concepts/workflows.html#workflow-definition","title":"Workflow definition","text":"<p>The workflow definition describes the flow and behavior of your business logic. Think of it as a blueprint specifying how it should execute at runtime until it reaches a terminal state. The workflow definition includes:</p> <ul> <li>The workflow\u2019s input/output keys.</li> <li>A collection of task configurations that specify the task conditions, sequence, and data flow until the workflow is completed.</li> <li>The workflow's runtime behavior, such as the timeout policy and compensation flow.</li> </ul>"},{"location":"devguide/concepts/workflows.html#workflow-execution","title":"Workflow execution","text":"<p>A workflow execution is the execution instance of a workflow definition. </p> <p>Whenever a workflow definition is invoked with a given input, a new workflow execution with a unique ID is created. The workflow is governed by a defined state (like RUNNING or COMPLETED), which makes it intuitive to track the workflow.</p>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html","title":"Creating Task Definitions","text":"<p>Tasks can be created using the tasks metadata API</p> <p><code>POST {{ api_prefix }}/metadata/taskdefs</code></p> <p>This API takes an array of new task definitions.</p>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#examples","title":"Examples","text":""},{"location":"devguide/how-tos/Tasks/creating-tasks.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl '{{ server_host }}{{ api_prefix }}/metadata/taskdefs' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '[{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}]'\n</code></pre>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"{{ server_host }}{{ api_prefix }}/metadata/taskdefs\", {\n    \"headers\": {\n        \"accept\": \"*/*\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"[{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}]\",\n    \"method\": \"POST\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#best-practices","title":"Best Practices","text":"<ol> <li>You can update a set of tasks together in this API</li> <li>Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details' </li> <li>You can also use the Conductor Swagger UI to update the tasks</li> </ol>"},{"location":"devguide/how-tos/Tasks/dynamic-vs-switch-tasks.html","title":"Dynamic vs Switch Tasks","text":"<p>Dynamic Tasks are useful in situations when need to run a task of which the task type is determined at runtime instead of during the configuration. It is similar to the <code>SWITCH</code> use case but with <code>DYNAMIC</code> we won't need to preconfigure all case options in the workflow definition itself. Instead, we can mark the task as <code>DYNAMIC</code> and determine which underlying task does it run during the workflow execution itself.</p> <ul> <li>Use DYNAMIC task as a replacement for SWITCH if you have too many case options</li> <li>DYNAMIC task is an option when you want to programmatically determine the next task to run instead of using expressions</li> <li>DYNAMIC task simplifies the workflow execution UI view which will now only show the selected task</li> <li>SWITCH task visualization is helpful as a documentation - showing you all options that the workflow could have    taken</li> <li>SWITCH task comes with a default task option which can be useful in some use cases</li> </ul> <p>Learn more about</p> <ul> <li>Dynamic Tasks</li> <li>Switch Tasks</li> </ul>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html","title":"Extending System Tasks","text":"<p>System tasks allow Conductor to run simple tasks on the server - removing the need to build (and deploy) workers for basic tasks.  This allows for automating more mundane tasks without building specific microservices for them.</p> <p>However, sometimes it might be necessary to add additional parameters to a System Task to gain the behavior that is desired.</p>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html#example-http-task","title":"Example HTTP Task","text":"<pre><code>{\n  \"name\": \"get_weather_90210\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"get_weather_90210\",\n      \"taskReferenceName\": \"get_weather_90210\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"https://weatherdbi.herokuapp.com/data/weather/90210\",\n          \"method\": \"GET\",\n          \"connectionTimeOut\": 1300,\n          \"readTimeOut\": 1300\n        }\n      },\n      \"type\": \"HTTP\",\n      \"decisionCases\": {},\n      \"defaultCase\": [],\n      \"forkTasks\": [],\n      \"startDelay\": 0,\n      \"joinOn\": [],\n      \"optional\": false,\n      \"defaultExclusiveJoinTask\": [],\n      \"asyncComplete\": false,\n      \"loopOver\": []\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"data\": \"${get_weather_ref.output.response.body.currentConditions.comment}\"\n  },\n  \"schemaVersion\": 2,\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0,\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>This very simple workflow has a single HTTP Task inside.  No parameters need to be passed, and when run, the HTTP task will return the weather in Beverly Hills, CA (Zip code = 90210).</p> <p>This API has a very slow response time. In the HTTP task, the connection is set to time out after 1300ms, which is too short for this API, resulting in a timeout.  This API will work if we allowed for a longer timeout, but in order to demonstrate adding retries to the HTTP Task, we will artificially force the API call to fail.</p> <p>When this workflow is run - it fails, as expected.</p> <p>Now, sometimes an API call might fail due to an issue on the remote server, and retrying the call will result in a response.  With many Conductor tasks,  <code>retryCount</code>, <code>retryDelaySeconds</code> and <code>retryLogic</code> fields can be applied to retry the worker (with the desired parameters).</p> <p>By default, the HTTP Task does not have <code>retryCount</code>, <code>retryDelaySeconds</code> or <code>retryLogic</code> built in.  Attempting to add these parameters to a HTTP Task results in an error.</p>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html#the-solution","title":"The Solution","text":"<p>We can create a task with the same name with the desired parameters.  Defining the following task (note that the <code>name</code> is identical to the one in the workflow):</p> <pre><code>{\n\n  \"createdBy\": \"\",\n  \"name\": \"get_weather_90210\",\n  \"description\": \"editing HTTP task\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 5,\n  \"inputKeys\": [],\n  \"outputKeys\": [],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 5,\n  \"responseTimeoutSeconds\": 5,\n  \"inputTemplate\": {},\n  \"rateLimitPerFrequency\": 0,\n  \"rateLimitFrequencyInSeconds\": 1\n}\n</code></pre> <p>We've added the three parameters: <code>retryCount: 3, retryDelaySeconds: 5, retryLogic: FIXED</code></p> <p>The <code>get_weather_90210</code> task will now run 4 times (it will fail once, and then retry 3 times), with a <code>FIXED</code> 5 second delay between attempts.</p> <p>Re-running the task (and looking at the timeline view) shows that this is what occurs.  There are 4 attempts, with a 5 second delay between them.</p> <p>If we change the <code>retryLogic</code> to EXPONENTIAL_BACKOFF, the delay between attempts grows exponentially:</p> <ol> <li>5*2^0 = 5 seconds</li> <li>5*2^1 = 10 seconds</li> <li>5*2^2 = 20 seconds</li> </ol>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html","title":"Monitoring Task Queues","text":"<p>Conductor offers an API and UI interface to monitor the task queues. This is useful to see details of the number of workers polling and monitoring the queue backlog.</p>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html#using-the-ui","title":"Using the UI","text":"<pre><code>&lt;your UI server URL&gt;/taskQueue\n</code></pre> <p>Access this screen via - Home &gt; Task Queues</p> <p>On this screen you can select and view the details of the task queue. The following information is shown:</p> <ol> <li>Queue Size - The number of tasks waiting to be executed</li> <li>Workers - The count and list of works and their instance reference who are polling for work for this task</li> </ol>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html#using-apis","title":"Using APIs","text":"<p>To see the size of the task queue via API:</p> <pre><code>curl '{{ server_host }}{{ api_prefix }}/tasks/queue/sizes?taskType=&lt;TASK_NAME&gt;' \\\n  -H 'accept: */*' \n</code></pre> <p>To see the worker poll information of the task queue via API:</p> <pre><code>curl '{{ server_host }}{{ api_prefix }}/tasks/queue/polldata?taskType=&lt;TASK_NAME&gt;' \\\n  -H 'accept: */*'\n</code></pre> <p>Note</p> <p>Replace <code>&lt;TASK_NAME&gt;</code> with your task name</p>"},{"location":"devguide/how-tos/Tasks/reusing-tasks.html","title":"Reusing Tasks","text":"<p>A powerful feature of Conductor is that it supports and enables re-usability out of the box. Task workers typically perform a unit of work and is usually a part of a larger workflow. Such workers are often re-usable in multiple workflows. Once a task is defined, you can use it across as any workflow.</p> <p>When re-using tasks, it's important to think of situations that a multi-tenant system faces. All the work assigned to this worker by default goes to the same task scheduling queue. This could result in your worker not being polled quickly if there is a noisy neighbour in the ecosystem. One way you can tackle this situation is by re-using the worker code, but having different task names registered for different use cases. And for each task name, you can run an appropriate number of workers based on expected load.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html","title":"Task Inputs","text":"<p>Task inputs can be provided in multiple ways. This is configured in the workflow definition when a task is participating in the workflow.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#inputs-referred-from-workflow-inputs","title":"Inputs referred from Workflow inputs","text":"<p>When we start a workflow, we can provide inputs to the workflow in a json format. For example:</p> <pre><code>{\n  \"worfklowInputNumberExample\": 1,\n  \"worfklowInputTextExample\": \"SAMPLE\",\n  \"worfklowInputJsonExample\": {\n    \"nestedKey\": \"nestedValue\"\n  }\n}\n</code></pre> <p>These values can be referred as inputs into your task using the following expression:</p> <pre><code>{\n  \"taskInput1Key\": \"${workflow.input.worfklowInputNumberExample}\",\n  \"taskInput2Key\": \"${workflow.input.worfklowInputJsonExample.nestedKey}\"\n}\n</code></pre> <p>In this example, the tasks will receive the following inputs after they are evaluated: <pre><code>{\n  \"taskInput1Key\": 1,\n  \"taskInput2Key\": \"nestedValue\"\n}\n</code></pre></p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#inputs-referred-from-other-task-outputs","title":"Inputs referred from other Task outputs","text":"<p>Similar to how we can refer to workflow inputs, we can also refer to an output field that was generated by a task that executed before.</p> <p>Let's assume a task with the task reference name <code>previousTaskReference</code> executed and produced the following output:</p> <pre><code>{\n  \"taskOutputKey1\": \"outputValue\",\n  \"taskOutputKey2\": {\n    \"nestedKey1\": \"outputValue-1\"\n  }\n}\n</code></pre> <p>We can refer to these as the new task's input by using the following expression:</p> <pre><code>{\n  \"taskInput1Key\": \"${previousTaskReference.output.taskOutputKey1}\",\n  \"taskInput2Key\": \"${previousTaskReference.output.taskOutputKey2.nestedKey1}\"\n}\n</code></pre> <p>The expression format is based on Json Path and you can construct complex input params based on the syntax.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#hard-coded-inputs","title":"Hard coded inputs","text":"<p>Task inputs can also be hard coded in the workflow definitions. This is useful when you have a re-usable task which has configurable options that can be applied in different workflow contexts.</p> <pre><code>{\n  \"taskInput1\": \"OPTION_A\",\n  \"taskInput2\": 100\n}\n</code></pre>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html","title":"Task Timeouts","text":"<p>Tasks can be configured to handle various scenarios of timeouts. Here are some scenarios and the relevance configuration fields.</p> Scenario Configuration A task worker picked up the task, but fails to respond back with an update <code>responseTimeoutSeconds</code> A task worker picked up the task and updates progress, but fails to complete within an expected timeframe <code>timeoutSeconds</code> A task is stuck in a retry loop with repeated failures beyond an expected timeframe <code>timeoutSeconds</code> Task doesn't get picked by any workers for a specific amount of time <code>pollTimeoutSeconds</code> Task isn't completed within a specified amount of time despite being picked up by task workers <code>timeoutSeconds</code> <p><code>timeoutSeconds</code> should always be greater than <code>responseTimeoutSeconds</code></p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#timeout-seconds","title":"Timeout Seconds","text":"<pre><code>\"timeoutSeconds\" : 30\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this task to complete successfully up until this number of seconds from when the task is first polled. We can use this to fail a workflow when a task breaches the overall SLA for completion.</p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#response-timeout-seconds","title":"Response Timeout Seconds","text":"<pre><code>\"responseTimeoutSeconds\" : 10\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this number of seconds from when the task is polled before the worker updates back with a status. The worker can keep the task in <code>IN_PROGRESS</code> state if it requires more time to complete.</p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#poll-timeout-seconds","title":"Poll Timeout Seconds","text":"<pre><code>\"pollTimeoutSeconds\" : 10\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this number of seconds for the task to be picked up by a task worker. Useful when you want to detect a backlogged task queue with not enough workers.</p>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html","title":"Updating Task Definitions","text":"<p>Updates to the task definitions can be made using the following API</p> <pre><code>PUT {{ api_prefix }}/metadata/taskdefs\n</code></pre> <p>This API takes a single task definition and updates itself. </p>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#examples","title":"Examples","text":""},{"location":"devguide/how-tos/Tasks/updating-tasks.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl '{{ server_host }}{{ api_prefix }}/metadata/taskdefs' \\\n  -X 'PUT' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}'\n</code></pre>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"{{ server_host }}{{ api_prefix }}/metadata/taskdefs\", {\n    \"headers\": {\n        \"accept\": \"*/*\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}\",\n    \"method\": \"PUT\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#best-practices","title":"Best Practices","text":"<ol> <li>You can also use the Conductor Swagger UI to update the tasks </li> <li>Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details'</li> </ol>"},{"location":"devguide/how-tos/Workers/build-a-golang-task-worker.html","title":"Build a Go Task Worker","text":"<p>See conductor-sdk/conductor-go</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html","title":"Build a Java Task Worker","text":"<p>This guide provides introduction to building Task Workers in Java.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#dependencies","title":"Dependencies","text":"<p>Conductor provides Java client libraries, which we will use to build a simple task worker.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#maven-dependency","title":"Maven Dependency","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.conductoross&lt;/groupId&gt;\n    &lt;artifactId&gt;conductor-client&lt;/artifactId&gt;\n    &lt;version&gt;3.16.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.conductoross&lt;/groupId&gt;\n    &lt;artifactId&gt;conductor-common&lt;/artifactId&gt;\n    &lt;version&gt;3.16.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#gradle","title":"Gradle","text":"<pre><code>implementation group: 'org.conductoross', name: 'conductor-client', version: '3.16.0'\nimplementation group: 'org.conductoross', name: 'conductor-common', version: '3.16.0'\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#implementing-a-task-worker","title":"Implementing a Task Worker","text":"<p>To create a worker, implement the <code>Worker</code> interface.</p> <pre><code>public class SampleWorker implements Worker {\n\n    private final String taskDefName;\n\n    public SampleWorker(String taskDefName) {\n        this.taskDefName = taskDefName;\n    }\n\n    @Override\n    public String getTaskDefName() {\n        return taskDefName;\n    }\n\n    @Override\n    public TaskResult execute(Task task) {\n        TaskResult result = new TaskResult(task);\n        result.setStatus(Status.COMPLETED);\n\n        //Register the output of the task\n        result.getOutputData().put(\"outputKey1\", \"value\");\n        result.getOutputData().put(\"oddEven\", 1);\n        result.getOutputData().put(\"mod\", 4);\n\n        return result;\n    }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#implementing-workers-logic","title":"Implementing worker's logic","text":"<p>Worker's core implementation logic goes in the <code>execute</code> method. Upon completion, set the <code>TaskResult</code> with status as one of the following:</p> <ol> <li>COMPLETED: If the task has completed successfully.</li> <li>FAILED: If there are failures - business or system failures. Based on the task's configuration, when a task fails, it may be retried.</li> </ol> <p>The <code>getTaskDefName()</code> method returns the name of the task for which this worker provides the execution logic.</p> <p>See SampleWorker.java for the complete example.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#configuring-polling-using-taskrunnerconfigurer","title":"Configuring polling using TaskRunnerConfigurer","text":"<p>The <code>TaskRunnerConfigurer</code> can be used to register the worker(s) and initialize the polling loop. It manages the task workers thread pool and server communication (poll and task update).</p> <p>Use the Builder to create an instance of the <code>TaskRunnerConfigurer</code>. The builder accepts the following parameters:</p> <pre><code> TaskClient taskClient = new TaskClient();\n taskClient.setRootURI(\"{{ server_host }}{{ api_prefix }}/\");        //Point this to the server API\n\n        int threadCount = 2;            //number of threads used to execute workers.  To avoid starvation, should be same or more than number of workers\n\n        Worker worker1 = new SampleWorker(\"task_1\");\n        Worker worker2 = new SampleWorker(\"task_5\");\n\n        // Create TaskRunnerConfigurer\n        TaskRunnerConfigurer configurer = new TaskRunnerConfigurer.Builder(taskClient, Arrays.asList(worker1, worker2))\n            .withThreadCount(threadCount)\n            .build();\n\n        // Start the polling and execution of tasks\n        configurer.init();\n</code></pre> <p>See Sample for full example.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#configuration-details","title":"Configuration Details","text":"<p>Initialize the <code>Builder</code> with the following:</p> Parameter Description TaskClient TaskClient used to communicate with the Conductor server Workers Workers that will be used for polling work and task execution. Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless <code>pollOutOfDiscovery</code> is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- <p>Once an instance is created, call <code>init()</code> method to initialize the <code>TaskPollExecutor</code> and begin the polling and execution of tasks.</p> <p>Note</p> <p>To ensure that the <code>TaskRunnerConfigurer</code> stops polling for tasks when the instance becomes unhealthy, call the provided <code>shutdown()</code> hook in a <code>PreDestroy</code> block.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#properties","title":"Properties","text":"<p>The worker behavior can be further controlled by using these properties:</p> Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false <p>Further, these properties can be set either by a <code>Worker</code> implementation or by setting the following system properties in the JVM:</p> Name Description <code>conductor.worker.&lt;property&gt;</code> Applies to ALL the workers in the JVM. <code>conductor.worker.&lt;taskDefName&gt;.&lt;property&gt;</code> Applies to the specified worker.  Overrides the global property."},{"location":"devguide/how-tos/Workers/build-a-python-task-worker.html","title":"Build a Python Task Worker","text":"<p>See  conductor-sdk/conductor-python</p>"},{"location":"devguide/how-tos/Workers/scaling-workers.html","title":"Scaling Workers","text":"<p>Workers execute business logic in workflow applications. Scaling and optimizing worker performance depend on the following metrics:</p> <ul> <li>Number of pending requests in the task queue</li> <li>Throughput of an individual worker</li> <li>Total number of worker processes running</li> </ul> <p>Conductor servers publish metrics that help monitor worker health. Learn how to use these metrics with PromQL.</p> <p>Tip: Each of the following metrics includes <code>taskType</code> as a tag. Use this tag to monitor metrics for a specific task.</p>"},{"location":"devguide/how-tos/Workers/scaling-workers.html#pending-requests-gauge","title":"Pending requests (Gauge)\u200b","text":"<pre><code>max(task_queue_depth{taskType=})\n</code></pre> <p>How to use the metric:</p> <ul> <li>The goal should be to keep the queue depth constant. It may not always be zero, especially for long-running tasks.</li> <li>Configure alerts and autoscaling policies for workers based on changes in queue depth over a specified period.</li> </ul>"},{"location":"devguide/how-tos/Workers/scaling-workers.html#number-of-tasks-completed-per-second-counter","title":"Number of tasks completed per second (Counter)\u200b","text":"<p>The <code>task_completed_seconds_count</code> metric is published as a counter and includes <code>taskType</code> as a tag.</p> <pre><code>rate(task_completed_seconds_count{taskType=}[$__rate_interval])\n</code></pre> <p>How to use the metric:</p> <ul> <li>The metric measures the throughput. The goal is to keep the throughput at a threshold depending on the application's needs.</li> <li>Configure alerts and autoscaling policies for workers based on fluctuations in throughput.</li> </ul>"},{"location":"devguide/how-tos/Workers/scaling-workers.html#duration-the-task-remained-in-the-queue","title":"Duration the task remained in the queue","text":"<p>This metric tracks how long a task remains in the queue before a worker picks it up.</p> <pre><code>max(task_queue_wait_time_seconds{quantile=, taskType=})\n</code></pre> <p>How to use the metric:</p> <p>If the value is too high (more than a few seconds), check the following: - The number of workers. If they are all busy, consider increasing the number of workers. - The worker polling interval. Reduce it if necessary.</p> <p>Note: Reducing the polling interval may increase API requests to the server, which could trigger system limits.</p>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html","title":"Creating / Updating Workflows","text":"<p>You can create and update workflows using the Conductor UI, APIs, or SDKs. These workflows can be versioned, which is useful for a variety of cases.</p> <p>If your workflow definition contains any new tasks, you must also register the task definitions to Conductor before running the workflow.</p>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#using-conductor-ui","title":"Using Conductor UI","text":"<p>With the UI, you can create or update workflow definitions visually.</p>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#creating-workflows","title":"Creating workflows","text":"<p>To create a workflow definition:</p> <ol> <li>In Definitions, select + New Workflow Definition.</li> <li>Configure the workflow definition JSON. Refer to Workflow Definition for the reference guide on the full parameters.</li> <li>Select Save &gt; Save.</li> </ol>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#updating-workflows","title":"Updating workflows","text":"<p>To update a workflow definition:</p> <ol> <li>In Definitions, select the workflow to be updated.</li> <li>Modify the workflow definition JSON. Refer to Workflow Definition for the reference guide on the full parameters.</li> <li>Select Save. The workflow version will automatically increment by 1.</li> <li>(Optional) Clear the Automatically set version checkbox to save the updated workflow definition without creating a new version.</li> <li>Select Save again to confirm.</li> </ol>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#using-apis","title":"Using APIs","text":"<p>You can create or update workflow definitions using the Update Workflow Definition API (<code>PUT api/metadata/workflow</code>). </p> <p>Refer to Workflow Definition for the reference guide on the full parameters.</p>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#example-using-curl","title":"Example using cURL","text":"<pre><code>curl '{{ server_host }}/api/metadata/workflow' \\\n  -X 'PUT' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '[{\"name\":\"sample_workflow\",\"description\":\"shipping\",\"version\":1,\"tasks\":[{\"name\":\"ship_via\",\"taskReferenceName\":\"ship_via\",\"type\":\"SIMPLE\",\"inputParameters\":{\"service\":\"${workflow.input.service}\"}}],\"inputParameters\":[\"service\"],\"outputParameters\":{},\"schemaVersion\":2, \"ownerEmail\": \"example@email.com\"}]'\n</code></pre>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#using-sdks","title":"Using SDKs","text":"<p>Conductor offers client SDKs for popular languages which have library methods for making the API call. Refer to the SDK documentation to configure a client in your selected language to invoke workflow executions.</p> <p>Refer to Workflow Definition for the reference guide on the full parameters.</p>"},{"location":"devguide/how-tos/Workflows/creating-workflows.html#example-using-javascript","title":"Example using JavaScript","text":"<p>In this example, the JavaScript Fetch API is used to create the workflow <code>sample_workflow</code>.</p> <pre><code>fetch(\"{{ server_host }}/api/metadata/workflow\", {\n  \"headers\": {\n    \"accept\": \"*/*\",\n    \"content-type\": \"application/json\"\n  },\n  \"body\": \"[{\\\"name\\\":\\\"sample_workflow\\\",\\\"description\\\":\\\"shipping\\\",\\\"version\\\":1,\\\"tasks\\\":[{\\\"name\\\":\\\"ship_via\\\",\\\"taskReferenceName\\\":\\\"ship_via\\\",\\\"type\\\":\\\"SIMPLE\\\",\\\"inputParameters\\\":{\\\"service\\\":\\\"${workflow.input.service}\\\"}}],\\\"inputParameters\\\":[\\\"service\\\"],\\\"outputParameters\\\":{},\\\"schemaVersion\\\":2,\\\"ownerEmail\\\": \\\"example@email.com\\\"}]\",\n  \"method\": \"PUT\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html","title":"Debugging Workflows","text":"<p>The workflow execution views in the Conductor UI are useful for debugging workflow issues. Learn how to debug failed executions and rerun them. </p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#debug-procedure","title":"Debug procedure","text":"<p>When you view the workflow execution details, the cause of the workflow failure will be stated at the top. Go to the Tasks &gt; Diagram tab to quickly identify the failed task, which is marked in red. You can select the failed task to investigate the details of the failure.</p> <p>The following tab views or fields in the task details are useful for debugging:</p> Field or Tab Name Description Reason for Incompletion in Task Detail &gt; Summary Contains the exception message thrown by the task worker. Worker in Task Detail &gt; Summary Contains the worker instance ID where the failure occurred. Useful for digging up detailed logs, if it has not already captured by Conductor. Task Detail &gt; Input Useful for verifying if the task inputs were correctly computed and provided to the task. Task Detail &gt; Output Useful for verifying what the task produced as output. Task Detail &gt; Logs Contains the task logs, if supplied by the task worker. Task Detail &gt; Retried Task - Select an instance (If the task has been retried multiple times) Contains all retry attempts in a dropdown list. Each list item contains the task details for a particular attempt. <p></p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#recovering-from-failure","title":"Recovering from failure","text":"<p>Once you have resolved the underlying issue for the execution failure, you can manually restart or retry the failed workflow execution using the Conductor UI or APIs.</p> <p>Here are the recovery options:</p> Recovery Action Description Restart with Current Definitions Restart the workflow from the beginning using the same workflow definition that was used in the original execution. This option is useful if the workflow definition has changed and you want to run the execution instance using the original definition. Restart with Latest Definitions Restart the workflow from the beginning using the latest workflow definition. This option is useful if changes were made to the workflow definition and you want to run the execution instance with the latest definition. Retry - From failed task Retry the workflow from the failed task. <p>Note</p> <p>You can set tasks to be retried automatically in case of transient failures. Refer to Task Definition for more information.</p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#using-conductor-ui","title":"Using Conductor UI","text":"<p>To recover from failure:</p> <ol> <li>In the workflow execution details page, select Actions in the top right corner.</li> <li>Select one of the following options:<ul> <li>Restart with Current Definitions</li> <li>Restart with Latest Definitions</li> <li>Retry - From failed task</li> </ul> </li> </ol>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#using-apis","title":"Using APIs","text":"<p>You can restart workflow executions using the Restart Workflow API (<code>POST api/workflow/{workflowId}/restart</code>) or the Bulk Restart Workflow API (<code>POST api/workflow/bulk/restart</code>).</p> <p>Likewise, you can retry workflow executions from the last failed task using the Retry Workflow API (<code>POST api/workflow/{workflowId}/retry</code>) or the Bulk Retry Workflow API (<code>POST api/workflow/bulk/retry</code>)</p>"},{"location":"devguide/how-tos/Workflows/handling-errors.html","title":"Handling Workflow Errors","text":"<p>In Conductor there are several ways to handle workflow failure automatically:</p> <ul> <li>Set a compensation flow in the workflow definition.</li> <li>Configure workflow status notifications by implementing Workflow Status Listener.</li> </ul>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#set-a-compensation-flow","title":"Set a compensation flow","text":"<p>You can configure a workflow to automatically run upon failure by adding the <code>failureWorkflow</code> parameter to your main workflow definition:</p> <p><pre><code>\"failureWorkflow\": \"&lt;name of your compensation flow&gt;\",\n</code></pre> If your main workflow fails, Conductor will trigger this failure workflow.</p> <p>By default, the following parameters will be passed to the failure workflow:</p> <ul> <li><code>reason</code>\u2014The reason for workflow's failure.</li> <li><code>workflowId</code>\u2014The failed workflow's execution ID.</li> <li><code>failureStatus</code>\u2014The failed workflow's status.</li> <li><code>failureTaskId</code>\u2014The execution ID for task that failed in the workflow.</li> <li><code>failedWorkflow</code>\u2014The workflow execution JSON for the failed workflow.</li> </ul> <p>You can use these parameters to implement compensation actions in the failure workflow, like notification alerts or clean-up.</p> <p>Example</p> <p>Here is a sample failure workflow that sends a Slack message when the main workflow fails. It posts the <code>reason</code> and <code>workflowId</code> to enable relevant teams to debug the failure:</p> <pre><code>{\n  \"name\": \"shipping_failure\",\n  \"description\": \"workflow for failures with Bobs widget workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"slack_message\",\n      \"taskReferenceName\": \"send_slack_message\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"headers\": {\n            \"Content-type\": \"application/json\"\n          },\n          \"uri\": \"https://hooks.slack.com/services/&lt;_unique_Slack_generated_key_&gt;\",\n          \"method\": \"POST\",\n          \"body\": {\n            \"text\": \"workflow: ${workflow.input.workflowId} failed. ${workflow.input.reason}\"\n          },\n          \"connectionTimeOut\": 5000,\n          \"readTimeOut\": 5000\n        }\n      },\n      \"type\": \"HTTP\",\n      \"retryCount\": 3\n    }\n  ],\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#implement-a-workflow-status-listener","title":"Implement a Workflow Status Listener","text":"<p>Using a Workflow Status Listener, you can send a notification to an external system or an event to Conductor's internal queue upon failure. Here is the high-level overview for using a Workflow Status Listener:</p> <ol> <li>Set the <code>workflowStatusListenerEnabled</code> parameter to true in your main workflow definition:   <pre><code>\"workflowStatusListenerEnabled\": true,\n</code></pre></li> <li>Implement the WorkflowStatusListener interface to plug into a custom notification or eventing system upon workflow failure. </li> </ol>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html","title":"Searching Workflows","text":"<p>The Conductor UI provides a convenient interface for searching workflow executions. There are two modes of searching:</p> <ul> <li>Workflows tab \u2014 Search using workflow parameters.</li> <li>Tasks tab \u2014 Search workflows by tasks.</li> </ul> <p>To search workflow executions:</p> <ol> <li>Go to Executions in the Conductor UI.</li> <li>Configure the search parameters.</li> <li>Select Search.</li> </ol> <p>Once the search results are displayed, you can sort the results by different column values and select additional columns to display.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#search-parameters","title":"Search parameters","text":"<p>Here are the search parameters for each search mode.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#search-by-workflows","title":"Search by workflows","text":"<p>The following fields are available for searching workflows in the Workflows tab.</p> Search Field Name Description Workflow Name Filters workflow executions by its name. Workflow ID Filters to a specific workflow execution by its execution ID. Status Filters workflow executions by its status (RUNNING, COMPLETED, FAILED, TIMED_OUT, TERMINATED, PAUSED). Start Time - From Filters workflow executions that started on or after the specified time. Start Time - To Filters workflow executions that started on or before the specified time. Lookback (days) Filters workflow executions that ran in the last given number of days. Lucene-syntax Query (Double-quote strings for Free Text) (If indexing is enabled) Filters workflow executions by querying workflow input and output values."},{"location":"devguide/how-tos/Workflows/searching-workflows.html#search-workflows-by-tasks","title":"Search workflows by tasks","text":"<p>The following fields are available for searching workflows by its tasks in the Tasks tab.</p> Search Field Name Description Task Name Filters workflow executions by its task name. Task ID Filters to a specific workflow execution that contains this task execution ID. Task Status Filters workflow executions by its task status (IN_PROGRESS, CANCELED, FAILED, FAILED_WITH_TERMINAL_ERROR, COMPLETED, COMPLETED_WITH_ERRORS, SCHEDULED, TIMED_OUT, SKIPPED). Task Type Filters workflow executions by its task type. Workflow Name Filters workflow executions by its workflow name. Update Time - From Filters workflow executions by tasks that started on or after the specified time. Update Time - To Filters workflow executions by tasks that started on or before the specified time. Lookback (days) Filters workflow executions by tasks that ran in the last given number of days. Lucene-syntax Query (Double-quote strings for Free Text) (If indexing is enabled) Filters workflow executions by querying task input and output values."},{"location":"devguide/how-tos/Workflows/starting-workflows.html","title":"Starting Workflows","text":"<p>In Conductor, workflows can be started using the Conductor UI, APIs, or SDKs.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#using-conductor-ui","title":"Using Conductor UI","text":"<p>The Conductor UI is useful for sandbox testing before deploying the workflows to production using the APIs or SDKs.</p> <p>To start a workflow:</p> <ol> <li>Go to Workbench in the Conductor UI.</li> <li>Select the  Workflow Name and Workflow version.</li> <li>If required, provide the workflow inputs in Input (JSON).</li> <li>(Optional) Specify the Correlation ID and Task to Domain (JSON) for the execution.</li> <li>Select the \u25b6 icon (Execute Workflow) at the top to run the workflow.</li> </ol> <p>Once the workflow has started, you can view the ongoing execution by selecting the Workflow ID hyperlink in the Execution History side panel on the right.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#using-apis","title":"Using APIs","text":"<p>You can start workflow executions using the Start Workflow API (<code>POST api/workflow/{name}</code>). <code>{name}</code> is the placeholder for the workflow name, and the request body contains the workflow inputs if any.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#example-using-curl","title":"Example using cURL","text":"<p>In this example, a cURL request is used to invoke the workflow <code>sample_workflow</code> with the input <code>service</code>  specified as <code>fedex</code>.</p> <pre><code>curl '{{ server_host }}/api/workflow/sample_workflow' \\\n  -H 'accept: text/plain' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"service\":\"fedex\"}'\n</code></pre>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#using-sdks","title":"Using SDKs","text":"<p>Conductor offers client SDKs for popular languages which have library methods for making the Start Workflow API call. Refer to the SDK documentation to configure a client in your selected language to invoke workflow executions.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#example-using-javascript","title":"Example using JavaScript","text":"<p>In this example, the JavaScript Fetch API is used to invoke the workflow <code>sample_workflow</code> with the input <code>service</code>  specified as <code>fedex</code>.</p> <pre><code>fetch(\"{{ server_host }}/api/workflow/sample_workflow\", {\n    \"headers\": {\n        \"accept\": \"text/plain\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"{\\\"service\\\":\\\"fedex\\\"}\",\n    \"method\": \"POST\",\n});\n</code></pre>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html","title":"Versioning Workflows","text":"<p>Conductor allows you to safely run different workflow versions without disrupting ongoing or scheduled workflow executions in production. </p> <p>Refer to Updating workflows for more information on modifying a workflow and saving it as a new version.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#when-to-version-workflows","title":"When to version workflows","text":"<p>Workflow versioning is useful for various scenarios, like gradually upgrading a process, or rolling out different workflow versions to different user bases.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#example","title":"Example","text":"<p>For example, a new version of your core workflow will add a capability that is required for customerA.  However, customerB will not be ready to implement this code for another 6 months.</p> <p>With workflow versioning, you can begin transitioning traffic onto version 2 for customerA, while customerB remains on version 1. 6 months later, customerB can begin transitioning traffic to version 2 as well.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#runtime-behavior-with-multiple-workflow-versions","title":"Runtime behavior with multiple workflow versions","text":"<p>At runtime, all Conductor workflows will reference a snapshot of the workflow definition at the start of its invocation. In other words, all changes to a workflow definition are decoupled from all of its ongoing workflow executions.</p> <p>Here is an illustration of workflow versions at runtime, when you run workflows based on the latest version, versus when you run workflows based on a specific version.</p> <p></p> <p>In the illustration above, the workflow with version V1 is executed at timestamp T1 and thus uses the workflow definition at that time.</p> <p>At T2, a new workflow version V2 is created.  From that point on, any newly-triggered executions using the latest version will run based on V2, even if V1 gets updated later on at T3.</p> <p>At T3, even when the V1 workflow definition is updated, all existing V1 executions will continue based on the definition at timestamp T1. From that point on, any newly-triggered executions using version V1 will run based on the V1 definition at timestamp T3.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#runtime-behavior-during-restarts","title":"Runtime behavior during restarts","text":"<p>Likewise, by default all workflow restarts, workflow retries, and task reruns will be executed based on the snapshot of the workflow definition at the start of the first execution attempt. If required, you can choose to restart workflows with the latest definitions.</p> <p>Here is an illustration of workflow versions at runtime, when you restart workflows using the current definitions versus using the latest definitions.</p> <p></p> <p>In the illustration above, if a V1 execution is restarted with the current definitions after a new version V2 has been created, the restarted execution will still run based on the V1 definition at T1. This applies even if the same execution is restarted after the V1 definition itself has been updated at T3.</p> <p>At T2, if a V1 execution is restarted with the latest definitions, the V1 execution will restart using the V2 definition instead. This also applies even if the same execution is restarted after the V1 definition itself has been updated at T3.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#upgrading-running-workflows","title":"Upgrading running workflows","text":"<p>Since any changes to a workflow definition will not impact its ongoing executions, running workflows need to be explicitly upgraded if required.</p> <p>Using the Conductor UI or APIs, you can upgrade a running workflow by terminating the execution and restarting it with the latest definition.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#using-conductor-ui","title":"Using Conductor UI","text":"<p>To upgrade a running workflow:</p> <ol> <li>In Executions, select an ongoing workflow to upgrade.</li> <li>In the top right, select Actions &gt; Terminate.</li> <li>Once terminated, select Actions &gt; Restart with Latest Definitions.</li> </ol>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#using-conductor-apis","title":"Using Conductor APIs","text":"<p>The API approach allows you to upgrade running workflows in bulk. Use the Bulk Terminate API (<code>POST /api/workflow/bulk/terminate</code>) to specify a list of ongoing workflows. Then, use the Bulk Restart API (<code>POST /api/workflow/bulk/restart</code>) to restart the terminated workflows.</p>"},{"location":"devguide/how-tos/Workflows/viewing-workflow-executions.html","title":"Viewing Workflow Executions","text":"<p>The Conductor UI provides a convenient interface for viewing workflow executions as visual diagrams. You can view workflow executions:</p> <ul> <li>In Executions, after searching for workflows.</li> <li>In Workbench &gt; Execution History</li> </ul> <p>To view a workflow execution:</p> <p>In Executions or Workbench, select the Workflow ID hyperlink.</p>"},{"location":"devguide/how-tos/Workflows/viewing-workflow-executions.html#workflow-execution-details","title":"Workflow execution details","text":"<p>The following tabs are available for each workflow execution:</p> Tab Name Description Tasks &gt; Diagram Visual diagram of the workflow and its tasks. Tasks &gt; Task List List of the task executions in this workflow, including details like the task name, task ID, status, and so on. Tasks &gt; Timeline Timeline showcasing the duration and sequence of each task in the workflow. Summary Summary view of the workflow execution, which includes the workflow ID, status, duration, and so on. Workflow Input/Output View of the JSON payload for the workflow inputs, outputs, and variables. JSON View of the full workflow execution JSON, including all tasks, inputs, outputs, and so on."},{"location":"devguide/how-tos/Workflows/viewing-workflow-executions.html#workflow-diagram-view","title":"Workflow diagram view","text":"<p>In Tasks &gt; Diagram, you can view the workflow's exact execution path. The executed paths are shown in green and while other alternative paths are greyed out.</p> <p></p> <p>Each task status will also be clearly marked, highlighting any task errors.</p> <p></p>"},{"location":"devguide/how-tos/Workflows/viewing-workflow-executions.html#task-execution-details","title":"Task execution details","text":"<p>You can also view a task's execution details by selecting a task from the following tabs: </p> <ul> <li>Tasks &gt; Diagram </li> <li>Tasks &gt; Task List</li> <li>Tasks &gt; Timeline </li> </ul> <p>This action opens a left-side panel that contains the following tabs:</p> Tab Name Description Summary Summary view of the task execution, which includes the task execution ID, status, duration, and so Input View of the JSON payload for the task inputs. Output View of the JSON payload for the task outputs. Logs View of the log messages logged by the task, if any. JSON View of the full task execution JSON, including retry count, start time, worker ID, and so on. Definition View of the task configuration used when executing the task."},{"location":"devguide/labs/index.html","title":"Guided Tutorial","text":""},{"location":"devguide/labs/index.html#high-level-steps","title":"High Level Steps","text":"<p>Generally, these are the steps necessary in order to put Conductor to work for your business workflow:</p> <ol> <li>Create task worker(s) that poll for scheduled tasks at regular interval</li> <li>Create task definitions for these workers and register them.</li> <li>Create the workflow definition</li> </ol>"},{"location":"devguide/labs/index.html#before-we-begin","title":"Before We Begin","text":"<p>Ensure you have a Conductor instance up and running. This includes both the Server and the UI. We recommend following the Docker Instructions.</p>"},{"location":"devguide/labs/index.html#tools","title":"Tools","text":"<p>For the purpose of testing and issuing API calls, the following tools are useful</p> <ul> <li>Linux cURL command</li> <li>Postman or similar REST client</li> </ul>"},{"location":"devguide/labs/index.html#lets-go","title":"Let's Go","text":"<p>We will begin by defining a simple workflow that utilizes System Tasks. </p> <p>Next</p>"},{"location":"devguide/labs/eventhandlers.html","title":"Events and Event Handlers","text":"<p>In this exercise, we shall:</p> <ul> <li>Publish an Event to Conductor using <code>Event</code> task.</li> <li>Subscribe to Events, and perform actions:<ul> <li>Start a Workflow</li> <li>Complete Task</li> </ul> </li> </ul> <p>Conductor supports eventing with two Interfaces:</p> <ul> <li>Event Task</li> <li>Event Handlers</li> </ul>"},{"location":"devguide/labs/eventhandlers.html#create-workflow-definitions","title":"Create Workflow Definitions","text":"<p>Let's create two workflows:</p> <ul> <li><code>test_workflow_for_eventHandler</code> which will have an <code>Event</code> task to start another workflow, and a <code>WAIT</code> System task that will be completed by an event.</li> <li><code>test_workflow_startedBy_eventHandler</code> which will have an <code>Event</code> task to generate an event to complete <code>WAIT</code> task in the above workflow.</li> </ul> <p>Send <code>POST</code> requests to <code>/metadata/workflow</code> endpoint with below payloads:</p> <pre><code>{\n  \"name\": \"test_workflow_for_eventHandler\",\n  \"description\": \"A test workflow to start another workflow with EventHandler\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"test_start_workflow_event\",\n      \"taskReferenceName\": \"start_workflow_with_event\",\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    },\n    {\n      \"name\": \"test_task_tobe_completed_by_eventHandler\",\n      \"taskReferenceName\": \"test_task_tobe_completed_by_eventHandler\",\n      \"type\": \"WAIT\"\n    }\n  ]\n}\n</code></pre> <pre><code>{\n  \"name\": \"test_workflow_startedBy_eventHandler\",\n  \"description\": \"A test workflow which is started by EventHandler, and then goes on to complete task in another workflow.\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"test_complete_task_event\",\n      \"taskReferenceName\": \"complete_task_with_event\",\n      \"inputParameters\": {\n        \"sourceWorkflowId\": \"${workflow.input.sourceWorkflowId}\"\n      },\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    }\n  ]\n}\n</code></pre>"},{"location":"devguide/labs/eventhandlers.html#event-tasks-in-workflow","title":"Event Tasks in Workflow","text":"<p><code>EVENT</code> task is a System task, and we shall define it just like other Tasks in Workflow, with <code>sink</code> parameter. Also, <code>EVENT</code> task doesn't have to be registered before using in Workflow. This is also true for the <code>WAIT</code> task. Hence, we will not be registering any tasks for these workflows.</p>"},{"location":"devguide/labs/eventhandlers.html#events-are-sent-but-theyre-not-handled-yet","title":"Events are sent, but they're not handled (yet)","text":"<p>Once you try to start <code>test_workflow_for_eventHandler</code> workflow, you would notice that the event is sent successfully, but the second worflow <code>test_workflow_startedBy_eventHandler</code> is not started. We have sent the Events, but we also need to define <code>Event Handlers</code> for Conductor to take any <code>actions</code> based on the Event. Let's create <code>Event Handlers</code>.</p>"},{"location":"devguide/labs/eventhandlers.html#create-event-handlers","title":"Create Event Handlers","text":"<p>Event Handler definitions are pretty much like Task or Workflow definitions. We start by name:</p> <pre><code>{\n  \"name\": \"test_start_workflow\"\n}\n</code></pre> <p>Event Handler should know the Queue it has to listen to. This should be defined in <code>event</code> parameter.</p> <p>When using Conductor queues, define <code>event</code> with format: </p> <p><code>conductor:{workflow_name}:{taskReferenceName}</code></p> <p>And when using SQS, define with format: </p> <p><code>sqs:{my_sqs_queue_name}</code></p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\"\n}\n</code></pre> <p>Event Handler can perform a list of actions defined in <code>actions</code> array parameter, for this particular <code>event</code> queue.</p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\",\n  \"actions\": [\n      \"&lt;insert-actions-here&gt;\"\n  ],\n  \"active\": true\n}\n</code></pre> <p>Let's define <code>start_workflow</code> action. We shall pass the name of workflow we would like to start. The <code>start_workflow</code> parameter can use any of the values from the general Start Workflow Request. Here we are passing in the workflowId, so that the Complete Task Event Handler can use it.</p> <pre><code>{\n    \"action\": \"start_workflow\",\n    \"start_workflow\": {\n        \"name\": \"test_workflow_startedBy_eventHandler\",\n        \"input\": {\n            \"sourceWorkflowId\": \"${workflowInstanceId}\"\n        }\n    }\n}\n</code></pre> <p>Send a <code>POST</code> request to <code>/event</code> endpoint:</p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\",\n  \"actions\": [\n    {\n      \"action\": \"start_workflow\",\n      \"start_workflow\": {\n        \"name\": \"test_workflow_startedBy_eventHandler\",\n        \"input\": {\n          \"sourceWorkflowId\": \"${workflowInstanceId}\"\n        }\n      }\n    }\n  ],\n  \"active\": true\n}\n</code></pre> <p>Similarly, create another Event Handler to complete task.</p> <pre><code>{\n  \"name\": \"test_complete_task_event\",\n  \"event\": \"conductor:test_workflow_startedBy_eventHandler:complete_task_with_event\",\n  \"actions\": [\n    {\n        \"action\": \"complete_task\",\n        \"complete_task\": {\n            \"workflowId\": \"${sourceWorkflowId}\",\n            \"taskRefName\": \"test_task_tobe_completed_by_eventHandler\"\n         }\n    }\n  ],\n  \"active\": true\n}\n</code></pre>"},{"location":"devguide/labs/eventhandlers.html#summary","title":"Summary","text":"<p>After wiring all of the above, starting the <code>test_workflow_for_eventHandler</code> should:</p> <ol> <li>Start <code>test_workflow_startedBy_eventHandler</code> workflow.</li> <li>Sets <code>test_task_tobe_completed_by_eventHandler</code> WAIT task <code>IN_PROGRESS</code>.</li> <li><code>test_workflow_startedBy_eventHandler</code> event task would publish an Event to complete the WAIT task above.</li> <li>Both the workflows would move to <code>COMPLETED</code> state.</li> </ol>"},{"location":"devguide/labs/first-workflow.html","title":"A First Workflow","text":"<p>In this article we will explore how we can run a really simple workflow that runs without deploying any new microservice. </p> <p>Conductor can orchestrate HTTP services out of the box without implementing any code.  We will use that to create and run the first workflow.</p> <p>See System Task for the list of such built-in tasks. Using system tasks is a great way to run a lot of our code in production.</p>"},{"location":"devguide/labs/first-workflow.html#configuring-our-first-workflow","title":"Configuring our First Workflow","text":"<p>This is a sample workflow that we can leverage for our test.</p> <pre><code>{\n  \"name\": \"first_sample_workflow\",\n  \"description\": \"First Sample Workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"get_population_data\",\n      \"taskReferenceName\": \"get_population_data\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&amp;measures=Population\",\n          \"method\": \"GET\"\n        }\n      },\n      \"type\": \"HTTP\"\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"data\": \"${get_population_data.output.response.body.data}\",\n    \"source\": \"${get_population_data.output.response.body.source}\"\n  },\n  \"schemaVersion\": 2,\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"example@email.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0\n}\n</code></pre> <p>This is an example workflow that queries a publicly available JSON API to retrieve some data. This workflow doesn\u2019t require any worker implementation as the tasks in this workflow are managed by the system itself. This is an awesome feature of Conductor. For a lot of typical work, we won\u2019t have to write any code at all.</p> <p>Let's talk about this workflow a little more so that we can gain some context.</p> <pre><code>\"name\" : \"first_sample_workflow\"\n</code></pre> <p>This line here is how we name our workflow. In this case our workflow name is <code>first_sample_workflow</code></p> <p>This workflow contains just one worker. The workers are defined under the key <code>tasks</code>. Here is the worker definition with the most important values:</p> <pre><code>{\n  \"name\": \"get_population_data\",\n  \"taskReferenceName\": \"get_population_data\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&amp;measures=Population\",\n      \"method\": \"GET\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre> <p>Here is a list of fields and what it does:</p> <ol> <li><code>\"name\"</code> : Name of our worker</li> <li><code>\"taskReferenceName\"</code> : This is a reference to this worker in this specific workflow implementation. We can have multiple    workers of the same name in our workflow, but we will need a unique task reference name for each of them. Task    reference name should be unique across our entire workflow.</li> <li><code>\"inputParameters\"</code> : These are the inputs into our worker. We can hard code inputs as we have done here. We can    also provide dynamic inputs such as from the workflow input or based on the output of another worker. We can find    examples of this in our documentation.</li> <li><code>\"type\"</code> : This is what defines what the type of worker is. In our example - this is <code>HTTP</code>. There are more task    types which we can find in the Conductor documentation.</li> <li><code>\"http_request\"</code> : This is an input that is required for tasks of type <code>HTTP</code>. In our example we have provided a well    known internet JSON API url and the type of HTTP method to invoke - <code>GET</code></li> </ol> <p>We haven't talked about the other fields that we can use in our definitions as these are either just metadata or more advanced concepts which we can learn more in the detailed documentation.</p> <p>Ok, now that we have walked through our workflow details, let's run this and see how it works.</p> <p>To configure the workflow, head over to the swagger API of conductor server and access the metadata workflow create API:</p> <p>http://{{ server_host }}/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/metadata-resource/create</p> <p>If the link doesn\u2019t open the right Swagger section, we can navigate to Metadata-Resource \u2192 <code>POST {{ api_prefix }}/metadata/workflow</code></p> <p></p> <p>Paste the workflow payload into the Swagger API and hit Execute.</p> <p>Now if we head over to the UI, we can see this workflow definition created:</p> <p></p> <p>If we click through we can see a visual representation of the workflow:</p> <p></p>"},{"location":"devguide/labs/first-workflow.html#running-our-first-workflow","title":"Running our First Workflow","text":"<p>Let\u2019s run this workflow. To do that we can use the swagger API under the workflow-resources</p> <p>http://{{ server_host }}/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/workflow-resource/startWorkflow_1</p> <p></p> <p>Hit Execute!</p> <p>Conductor will return a workflow id. We will need to use this id to load this up on the UI. If our UI installation has search enabled we wouldn't need to copy this. If we don't have search enabled (using Elasticsearch) copy it from the Swagger UI.</p> <p></p> <p>Ok, we should see this running and get completed soon. Let\u2019s go to the UI to see what happened.</p> <p>To load the workflow directly, use this URL format:</p> <pre><code>http://localhost:5000/execution/&lt;WORKFLOW_ID&gt;\n</code></pre> <p>Replace <code>&lt;WORKFLOW_ID&gt;</code> with our workflow id from the previous step. We should see a screen like below. Click on the different tabs to see all inputs and outputs and task list etc. Explore away!</p> <p></p>"},{"location":"devguide/labs/first-workflow.html#summary","title":"Summary","text":"<p>In this article \u2014 we learned how to run a sample workflow in our Conductor installation. Concepts we touched on:</p> <ol> <li>Workflow creation</li> <li>System tasks such as HTTP</li> <li>Running a workflow via API</li> </ol>"},{"location":"devguide/labs/kitchensink.html","title":"Kitchen Sink","text":"<p>An example kitchensink workflow that demonstrates the usage of all the schema constructs.</p>"},{"location":"devguide/labs/kitchensink.html#definition","title":"Definition","text":"<pre><code>{\n  \"name\": \"kitchensink\",\n  \"description\": \"kitchensink workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"task_1\",\n      \"taskReferenceName\": \"task_1\",\n      \"inputParameters\": {\n        \"mod\": \"${workflow.input.mod}\",\n        \"oddEven\": \"${workflow.input.oddEven}\"\n      },\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"event_task\",\n      \"taskReferenceName\": \"event_0\",\n      \"inputParameters\": {\n        \"mod\": \"${workflow.input.mod}\",\n        \"oddEven\": \"${workflow.input.oddEven}\"\n      },\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    },\n    {\n      \"name\": \"dyntask\",\n      \"taskReferenceName\": \"task_2\",\n      \"inputParameters\": {\n        \"taskToExecute\": \"${workflow.input.task2Name}\"\n      },\n      \"type\": \"DYNAMIC\",\n      \"dynamicTaskNameParam\": \"taskToExecute\"\n    },\n    {\n      \"name\": \"oddEvenDecision\",\n      \"taskReferenceName\": \"oddEvenDecision\",\n      \"inputParameters\": {\n        \"oddEven\": \"${task_2.output.oddEven}\"\n      },\n      \"type\": \"DECISION\",\n      \"caseValueParam\": \"oddEven\",\n      \"decisionCases\": {\n        \"0\": [\n          {\n            \"name\": \"task_4\",\n            \"taskReferenceName\": \"task_4\",\n            \"inputParameters\": {\n              \"mod\": \"${task_2.output.mod}\",\n              \"oddEven\": \"${task_2.output.oddEven}\"\n            },\n            \"type\": \"SIMPLE\"\n          },\n          {\n            \"name\": \"dynamic_fanout\",\n            \"taskReferenceName\": \"fanout1\",\n            \"inputParameters\": {\n              \"dynamicTasks\": \"${task_4.output.dynamicTasks}\",\n              \"input\": \"${task_4.output.inputs}\"\n            },\n            \"type\": \"FORK_JOIN_DYNAMIC\",\n            \"dynamicForkTasksParam\": \"dynamicTasks\",\n            \"dynamicForkTasksInputParamName\": \"input\"\n          },\n          {\n            \"name\": \"dynamic_join\",\n            \"taskReferenceName\": \"join1\",\n            \"type\": \"JOIN\"\n          }\n        ],\n        \"1\": [\n          {\n            \"name\": \"fork_join\",\n            \"taskReferenceName\": \"forkx\",\n            \"type\": \"FORK_JOIN\",\n            \"forkTasks\": [\n              [\n                {\n                  \"name\": \"task_10\",\n                  \"taskReferenceName\": \"task_10\",\n                  \"type\": \"SIMPLE\"\n                },\n                {\n                  \"name\": \"sub_workflow_x\",\n                  \"taskReferenceName\": \"wf3\",\n                  \"inputParameters\": {\n                    \"mod\": \"${task_1.output.mod}\",\n                    \"oddEven\": \"${task_1.output.oddEven}\"\n                  },\n                  \"type\": \"SUB_WORKFLOW\",\n                  \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\",\n                    \"version\": 1\n                  }\n                }\n              ],\n              [\n                {\n                  \"name\": \"task_11\",\n                  \"taskReferenceName\": \"task_11\",\n                  \"type\": \"SIMPLE\"\n                },\n                {\n                  \"name\": \"sub_workflow_x\",\n                  \"taskReferenceName\": \"wf4\",\n                  \"inputParameters\": {\n                    \"mod\": \"${task_1.output.mod}\",\n                    \"oddEven\": \"${task_1.output.oddEven}\"\n                  },\n                  \"type\": \"SUB_WORKFLOW\",\n                  \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\",\n                    \"version\": 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n            \"name\": \"join\",\n            \"taskReferenceName\": \"join2\",\n            \"type\": \"JOIN\",\n            \"joinOn\": [\n              \"wf3\",\n              \"wf4\"\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"search_elasticsearch\",\n      \"taskReferenceName\": \"get_es_1\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      },\n      \"type\": \"HTTP\"\n    },\n    {\n      \"name\": \"task_30\",\n      \"taskReferenceName\": \"task_30\",\n      \"inputParameters\": {\n        \"statuses\": \"${get_es_1.output..status}\",\n        \"workflowIds\": \"${get_es_1.output..workflowId}\"\n      },\n      \"type\": \"SIMPLE\"\n    }\n  ],\n  \"outputParameters\": {\n    \"statues\": \"${get_es_1.output..status}\",\n    \"workflowIds\": \"${get_es_1.output..workflowId}\"\n  },\n  \"ownerEmail\": \"example@email.com\",\n  \"schemaVersion\": 2\n}\n</code></pre>"},{"location":"devguide/labs/kitchensink.html#visual-flow","title":"Visual Flow","text":""},{"location":"devguide/labs/kitchensink.html#running-kitchensink-workflow","title":"Running Kitchensink Workflow","text":"<ol> <li>If you are running Conductor locally, use the <code>-DloadSample=true</code> Java system property when launching the server.  This will create a kitchensink workflow,  related task definitions and kick off an instance of kitchensink workflow. Otherwise, you can create a new Workflow Definition in the UI by copying the sample above.</li> <li>Once the workflow has started, the first task remains in the <code>SCHEDULED</code> state.  This is because no workers are currently polling for the task.</li> <li>We will use the REST endpoints directly to poll for tasks and updating the status.</li> </ol>"},{"location":"devguide/labs/kitchensink.html#start-workflow-execution","title":"Start workflow execution","text":"<p>Start the execution of the kitchensink workflow by posting the following:</p> <p><pre><code>curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' '{{ server_host }}{{ api_prefix }}/workflow/kitchensink' -d '\n{\n    \"task2Name\": \"task_5\" \n}\n'\n</code></pre> The response is a text string identifying the workflow instance id.</p>"},{"location":"devguide/labs/kitchensink.html#poll-for-the-first-task","title":"Poll for the first task:","text":"<pre><code>curl {{ server_host }}{{ api_prefix }}/tasks/poll/task_1\n</code></pre> <p>The response should look something like:</p> <pre><code>{\n    \"taskType\": \"task_1\",\n    \"status\": \"IN_PROGRESS\",\n    \"inputData\": {\n        \"mod\": null,\n        \"oddEven\": null\n    },\n    \"referenceTaskName\": \"task_1\",\n    \"retryCount\": 0,\n    \"seq\": 1,\n    \"pollCount\": 1,\n    \"taskDefName\": \"task_1\",\n    \"scheduledTime\": 1486580932471,\n    \"startTime\": 1486580933869,\n    \"endTime\": 0,\n    \"updateTime\": 1486580933902,\n    \"startDelayInSeconds\": 0,\n    \"retried\": false,\n    \"callbackFromWorker\": true,\n    \"responseTimeoutSeconds\": 3600,\n    \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\",\n    \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\",\n    \"callbackAfterSeconds\": 0,\n    \"polledTime\": 1486580933902,\n    \"queueWaitTime\": 1398\n}\n</code></pre>"},{"location":"devguide/labs/kitchensink.html#update-the-task-status","title":"Update the task status","text":"<ul> <li>Note the values for <code>taskId</code> and <code>workflowInstanceId</code> fields from the poll response</li> <li>Update the status of the task as <code>COMPLETED</code> as below:</li> </ul> <pre><code>curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST {{ server_host }}{{ api_prefix }}/tasks/ -d '\n{\n    \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\",\n    \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\",\n    \"status\": \"COMPLETED\",\n    \"outputData\": {\n        \"mod\": 5,\n        \"taskToExecute\": \"task_1\",\n        \"oddEven\": 0,\n        \"dynamicTasks\": [\n            {\n                \"name\": \"task_1\",\n                \"taskReferenceName\": \"task_1_1\",\n                \"type\": \"SIMPLE\"\n            },\n            {\n                \"name\": \"sub_workflow_4\",\n                \"taskReferenceName\": \"wf_dyn\",\n                \"type\": \"SUB_WORKFLOW\",\n                \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\"\n                }\n            }\n        ],\n        \"inputs\": {\n            \"task_1_1\": {},\n            \"wf_dyn\": {}\n        }\n    }\n}'\n</code></pre> <p>This will mark the task_1 as completed and schedule <code>task_5</code> as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.</p>"},{"location":"devguide/running/docker.html","title":"Running Conductor Using Docker","text":"<p>Learn how to set up Conductor on your local machine using Docker and configure the backend options. </p>"},{"location":"devguide/running/docker.html#running-conductor-using-docker-compose","title":"Running Conductor using Docker Compose","text":"<p>This is the recommended method for setting up Conductor. The Docker Compose will bring up the following:</p> <ul> <li>Conductor API Server</li> <li>Conductor UI</li> <li>Elasticsearch (for searching workflows)</li> </ul> <p>Prerequisites</p> <ul> <li>Docker Desktop</li> <li>Java (JDK) v17 or later</li> <li>Node v14 for the UI to build</li> <li>Recommended host with enough CPU and RAM to run multiple Docker containers (at least 16GB RAM)</li> </ul> <p>To run Conductor using Docker:</p> <ol> <li> <p>Clone the Conductor repository.</p> <pre><code>$ git clone https://github.com/conductor-oss/conductor\n</code></pre> </li> <li> <p>Run Docker Compose.</p> <pre><code>$ cd conductor\nconductor $ docker compose -f docker/docker-compose.yaml up\n</code></pre> </li> </ol> <p>Once up and running, you will see the following containers in your Docker Desktop dashboard:</p> <ul> <li>Elasticsearch</li> <li>Redis</li> <li>Conductor server (contains both the backend and UI)</li> </ul> <p>If you encounter any issues running Conductor from Docker, refer to Troubleshooting below.</p>"},{"location":"devguide/running/docker.html#accessing-conductor","title":"Accessing Conductor","text":"<p>To access the Conductor UI, go to http://localhost:8127.</p> <p></p> <p>To access the REST API Swagger documentation, go to http://localhost:8080/swagger-ui/index.html</p> <p></p>"},{"location":"devguide/running/docker.html#exiting-conductor","title":"Exiting Conductor","text":"<p>To shut down Conductor:</p> <ol> <li>From the terminal, exit Docker Compose using Ctrl + c keys.</li> <li>Run <code>docker-compose down</code> to make sure that the images are stopped.</li> </ol>"},{"location":"devguide/running/docker.html#default-database-specifications","title":"Default database specifications","text":"<p>By default, <code>docker-compose.yaml</code> uses <code>config-redis.properties</code>. The default databases used when you run Conductor with Docker Compose are:</p> <ul> <li>Elasticsearch\u2014Default indexing backend for the UI.</li> <li>Redis\u2014Default persistence store for workflow state and queue management.</li> </ul> <p>However, the choice of backend is quite flexible and can be changed:</p> Default Alternatives Elasticsearch <ul><li>Opensearch</li></ul> Redis <ul><li>Postgres</li><li>MySQL</li><li>Cassandra</li></ul>"},{"location":"devguide/running/docker.html#alternative-persistence-stores","title":"Alternative persistence stores","text":"<p>A selection of <code>docker-compose-*.yaml</code> and <code>config-*.properties</code> files are provided to demonstrate the use of alternative persistence stores.</p> File Containers docker-compose.yaml <ul><li>Redis</li><li>Elasticsearch v7</li><li>Conductor server (includes UI)</li></ul> docker-compose-postgres.yaml <ul><li>Postgres</li><li>Conductor server (includes UI)</li></ul> docker-compose-postgres-es7.yaml <ul><li>Postgres</li><li>Elasticsearch v7</li><li>Conductor server (includes UI)</li></ul> docker-compose-mysql.yaml <ul><li>MySQL</li><li>Redis</li><li>Elasticsearch v7</li><li>Conductor server (includes UI)</li></ul> docker-compose-redis-os.yaml <ul><li>Redis</li><li>Opensearch</li><li>Conductor server (includes UI)</li></ul>"},{"location":"devguide/running/docker.html#running-conductor-with-alternative-persistence-stores","title":"Running Conductor with alternative persistence stores","text":"<p>To run Conductor with a different configuration, simply use the appropriate <code>docker-compose*.yaml</code> file.</p> <p>For example, run the following command to start the server instance backed by a PostgreSQL database:</p> <pre><code>conductor $ docker compose -f docker/docker-compose-postgres.yaml up\n</code></pre>"},{"location":"devguide/running/docker.html#configuring-elasticsearch","title":"Configuring Elasticsearch","text":"<p>By default, Conductor comes packaged with Elasticsearch for the UI's indexing backend. While Elasticsearch is optional, disabling it will disable the search functionality in the UI.</p>"},{"location":"devguide/running/docker.html#disabling-elasticsearch","title":"Disabling Elasticsearch","text":"<p>To disable Elasticsearch:</p> <ol> <li>Set <code>conductor.indexing.enabled=false</code> in config-*.properties files.</li> <li>Comment out all the configuration code related to Elasticsearch. For example: <code>conductor.elasticsearch.url=http://es:9200</code>.</li> </ol>"},{"location":"devguide/running/docker.html#re-enabling-elasticsearch","title":"Re-enabling Elasticsearch","text":"<p>To re-enable Elasticsearch:</p> <ol> <li>Set <code>conductor.indexing.enabled=true</code> in <code>config-*.properties</code> files.</li> <li>Uncomment or add in the configuration code related to Elasticsearch. For example: <code>conductor.elasticsearch.url=http://es:9200</code>.</li> </ol>"},{"location":"devguide/running/docker.html#configuring-opensearch","title":"Configuring Opensearch","text":"<p>To use Opensearch:</p> <ol> <li>Comment out the Elasticsearch import in <code>server/build.gradle</code>.</li> <li>Uncomment the Opensearch import in <code>server/build.gradle</code>.</li> </ol>"},{"location":"devguide/running/docker.html#standalone-images","title":"Standalone images","text":"<p>You can also build and run the Conductor server and UI as standalone images.</p>"},{"location":"devguide/running/docker.html#running-the-standalone-server-image","title":"Running the standalone server image","text":"<p>To run the server image:</p> <ol> <li> <p>Build the <code>conductor:server</code> image from the <code>docker</code> directory.</p> <pre><code>conductor $ cd docker\ndocker $ docker build -t conductor:server -f server/Dockerfile ../\n</code></pre> </li> <li> <p>Run the server image in a container named <code>conductor_server</code>.</p> <pre><code>docker $ docker run -p 8080:8080 -d --name conductor_server conductor:server\n</code></pre> </li> <li> <p>Log in to the running container.</p> <pre><code>docker $  docker exec -it conductor_server /bin/sh\n</code></pre> </li> </ol> <p>The API documentation should now be accessible at http://localhost:8080/swagger-ui/index.html.</p>"},{"location":"devguide/running/docker.html#running-the-standalone-ui-image","title":"Running the standalone UI image","text":"<p>Note</p> <ul> <li>In order for the UI to do anything useful the Conductor server must already be running on port 8080, either in a Docker container (as above), or running directly in the local JRE.</li> <li>Significant parts of the UI will also not be functional without Elastisearch.</li> </ul> <p>Using the <code>docker-compose</code> approach alleviates these considerations.</p> <p>To run the UI image:</p> <ol> <li> <p>Build the <code>conductor:ui</code> image from the <code>docker</code> directory.</p> <pre><code>docker build -t conductor:ui -f ui/Dockerfile ../\n</code></pre> </li> <li> <p>Run the UI image in a container named <code>conductor_ui</code>.</p> <pre><code>docker run -p 8127:5000 -d --name conductor_ui conductor:ui\n</code></pre> </li> </ol> <p>The UI should now be accessible at http://localhost:8127.</p>"},{"location":"devguide/running/docker.html#troubleshooting","title":"Troubleshooting","text":"<p>To troubleshoot a failed start, check the server logs located at <code>/app/logs</code> (default directory in dockerfile).</p> <pre><code>docker logs &lt;container-name&gt;\n</code></pre> <p>Here are some common issues when using Docker images and ways to resolve them:</p> Issue Fix Not enough memory You need at least 16 GB of memory to run everything. You can modify the Docker Compose to skip using Elasticsearch if you have no option to run this with your memory options.  Refer to Disabling Elasticsearch for more information. Elasticsearch fails to come up in arm64-based CPU machines Elasticsearch v6.8.x does not have an arm64-based Docker image. Make sure you are using Elasticsearch v7 and later. Server times out at start up because Elasticsearch remains in Yellow health By default, the Conductor server requires Elasticsearch to be in Green state so that it can run when indexing is enabled. To work around this, use one of the following solutions: <ul><li>Set the following property <code>conductor.elasticsearch.clusterHealthColor=yellow</code> in <code>config-*.properties</code> files.</li> <li>Spin up another cluster (more than one node) to prevent the timeout</li></ul> Refer to the GitHub issue. Changes in <code>config-*.properties</code> files are not taking effect The configuration files are copied into the Docker image when it is built. For the changes to take effect, you must rebuild the image.  For best practices, you can mount the configuration files as a Docker volume instead to reflect the new changes automatically without rebuilding. Unable to access the Conductor server on port 8080 It may takes some time for Conductor server to start. Wait before trying again, and if it is still not loading, check the server log for errors."},{"location":"devguide/running/hosted.html","title":"Hosted Solutions","text":""},{"location":"devguide/running/hosted.html#orkes","title":"Orkes","text":"<p>Orkes offers a cloud-hosted, enterprise-grade version of Conductor, enabling teams to get started with minimal operational overhead. Besides full compatibility with Conductor OSS, Orkes Conductor provides additional features not available in the open source release.</p> <p>Here are the options for using Conductor via Orkes:</p> <ul> <li>Developer Edition</li> <li>Cloud Hosting Plans</li> </ul> <p>Orkes also operates a Discourse forum for the community to discuss and share how to use Conductor.</p>"},{"location":"devguide/running/hosted.html#developer-edition","title":"Developer Edition","text":"<p>The free Orkes Developer Edition for Conductor is available at developer.orkescloud.com. The Developer Edition comes with all of Orkes' enterprise features, including a visual workflow editor, AI orchestration suite, event-driven connectors, human-in-the-loop tasks, and more. You can create and execute workflows from the UI or API.</p>"},{"location":"devguide/running/hosted.html#cloud-hosted-conductor","title":"Cloud Hosted Conductor","text":"<p>Orkes provides multiple options of hosted Conductor clusters in the cloud (AWS, Azure, and GCP, in addition to private clouds) with enterprise support provided by the Orkes team. Learn more about Orkes Cloud here.</p>"},{"location":"devguide/running/source.html","title":"Building Conductor From Source","text":"<p>Learn how you can set up the Conductor server and UI on your local machine by building it from source. </p> <p>When building from source, the default configuration comes with in-memory persistence and no indexing. This means all data will be wiped once the server is terminated, and the search functionality in the UI will not work. This set-up is useful for testing or demo only.</p> <p>You can run Conductor using Docker to download Conductor with persistence and indexing already configured.</p>"},{"location":"devguide/running/source.html#building-conductor-server-from-source","title":"Building Conductor server from source","text":"<p>[!NOTE]  \"Prerequisites\"  * Java (JDK) v17  * (Optional) Docker for running tests</p> <p>To build Conductor server from source:</p> <ol> <li> <p>Clone the Conductor repository.</p> <pre><code>$ git clone https://github.com/conductor-oss/conductor.git\n</code></pre> </li> <li> <p>(For Mac users) If you are using a new Mac with an Apple silicon chip, you must modify the <code>conductor/grpc/build.gradle</code> file by adding \"osx-x86_64\" to the following plugin:     <pre><code>protobuf {\n    plugins {\n        grpc {\n            artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\"\n        }\n    }\n...\n}\n</code></pre></p> </li> <li> <p>(For Mac users) If you are using a new Mac with an Apple silicon chip, you may also need to install Rosetta:</p> <pre><code>softwareupdate --install-rosetta\n</code></pre> </li> <li> <p>Run Conductor with Gradle.</p> <pre><code>$ cd conductor/server\nserver $ ../gradlew bootRun\n</code></pre> <p>To run Conductor, with a specific configuration file, specify <code>CONFIG_PROP</code>.</p> <pre><code># Ensure all other services have been started before running the server\nserver $ CONFIG_PROP=config.properties ../gradlew bootRun\n</code></pre> </li> </ol> <p>The API documentation should now be accessible at http://localhost:8080/swagger-ui/index.html.</p> <p></p>"},{"location":"devguide/running/source.html#running-conductor-server-from-a-pre-compiled-jar","title":"Running Conductor server from a pre-compiled JAR","text":"<p>As an alternative to building from source, you can download and run the pre-compiled JAR.</p> <pre><code>export CONDUCTOR_VER=3.21.10\nexport REPO_URL=https://repo1.maven.org/maven2/org/conductoross/conductor-server\ncurl $REPO_URL/$CONDUCTOR_VER/conductor-core-$CONDUCTOR_VER-boot.jar \\\n--output conductor-core-$CONDUCTOR_VER-boot.jar; java -jar conductor-core-$CONDUCTOR_VER-boot.jar\n</code></pre> <p>The API documentation should now be accessible at http://localhost:8080/swagger-ui/index.html.</p>"},{"location":"devguide/running/source.html#running-conductor-ui-from-source","title":"Running Conductor UI from source","text":"<p>[!NOTE] \"Prerequisites\" * A running Conductor server on port 8080 * Node v14 for the UI to build * Yarn for building and running the UI</p> <p>The UI is a standard <code>create-react-app</code> React single page application (SPA).</p> <p>To run Conductor UI from source:</p> <ol> <li> <p>Run <code>yarn install</code> from the <code>/ui</code> directory to retrieve package dependencies.</p> <pre><code>$ cd conductor/ui\nui $ yarn install\n</code></pre> </li> <li> <p>Run the UI on the bundled development server using <code>yarn run start</code>.</p> <pre><code>ui $ yarn run start\n</code></pre> </li> </ol> <p>The UI should now be accessible at http://localhost:5000.</p> <p></p> <p>[!NOTE] To use the UI locally, there is no need to build the project. If you require compiled assets to host on a production web server, you can build the project using <code>yarn build</code>.</p>"},{"location":"documentation/advanced/annotation-processor.html","title":"Annotation Processor","text":"<p>This module is strictly for code generation tasks during builds based on annotations. Currently supports <code>protogen</code></p>"},{"location":"documentation/advanced/annotation-processor.html#usage","title":"Usage","text":"<p>This is an actual example of this module which is implemented in common/build.gradle</p> <pre><code>task protogen(dependsOn: jar, type: JavaExec) {\n    classpath configurations.annotationsProcessorCodegen\n    main = 'com.netflix.conductor.annotationsprocessor.protogen.ProtoGenTask'\n    args(\n            \"conductor.proto\",\n            \"com.netflix.conductor.proto\",\n            \"github.com/netflix/conductor/client/gogrpc/conductor/model\",\n            \"${rootDir}/grpc/src/main/proto\",\n            \"${rootDir}/grpc/src/main/java/com/netflix/conductor/grpc\",\n            \"com.netflix.conductor.grpc\",\n            jar.archivePath,\n            \"com.netflix.conductor.common\",\n    )\n}\n</code></pre>"},{"location":"documentation/advanced/archival-of-workflows.html","title":"Archiving Workflows","text":"<p>Conductor has support for archiving workflow upon termination or completion. Enabling this will delete the workflow from the configured database, but leave the associated data in Elasticsearch so it is still searchable. </p> <p>To enable, set the <code>conductor.workflow-status-listener.type</code> property to <code>archive</code>.</p> <p>A number of additional properties are available to control archival.</p> Property Default Value Description conductor.workflow-status-listener.archival.ttlDuration 0s The time to live in seconds for workflow archiving module. Currently, only RedisExecutionDAO supports this conductor.workflow-status-listener.archival.delayQueueWorkerThreadCount 5 The number of threads to process the delay queue in workflow archival conductor.workflow-status-listener.archival.delaySeconds 60 The time to delay the archival of workflow"},{"location":"documentation/advanced/azureblob-storage.html","title":"Azure Blob Storage","text":"<p>The AzureBlob storage module uses azure blob to store and retrieve workflows/tasks input/output payload that went over the thresholds defined in properties named <code>conductor.[workflow|task].[input|output].payload.threshold.kb</code>.</p> <p>Warning Azure Java SDK use libs already present inside <code>conductor</code> like <code>jackson</code> and <code>netty</code>. You may encounter deprecated issues, or conflicts and need to adapt the code if the module is not maintained along with <code>conductor</code>. It has only been tested with v12.2.0.</p>"},{"location":"documentation/advanced/azureblob-storage.html#configuration","title":"Configuration","text":""},{"location":"documentation/advanced/azureblob-storage.html#example","title":"Example","text":"<pre><code>conductor.additional.modules=com.netflix.conductor.azureblob.AzureBlobModule\nes.set.netty.runtime.available.processors=false\n\nworkflow.external.payload.storage=AZURE_BLOB\nworkflow.external.payload.storage.azure_blob.connection_string=DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;EndpointSuffix=localhost\nworkflow.external.payload.storage.azure_blob.signedurlexpirationseconds=360\n</code></pre>"},{"location":"documentation/advanced/azureblob-storage.html#testing","title":"Testing","text":"<p>You can use Azurite to simulate an Azure Storage.</p>"},{"location":"documentation/advanced/azureblob-storage.html#troubleshoots","title":"Troubleshoots","text":"<ul> <li>When using es5 persistence you will receive an <code>java.lang.IllegalStateException</code> because the Netty lib will call <code>setAvailableProcessors</code> two times. To resolve this issue you need to set the following system property</li> </ul> <pre><code>es.set.netty.runtime.available.processors=false\n</code></pre> <p>If you want to change the default HTTP client of azure sdk, you can use <code>okhttp</code> instead of <code>netty</code>. For that you need to add the following dependency.</p> <pre><code>com.azure:azure-core-http-okhttp:${compatible version}\n</code></pre>"},{"location":"documentation/advanced/extend.html","title":"Extending Conductor","text":""},{"location":"documentation/advanced/extend.html#backend","title":"Backend","text":"<p>Conductor provides a pluggable backend.  The current implementation uses Dynomite.</p> <p>There are 4 interfaces that need to be implemented for each backend:</p> <pre><code>//Store for workflow and task definitions\ncom.netflix.conductor.dao.MetadataDAO\n</code></pre> <pre><code>//Store for workflow executions\ncom.netflix.conductor.dao.ExecutionDAO\n</code></pre> <pre><code>//Index for workflow executions\ncom.netflix.conductor.dao.IndexDAO\n</code></pre> <pre><code>//Queue provider for tasks\ncom.netflix.conductor.dao.QueueDAO\n</code></pre> <p>It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others.</p>"},{"location":"documentation/advanced/extend.html#system-tasks","title":"System Tasks","text":"<p>To create system tasks follow the steps below:</p> <ul> <li>Extend <code>com.netflix.conductor.core.execution.tasks.WorkflowSystemTask</code></li> <li>Instantiate the new class as part of the startup (eager singleton)</li> <li>Implement the <code>TaskMapper</code> interface</li> <li>Add this implementation to the map identified by TaskMappers</li> </ul>"},{"location":"documentation/advanced/extend.html#external-payload-storage","title":"External Payload Storage","text":"<p>To configure conductor to externalize the storage of large payloads:</p> <ul> <li>Implement the <code>ExternalPayloadStorage</code> interface.</li> <li>Add the storage option to the enum here.</li> <li>Set this JVM system property <code>workflow.external.payload.storage</code> to the value of the enum element added above.</li> <li>Add a binding similar to this.</li> </ul>"},{"location":"documentation/advanced/extend.html#workflow-status-listener","title":"Workflow Status Listener","text":"<p>To provide a notification mechanism upon completion/termination of workflows:</p> <ul> <li>Implement the <code>WorkflowStatusListener</code> interface</li> <li>This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state.</li> </ul>"},{"location":"documentation/advanced/extend.html#locking-service","title":"Locking Service","text":"<p>By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module,  for eg., with Dynomite and Redlock:</p> <ul> <li>Implement <code>Lock</code> interface.</li> <li>Add a binding similar to this</li> <li>Enable locking service: <code>conductor.app.workflowExecutionLockEnabled: true</code></li> </ul>"},{"location":"documentation/advanced/extend.html#event-handling","title":"Event Handling","text":"<p>Provide the implementation of EventQueueProvider.</p> <p>E.g. SQS Queue Provider:  SQSEventQueueProvider.java </p>"},{"location":"documentation/advanced/externalpayloadstorage.html","title":"External Payload Storage","text":"<p>Warning</p> <p>The external payload storage is currently only implemented to be used to by the Java client. Client libraries in other languages need to be modified to enable this. Contributions are welcomed.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#context","title":"Context","text":"<p>Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#barriers","title":"Barriers","text":"<p>Conductor typically applies two kinds of barriers:</p> <ul> <li>Soft Barrier</li> <li>Hard Barrier</li> </ul>"},{"location":"documentation/advanced/externalpayloadstorage.html#soft-barrier","title":"Soft Barrier","text":"<p>The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process.  </p>"},{"location":"documentation/advanced/externalpayloadstorage.html#hard-barrier","title":"Hard Barrier","text":"<p>The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#usage","title":"Usage","text":""},{"location":"documentation/advanced/externalpayloadstorage.html#barriers-setup","title":"Barriers setup","text":"<p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value conductor.app.workflowInputPayloadSizeThreshold Soft barrier for workflow input payload in KB 5120 conductor.app.maxWorkflowInputPayloadSizeThreshold Hard barrier for workflow input payload in KB 10240 conductor.app.workflowOutputPayloadSizeThreshold Soft barrier for workflow output payload in KB 5120 conductor.app.maxWorkflowOutputPayloadSizeThreshold Hard barrier for workflow output payload in KB 10240 conductor.app.taskInputPayloadSizeThreshold Soft barrier for task input payload in KB 3072 conductor.app.maxTaskInputPayloadSizeThreshold Hard barrier for task input payload in KB 10240 conductor.app.taskOutputPayloadSizeThreshold Soft barrier for task output payload in KB 3072 conductor.app.maxTaskOutputPayloadSizeThreshold Hard barrier for task output payload in KB 10240"},{"location":"documentation/advanced/externalpayloadstorage.html#amazon-s3","title":"Amazon S3","text":"<p>Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: <pre><code>conductor.external-payload-storage.type=S3\n</code></pre></p> <p>Note</p> <p>This implementation assumes that S3 access is configured on the instance.</p> <p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value conductor.external-payload-storage.s3.bucketName S3 bucket where the payloads will be stored conductor.external-payload-storage.s3.signedUrlExpirationDuration The expiration time in seconds of the signed url for the payload 5 <p>The payloads will be stored in the bucket configured above in a <code>UUID.json</code> file at locations determined by the type of the payload. See here for information about how the object key is determined.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#azure-blob-storage","title":"Azure Blob Storage","text":"<p>ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage.  </p> <p>To build conductor with azure blob feature read the README.md in <code>azureblob-storage</code> module </p> <p>Note</p> <p>This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token. If you want signed url to expired you must specify a Connection String. </p> <p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions <code>Read</code> and <code>Write</code> on Resource <code>Object</code> on Service <code>Blob</code>. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored <code>conductor-payloads</code> workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ <p>The payloads will be stored as done in Amazon S3.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#postgresql-storage","title":"PostgreSQL Storage","text":"<p>Frinx provides an implementation of PostgreSQL Storage used to externalize large payload storage.</p> <p>Note</p> <p>This implementation assumes that you have an PostgreSQL database server with all required credentials.</p> <p>Set the following properties to your application.properties:</p> Property Description default value conductor.external-payload-storage.postgres.conductor-url URL, that can be used to pull the json configurations, that will be downloaded from PostgreSQL to the conductor server. For example: for local development it is <code>{{ server_host }}</code> <code>\"\"</code> conductor.external-payload-storage.postgres.url PostgreSQL database connection URL. Required to connect to database. conductor.external-payload-storage.postgres.username Username for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.password Password for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.table-name The PostgreSQL schema and table name where the payloads will be stored <code>external.external_payload</code> conductor.external-payload-storage.postgres.max-data-rows Maximum count of data rows in PostgreSQL database. After overcoming this limit, the oldest data will be deleted. Long.MAX_VALUE (9223372036854775807L) conductor.external-payload-storage.postgres.max-data-days Maximum count of days of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-months Maximum count of months of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-years Maximum count of years of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 1 <p>The maximum date age for fields in the database will be: <code>years + months + days</code> The payloads will be stored in PostgreSQL database with key (externalPayloadPath) <code>UUID.json</code> and you can generate URI for this data using <code>external-postgres-payload-resource</code> rest controller.  To make this URI work correctly, you must correctly set the conductor-url property.</p>"},{"location":"documentation/advanced/isolationgroups.html","title":"Isolation Groups","text":"<p>Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency.</p> <p>We can isolate the execution of such tasks to have predictable performance using <code>isolationgroupId</code>, a property of task definition.</p> <p>When we set isolationGroupId,  the executor <code>SystemTaskWorkerCoordinator</code> will allocate an isolated queue and an isolated thread pool for execution of those tasks.</p> <p>If no <code>isolationgroupId</code> is specified in task definition, then fallback is default behaviour where the executor executes the task in shared thread-pool for all tasks. </p>"},{"location":"documentation/advanced/isolationgroups.html#example","title":"Example","text":"<p>** Task Definition ** <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"isolationgroupId\": \"myIsolationGroupId\"\n}\n</code></pre> ** Workflow Definition ** <pre><code>{\n  \"name\": \"encode_and_deploy\",\n  \"description\": \"Encodes a file and deploys to CDN\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"encode\",\n      \"taskReferenceName\": \"encode\",\n      \"type\": \"HTTP\", \n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      }\n    }\n  ],\n  \"outputParameters\": {\n    \"cdn_url\": \"${d1.output.location}\"\n  },\n  \"failureWorkflow\": \"cleanup_encode_resources\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre></p> <ul> <li>puts <code>encode</code> in <code>HTTP-myIsolationGroupId</code> queue, and allocates a new thread pool for this for execution.</li> </ul> <p>Note:   To enable this feature, the <code>workflow.isolated.system.task.enable</code> property needs to be made <code>true</code>,its default value is <code>false</code></p> <p>The property <code>workflow.isolated.system.task.worker.thread.count</code>  sets the thread pool size for isolated tasks; default is <code>1</code>.</p> <p>isolationGroupId is currently supported only in HTTP and kafka Task. </p>"},{"location":"documentation/advanced/isolationgroups.html#execution-name-space","title":"Execution Name Space","text":"<p><code>executionNameSpace</code> A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally.</p> <p>Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per <code>isolationGroupId</code>.  Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. </p> <p>To support JVM isolation, and also allow the executors to scale horizontally, we can use <code>executionNameSpace</code> property in taskdef.</p> <p>Executor consumes tasks whose executionNameSpace matches with the configuration property <code>workflow.system.task.worker.executionNameSpace</code></p> <p>If the property is not set, the executor executes tasks without any executionNameSpace set. </p> <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"executionNameSpace\": \"myExecutionNameSpace\"\n}\n</code></pre>"},{"location":"documentation/advanced/isolationgroups.html#example-workflow-task","title":"Example Workflow task","text":"<pre><code>{ \n  \"name\": \"encode_and_deploy\",\n  \"description\": \"Encodes a file and deploys to CDN\",\n  \"version\": 1,\n  \"tasks\": [\n    { \n      \"name\": \"encode\",\n      \"taskReferenceName\": \"encode\",\n      \"type\": \"HTTP\", \n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      }\n    }\n  ],\n  \"outputParameters\": {\n    \"cdn_url\": \"${d1.output.location}\"\n  },\n  \"failureWorkflow\": \"cleanup_encode_resources\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <ul> <li><code>encode</code> task is executed by the executor deployment whose <code>workflow.system.task.worker.executionNameSpace</code> property is <code>myExecutionNameSpace</code> </li> </ul> <p><code>executionNameSpace</code> can be used along with <code>isolationGroupId</code></p> <p>If the above task contains a isolationGroupId <code>myIsolationGroupId</code>, the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace</p>"},{"location":"documentation/advanced/postgresql.html","title":"PostgreSQL","text":"<p>By default conductor runs with an in-memory Redis mock. However, you can run Conductor against PostgreSQL which provides workflow management, queues, indexing, and locking. There are a number of configuration options that enable you to use more or less of PostgreSQL functionality for your needs. It has the benefit of requiring fewer moving parts for the infrastructure, but does not scale as well to handle high volumes of workflows. You should benchmark Conductor with Postgres against your specific workload to be sure.</p>"},{"location":"documentation/advanced/postgresql.html#configuration","title":"Configuration","text":"<p>To enable the basic use of PostgreSQL to manage workflow metadata, set the following property:</p> <pre><code>conductor.db.type=postgres\nspring.datasource.url=jdbc:postgresql://postgres:5432/conductor\nspring.datasource.username=conductor\nspring.datasource.password=password\n# optional\nconductor.postgres.schema=public\n</code></pre> <p>To also use PostgreSQL for queues, you can set:</p> <pre><code>conductor.queue.type=postgres\n</code></pre> <p>You can also use PostgreSQL to index workflows, configure this as follows:</p> <pre><code>conductor.indexing.enabled=true\nconductor.indexing.type=postgres\nconductor.elasticsearch.version=0\n</code></pre> <p>To use PostgreSQL for locking, set the following configurations: <pre><code>conductor.app.workflowExecutionLockEnabled=true\nconductor.workflow-execution-lock.type=postgres\n</code></pre></p>"},{"location":"documentation/advanced/postgresql.html#performance-optimisations","title":"Performance Optimisations","text":""},{"location":"documentation/advanced/postgresql.html#poll-data-caching","title":"Poll Data caching","text":"<p>By default, Conductor writes the latest poll for tasks to the database so that it can be used to determine which tasks and domains are active. This creates a lot of database traffic. To avoid some of this traffic you can configure the PollDataDAO with a write buffer so that it only flushes every x milliseconds. If you keep this value around 5s then there should be no impact on behaviour. Conductor uses a default duration of 10s to determine whether a queue for a domain is active or not (also configurable using <code>conductor.app.activeWorkerLastPollTimeout</code>) so this will ensure that there is plenty of time for the data to get to the database to be shared by other instances:</p> <pre><code># Flush the data every 5 seconds\nconductor.postgres.pollDataFlushInterval=5000\n</code></pre> <p>You can also configure a duration when the cached poll data will be considered stale. This means that the PollDataDAO will try to use the cached data, but if it is older than the configured period, it will check against the database. There is no downside to setting this as if this Conductor node already can confirm that the queue is active then there's no need to go to the database. If the record in the cache is out of date, then we still go to the database to check.</p> <pre><code># Data older than 5 seconds is considered stale\nconductor.postgres.pollDataCacheValidityPeriod=5000\n</code></pre>"},{"location":"documentation/advanced/postgresql.html#workflow-and-task-indexing-on-status-change","title":"Workflow and Task indexing on status change","text":"<p>If you have a workflow with many tasks, Conductor will index that workflow every time a task completes which can result in a lot of extra load on the database. By setting this parameter you can configure Conductor to only index the workflow when its status changes:</p> <pre><code>conductor.postgres.onlyIndexOnStatusChange=true\n</code></pre>"},{"location":"documentation/advanced/postgresql.html#control-over-what-gets-indexed","title":"Control over what gets indexed","text":"<p>By default Conductor will index both workflows and tasks to enable searching via the UI. If you find that you don't search for tasks, but only workflows, you can use the following option to disable task indexing:</p> <pre><code>conductor.app.taskIndexingEnabled=false\n</code></pre>"},{"location":"documentation/advanced/postgresql.html#experimental-listennotify-based-queues","title":"Experimental LISTEN/NOTIFY based queues","text":"<p>By default, Conductor will query the queues in the database 10 times per second for every task, which can result in a lot of traffic. By enabling this option, Conductor makes use of LISTEN/NOTIFY to use triggers that distribute metadata about the state of the queues to all of the Conductor servers. This drastically reduces the load on the database because a single message containing the state of the queues is sent to all subscribers. Enable it as follows:</p> <pre><code>conductor.postgres.experimentalQueueNotify=true\n</code></pre> <p>You can also configure how long Conductor will wait before considering a notification stale using the following property:</p> <pre><code># Data older than 5 seconds is considered stale\nconductor.postgres.experimentalQueueNotifyStalePeriod=5000\n</code></pre>"},{"location":"documentation/advanced/redis.html","title":"Redis","text":"<p>By default conductor runs with an in-memory Redis mock. However, you can change the configuration by setting the properties mentioned below.</p>"},{"location":"documentation/advanced/redis.html#conductordbtype-and-conductorqueuetype","title":"<code>conductor.db.type</code> and <code>conductor.queue.type</code>","text":"Value Description dynomite Dynomite Cluster. Dynomite is a proxy layer that provides sharding and replication. memory Uses an in-memory Redis mock. Should be used only for development and testing purposes. redis_cluster Redis Cluster configuration. redis_sentinel Redis Sentinel configuration. redis_standalone Redis Standalone configuration."},{"location":"documentation/advanced/redis.html#conductorredishosts","title":"<code>conductor.redis.hosts</code>","text":"<p>Expected format is <code>host:port:rack</code> separated by semicolon, e.g.: </p> <pre><code>conductor.redis.hosts=host0:6379:us-east-1c;host1:6379:us-east-1c;host2:6379:us-east-1c\n</code></pre>"},{"location":"documentation/advanced/redis.html#conductorredisdatabase","title":"<code>conductor.redis.database</code>","text":"<p>Redis database value other than default of 0 is supported in sentinel and standalone configurations.  Redis cluster mode only uses database 0, and the configuration is ignored.</p> <pre><code>conductor.redis.database=1\n</code></pre>"},{"location":"documentation/advanced/redis.html#conductorredisusername","title":"<code>conductor.redis.username</code>","text":"<p>Redis ACL using username and password authentication is now supported. </p> <p>The username property should be set as <code>conductor.redis.username</code>, e.g.: <pre><code>conductor.redis.username=conductor\n</code></pre> If not set, the client uses <code>default</code> as the username.</p> <p>The password should be set as the 4th param of the first host <code>host:port:rack:password</code>, e.g.:</p> <pre><code>conductor.redis.hosts=host0:6379:us-east-1c:my_str0ng_pazz;host1:6379:us-east-1c;host2:6379:us-east-1c\n</code></pre> <p>Notes</p> <ul> <li>In a cluster, all nodes use the same username and password.</li> <li>In a sentinel configuration, sentinels and redis nodes use the same database index, username, and password.</li> </ul>"},{"location":"documentation/api/index.html","title":"API Specification","text":"<p>See the following sections for API endpoint documentation. </p> <ul> <li>Metadata API</li> <li>Start Workflow API</li> <li>Workflow API</li> <li>Task API</li> </ul> <p>Task Domains can be used to address tasks to specific pools of workers at runtime.</p>"},{"location":"documentation/api/index.html#swagger-ui","title":"Swagger UI","text":"<p>As an alternative resource, the Swagger UI <code>{{ server_host }}/swagger_ui/index.html</code> will always have the definitive interface description.</p>"},{"location":"documentation/api/metadata.html","title":"Metadata API","text":""},{"location":"documentation/api/metadata.html#workflow-metadata","title":"Workflow Metadata","text":"Endpoint Description Input <code>GET {{ api_prefix }}/metadata/workflow</code> Get all the workflow definitions n/a <code>POST {{ api_prefix }}/metadata/workflow</code> Register new workflow Workflow Definition <code>PUT {{ api_prefix }}/metadata/workflow</code> Register/Update new workflows List of Workflow Definition <code>GET {{ api_prefix }}/metadata/workflow/{name}?version=</code> Get the workflow definitions workflow name, version (optional)"},{"location":"documentation/api/metadata.html#task-metadata","title":"Task Metadata","text":"Endpoint Description Input <code>GET {{ api_prefix }}/metadata/taskdefs</code> Get all the task definitions n/a <code>GET {{ api_prefix }}/metadata/taskdefs/{taskType}</code> Retrieve task definition Task Name <code>POST {{ api_prefix }}/metadata/taskdefs</code> Register new task definitions List of Task Definitions <code>PUT {{ api_prefix }}/metadata/taskdefs</code> Update a task definition A Task Definition <code>DELETE {{ api_prefix }}/metadata/taskdefs/{taskType}</code> Delete a task definition Task Name"},{"location":"documentation/api/startworkflow.html","title":"Start Workflow API","text":""},{"location":"documentation/api/startworkflow.html#api-parameters","title":"API Parameters","text":"<p>When starting a Workflow execution with a registered definition, <code>{{ api_prefix }}/workflow</code> accepts following parameters in the <code>POST</code> payload:</p> Field Description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef An adhoc Workflow Definition to run, without registering. See Dynamic Workflows. optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional"},{"location":"documentation/api/startworkflow.html#output","title":"Output","text":"<p>On success, this API returns the ID of the workflow.</p>"},{"location":"documentation/api/startworkflow.html#basic-example","title":"Basic Example","text":"<p><code>POST {{ server_host }}{{ api_prefix }}/workflow</code> with payload body:</p> <pre><code>{\n  \"name\": \"myWorkflow\", // Name of the workflow\n  \"version\": 1, // Version\n  \"correlationId\": \"corr1\", // Correlation Id\n  \"priority\": 1, // Priority\n    \"input\": { // Input Value Map\n      \"param1\": \"value1\",\n      \"param2\": \"value2\"\n    },\n  \"taskToDomain\": {\n    // Task to domain map\n  }\n}\n</code></pre>"},{"location":"documentation/api/startworkflow.html#dynamic-workflows","title":"Dynamic Workflows","text":"<p>If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used.</p> <p>This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the <code>workflowDef</code> parameter, avoiding the need to register the blueprints before execution.</p> <p>Example:</p> <p>Send a <code>POST</code> request to <code>{{ api_prefix }}/workflow</code> with payload like: <pre><code>{\n  \"name\": \"my_adhoc_unregistered_workflow\",\n  \"workflowDef\": {\n    \"ownerApp\": \"my_owner_app\",\n    \"ownerEmail\": \"my_owner_email@test.com\",\n    \"createdBy\": \"my_username\",\n    \"name\": \"my_adhoc_unregistered_workflow\",\n    \"description\": \"Test Workflow setup\",\n    \"version\": 1,\n    \"tasks\": [\n        {\n            \"name\": \"fetch_data\",\n            \"type\": \"HTTP\",\n            \"taskReferenceName\": \"fetch_data\",\n            \"inputParameters\": {\n              \"http_request\": {\n                \"connectionTimeOut\": \"3600\",\n                \"readTimeOut\": \"3600\",\n                \"uri\": \"${workflow.input.uri}\",\n                \"method\": \"GET\",\n                \"accept\": \"application/json\",\n                \"content-Type\": \"application/json\",\n                \"headers\": {\n                }\n              }\n            },\n            \"taskDefinition\": {\n                \"name\": \"fetch_data\",\n                \"retryCount\": 0,\n                \"timeoutSeconds\": 3600,\n                \"timeoutPolicy\": \"TIME_OUT_WF\",\n                \"retryLogic\": \"FIXED\",\n                \"retryDelaySeconds\": 0,\n                \"responseTimeoutSeconds\": 3000\n            }\n        }\n    ],\n    \"outputParameters\": {\n    }\n  },\n  \"input\": {\n    \"uri\": \"http://www.google.com\"\n  }\n}\n</code></pre></p> <p>Note</p> <p>If the <code>taskDefinition</code> is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.</p>"},{"location":"documentation/api/task.html","title":"Task API","text":""},{"location":"documentation/api/task.html#manage-tasks","title":"Manage Tasks","text":"Endpoint Description <code>GET {{ api_prefix }}/tasks/{taskId}</code> Get task details. <code>GET {{ api_prefix }}/tasks/queue/all</code> List the pending task sizes. <code>GET {{ api_prefix }}/tasks/queue/all/verbose</code> Same as above, includes the size per shard <code>GET {{ api_prefix }}/tasks/queue/sizes?taskType=&amp;taskType=&amp;taskType</code> Return the size of pending tasks for given task types"},{"location":"documentation/api/task.html#polling-ack-and-update-task","title":"Polling, Ack and Update Task","text":"<p>These endpoints are used by the worker to poll for task, send ack (after polling) and finally updating the task result. They typically should not be invoked manually.</p> Endpoint Description <code>GET {{ api_prefix }}/tasks/poll/{taskType}?workerid=&amp;domain=</code> Poll for a task. <code>workerid</code> identifies the worker that polled for the job and <code>domain</code> allows the poller to poll for a task in a specific domain <code>GET {{ api_prefix }}/tasks/poll/batch/{taskType}?count=&amp;timeout=&amp;workerid=&amp;domain</code> Poll for a task in a batch specified by <code>count</code>.  This is a long poll and the connection will wait until <code>timeout</code> or if there is at-least 1 item available, whichever comes first.<code>workerid</code> identifies the worker that polled for the job and <code>domain</code> allows the poller to poll for a task in a specific domain <code>POST {{ api_prefix }}/tasks</code> Update the result of task execution.  See the schema below."},{"location":"documentation/api/task.html#schema-for-updating-task-result","title":"Schema for updating Task Result","text":"<pre><code>{\n    \"workflowInstanceId\": \"Workflow Instance Id\",\n    \"taskId\": \"ID of the task to be updated\",\n    \"reasonForIncompletion\" : \"If failed, reason for failure\",\n    \"callbackAfterSeconds\": 0,\n    \"status\": \"IN_PROGRESS|FAILED|COMPLETED\",\n    \"outputData\": {\n        //JSON document representing Task execution output     \n    }\n\n}\n</code></pre> <p>Acknowledging tasks after poll</p> <p>If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.</p>"},{"location":"documentation/api/taskdomains.html","title":"Task Domains","text":"<p>Task domains helps support task development. The idea is same \"task definition\" can be implemented in different \"domains\". A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it.  </p> <p>As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \"Task Domain\" feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task.</p> <p>When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers (workers polled at least once in a 10 second window) for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on.</p> <p>If no workers are active for the domains provided:</p> <ul> <li>If <code>NO_DOMAIN</code> is provided as last token in list of domains, then no domain is set.</li> <li>Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain.</li> </ul> <p>Also, a <code>*</code> token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with <code>*</code>. </p> <p>For example, the below configuration:</p> <pre><code>\"taskToDomain\": {\n  \"*\": \"mydomain\",\n  \"some_task_x\":\"NO_DOMAIN\",\n  \"some_task_y\": \"someDomain, NO_DOMAIN\",\n  \"some_task_z\": \"someInactiveDomain1, someInactiveDomain2\"\n}\n</code></pre> <ul> <li>puts <code>some_task_x</code> in default queue (no domain).</li> <li>puts <code>some_task_y</code> in <code>someDomain</code> domain, if available or in default otherwise.</li> <li>puts <code>some_task_z</code> in <code>someInactiveDomain2</code>, even though workers are not available yet.</li> <li>and puts all other tasks in <code>mydomain</code> (even if workers are not available).</li> </ul> <p>Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, <code>NO_DOMAIN</code> token should be used last.</p>"},{"location":"documentation/api/taskdomains.html#how-to-use-task-domains","title":"How to use Task Domains","text":""},{"location":"documentation/api/taskdomains.html#change-the-poll-call","title":"Change the poll call","text":"<p>The poll call must now specify the domain. </p>"},{"location":"documentation/api/taskdomains.html#java-client","title":"Java Client","text":"<p>If you are using the java client then a simple property change will force  TaskRunnerConfigurer to pass the domain to the poller. <pre><code>    conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\"\n</code></pre></p>"},{"location":"documentation/api/taskdomains.html#rest-call","title":"REST call","text":"<p><code>GET {{ api_prefix }}/tasks/poll/batch/T2?workerid=myworker&amp;domain=mydomain</code> <code>GET {{ api_prefix }}/tasks/poll/T2?workerid=myworker&amp;domain=mydomain</code></p>"},{"location":"documentation/api/taskdomains.html#change-the-start-workflow-call","title":"Change the start workflow call","text":"<p>When starting the workflow, make sure the task to domain mapping is passes</p>"},{"location":"documentation/api/taskdomains.html#java-client_1","title":"Java Client","text":"<pre><code>{Map&lt;String, Object&gt; input = new HashMap&lt;&gt;();\ninput.put(\"wf_input1\", \"one\");\n\nMap&lt;String, String&gt; taskToDomain = new HashMap&lt;&gt;();\ntaskToDomain.put(\"T2\", \"mydomain\");\n\n// Other options ...\n// taskToDomain.put(\"*\", \"mydomain, NO_DOMAIN\")\n// taskToDomain.put(\"T2\", \"mydomain, fallbackDomain1, fallbackDomain2\")\n\nStartWorkflowRequest swr = new StartWorkflowRequest();\nswr.withName(\"myWorkflow\")\n    .withCorrelationId(\"corr1\")\n    .withVersion(1)\n    .withInput(input)\n    .withTaskToDomain(taskToDomain);\n\nwfclient.startWorkflow(swr);\n</code></pre>"},{"location":"documentation/api/taskdomains.html#rest-call_1","title":"REST call","text":"<p><code>POST {{ api_prefix }}/workflow</code></p> <pre><code>{\n  \"name\": \"myWorkflow\",\n  \"version\": 1,\n  \"correlatonId\": \"corr1\"\n  \"input\": {\n    \"wf_input1\": \"one\"\n  },\n  \"taskToDomain\": {\n    \"*\": \"mydomain\",\n    \"some_task_x\":\"NO_DOMAIN\",\n    \"some_task_y\": \"someDomain, NO_DOMAIN\"\n  }\n}\n</code></pre>"},{"location":"documentation/api/workflow.html","title":"Workflow API","text":""},{"location":"documentation/api/workflow.html#retrieve-workflows","title":"Retrieve Workflows","text":"Endpoint Description <code>GET {{ api_prefix }}/workflow/{workflowId}?includeTasks=true                               | false</code> Get Workflow State by workflow Id.  If includeTasks is set, then also includes all the tasks executed and scheduled. <code>GET {{ api_prefix }}/workflow/running/{name}</code> Get all the running workflows of a given type <code>GET {{ api_prefix }}/workflow/running/{name}/correlated/{correlationId}?includeClosed=true | false&amp;includeTasks=true                       |false</code> Get all the running workflows filtered by correlation Id.  If includeClosed is set, also includes workflows that have completed running. <code>GET {{ api_prefix }}/workflow/search</code> Search for workflows.  See Below."},{"location":"documentation/api/workflow.html#workflow-search","title":"Workflow Search","text":"<p>Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs.</p> <p><code>GET {{ api_prefix }}/workflow/search?start=&amp;size=&amp;sort=&amp;freeText=&amp;query=</code></p> Parameter Description start Page number.  Defaults to 0 size Number of results to return sort Sorting.  Format is: <code>ASC:&lt;fieldname&gt;</code> or <code>DESC:&lt;fieldname&gt;</code> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause.  e.g. workflowType = 'name_of_workflow'.  Optional if freeText is provided."},{"location":"documentation/api/workflow.html#output","title":"Output","text":"<p>Search result as described below: <pre><code>{\n  \"totalHits\": 0,\n  \"results\": [\n    {\n      \"workflowType\": \"string\",\n      \"version\": 0,\n      \"workflowId\": \"string\",\n      \"correlationId\": \"string\",\n      \"startTime\": \"string\",\n      \"updateTime\": \"string\",\n      \"endTime\": \"string\",\n      \"status\": \"RUNNING\",\n      \"input\": \"string\",\n      \"output\": \"string\",\n      \"reasonForIncompletion\": \"string\",\n      \"executionTime\": 0,\n      \"event\": \"string\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"documentation/api/workflow.html#manage-workflows","title":"Manage Workflows","text":"Endpoint Description <code>PUT {{ api_prefix }}/workflow/{workflowId}/pause</code> Pause.  No further tasks will be scheduled until resumed.  Currently running tasks are not paused. <code>PUT {{ api_prefix }}/workflow/{workflowId}/resume</code> Resume normal operations after a pause. <code>POST {{ api_prefix }}/workflow/{workflowId}/rerun</code> See Below. <code>POST {{ api_prefix }}/workflow/{workflowId}/restart</code> Restart workflow execution from the start.  Current execution history is wiped out. <code>POST {{ api_prefix }}/workflow/{workflowId}/retry</code> Retry the last failed task. <code>PUT {{ api_prefix }}/workflow/{workflowId}/skiptask/{taskReferenceName}</code> See below. <code>DELETE {{ api_prefix }}/workflow/{workflowId}</code> Terminates the running workflow. <code>DELETE {{ api_prefix }}/workflow/{workflowId}/remove</code> Deletes the workflow from system.  Use with caution."},{"location":"documentation/api/workflow.html#rerun","title":"Rerun","text":"<p>Re-runs a completed workflow from a specific task. </p> <p><code>POST {{ api_prefix }}/workflow/{workflowId}/rerun</code></p> <pre><code>{\n  \"reRunFromWorkflowId\": \"string\",\n  \"workflowInput\": {},\n  \"reRunFromTaskId\": \"string\",\n  \"taskInput\": {}\n}\n</code></pre>"},{"location":"documentation/api/workflow.html#skip-task","title":"Skip Task","text":"<p>Skips a task execution (specified as <code>taskReferenceName</code> parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. <code>PUT {{ api_prefix }}/workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&amp;taskReferenceName=</code> <pre><code>{\n  \"taskInput\": {},\n  \"taskOutput\": {}\n}\n</code></pre></p>"},{"location":"documentation/clientsdks/index.html","title":"Client SDKs","text":"<p>Conductor tasks that are executed by remote workers communicate over HTTP endpoints/gRPC to poll for the task and update the status of the execution. The follow SDKs are provided for implementing Conductor workers.</p> <ul> <li>Java</li> <li>Clojure</li> <li>C#</li> <li>Go</li> <li>Python</li> <li>Javascript/Typescript</li> </ul> <p>Conductor SDKs are hosted in the Conductor OSS GitHub org: github.com/conductor-oss.  Contributions from the community are encouraged!</p>"},{"location":"documentation/clientsdks/clojure-sdk.html","title":"Clojure SDK","text":"<p>Software Development Kit for Conductor, written on and providing support for Clojure.</p> <p>The code for the Clojure SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#get-the-sdk","title":"Get the SDK","text":"<p>https://clojars.org/io.orkes/conductor-clojure</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#quick-guide","title":"Quick Guide","text":"<ol> <li>Create connection options</li> </ol> <p><pre><code>(def options {\n                  :url  \"{{ server_host }}{{ api_prefix }}/\" ;; Conductor Server Path\n                  :app-key \"THIS-IS-SOME-APP-KEY\" ;; Optional if using Orkes Conductor\n                  :app-secret \"THIS-IS-SOME-APP-SECRET\" ;; Optional if using Orkes Conductor\n              } )\n</code></pre> 1. Creating a task using above options </p> <pre><code>(ns some.namespace \n    (:require [io.orkes.metadata :as metadata])\n\n    ;; Will Create a task. returns nil\n    (metadata/register-tasks options [{\n                         :name \"cool_clj_task\"\n                         :description \"some description\"\n                         :ownerEmail \"somemail@mail.com\"\n                         :retryCount 3\n                         :timeoutSeconds 300\n                         :responseTimeoutSeconds 180 }])\n)\n</code></pre> <ol> <li>Creating a Workflow that uses the task </li> </ol> <p><pre><code>(ns some.namespace \n    (:require [io.orkes.metadata :as metadata])\n\n;; Will Register a workflow that uses the above task returns nil\n(metadata/register-workflow-def options {\n                                              :name \"cool_clj_workflow\"\n                                              :description \"created programatically from clj\"\n                                              :version 1\n                                              :tasks [ {\n                                                       :name \"cool_clj_task\"\n                                                       :taskReferenceName \"cool_clj_task_ref\"\n                                                       :inputParameters {}\n                                                       :type \"SIMPLE\" \n                                                       } ]\n                                              :inputParameters []\n                                              :outputParameters {:message \"${clj_prog_task_ref.output.:message}\"}\n                                              :schemaVersion 2\n                                              :restartable true\n                                              :ownerEmail \"owner@yahoo.com\"\n                                              :timeoutSeconds 0\n                                         }))\n</code></pre> 3. Create and run a list of workers</p> <pre><code>;; Creates a worker and starts polling for work. will return an instance of Runner which can then be used to shutdown\n(def instance (runner-executor-for-workers\n               (list {\n                      :name \"cool_clj_task\"\n                      :execute (fn [someData]\n                                 [:completed {:message \"Hi From Clj i was created programatically\"}])\n                      })\n               options ))\n\n;; Shutsdown the polling for the workers defined above\n(.shutdown instance)\n</code></pre>"},{"location":"documentation/clientsdks/clojure-sdk.html#options","title":"Options","text":"<p>Options are a map with optional paremeters <pre><code>(def options {\n                  :url  \"{{ server_host }}{{ api_prefix }}/\" ;; Api url (Optional will default to \"{{ server_host }}\")\n                  :app-key \"THIS-IS-SOME-APP-KEY\" ;; Application Key (This is only relevant if you are using Orkes Conductor)\n                  :app-secret \"THIS-IS-SOME-APP-SECRET\" ;; Application Secret (This is only relevant if you are using Orkes Conductor)\n              } )\n</code></pre></p>"},{"location":"documentation/clientsdks/clojure-sdk.html#metadata-namespace","title":"Metadata namespace","text":"<p>Holds the functions to register workflows and tasks.</p> <p><code>(:require [conductor.metadata :as metadata])</code></p>"},{"location":"documentation/clientsdks/clojure-sdk.html#registering-tasks","title":"Registering tasks","text":"<p>Takes the option map and a list/vector of tasks to register. on success it will return nil</p> <pre><code>(metadata/register-tasks options [{\n                                                  :name \"cool_clj_task_b\"\n                                                  :description \"some description\"\n                                                  :ownerEmail \"mail@gmail.com\"\n                                                  :retryCount 3\n                                                  :timeoutSeconds 300\n                                                  :responseTimeoutSeconds 180 },\n                                                 {\n                                                  :name \"cool_clj_task_z\"\n                                                  :description \"some description\"\n                                                  :ownerEmail \"mail@gmail.com\"\n                                                  :retryCount 3\n                                                  :timeoutSeconds 300\n                                                  :responseTimeoutSeconds 180 }\n                                                 {\n                                                  :name \"cool_clj_task_x\"\n                                                  :description \"some description\"\n                                                  :ownerEmail \"mail@gmail.com\"\n                                                  :retryCount 3\n                                                  :timeoutSeconds 300\n                                                  :responseTimeoutSeconds 180 }\n                                                 ])\n</code></pre>"},{"location":"documentation/clientsdks/clojure-sdk.html#registering-a-workspace","title":"Registering a workspace","text":"<pre><code>(metadata/register-workflow-def options {\n                                                        :name \"cool_clj_workflow_2\"\n                                                        :description \"created programatically from clj\"\n                                                        :version 1\n                                                        :tasks [ {\n                                                                  :name \"cool_clj_task_b\"\n                                                                  :taskReferenceName \"cool_clj_task_ref\"\n                                                                  :inputParameters {}\n                                                                  :type \"SIMPLE\"\n                                                                  },\n                                                                {\n                                                                 :name \"someting\",\n                                                                 :taskReferenceName \"other\"\n                                                                 :inputParameters {}\n                                                                 :type \"FORK_JOIN\"\n                                                                 :forkTasks [[\n                                                                               {\n                                                                                :name \"cool_clj_task_z\"\n                                                                                :taskReferenceName \"cool_clj_task_z_ref\"\n                                                                                :inputParameters {}\n                                                                                :type \"SIMPLE\"\n                                                                                }\n                                                                               ]\n                                                                              [\n                                                                               {\n                                                                                :name \"cool_clj_task_x\"\n                                                                                :taskReferenceName \"cool_clj_task_x_ref\"\n                                                                                :inputParameters {}\n                                                                                :type \"SIMPLE\"\n                                                                                }\n                                                                               ]\n                                                                              ]\n                                                                 }\n                                                                {\n                                                                 :name \"join\"\n                                                                 :type \"JOIN\"\n                                                                 :taskReferenceName \"join_ref\"\n                                                                 :joinOn [ \"cool_clj_task_z\", \"cool_clj_task_x\"]\n                                                                 }\n                                                                ]\n                                                        :inputParameters []\n                                                        :outputParameters {\"message\" \"${clj_prog_task_ref.output.:message}\"}\n                                                        :schemaVersion 2\n                                                        :restartable true\n                                                        :ownerEmail \"mail@yahoo.com\"\n                                                        :timeoutSeconds 0\n                                                        :timeoutPolicy \"ALERT_ONLY\"\n                                                        })\n</code></pre>"},{"location":"documentation/clientsdks/clojure-sdk.html#client-namespace","title":"Client namespace","text":"<p>The client namespace holds the function to start a workflow and running a worker</p> <p><code>[io.orkes.client :as conductor]</code></p> <p><pre><code>;; Creates a worker and starts polling for work. will return an instance of Runner which can then be used to shutdown\n(def instance (runner-executor-for-workers\n               (list {\n                      :name \"cool_clj_task\"\n                      :execute (fn [someData]\n                                 [:completed {:message \"Hi From Clj i was created programatically\"}])\n                      })\n               options ))\n\n;; Shutsdown the polling for the workers defined above\n(.shutdown instance)\n</code></pre> The (runner-executor-for-workers) function will take a list of worker implementations map, and options and start pooling for work it will return a TaskRunnerConfigurer instance, which you can shutdown by calling the .shutdown() java method</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#mapper-utils-namespace","title":"Mapper-Utils namespace","text":"<p>The  <code>[io.orkes.mapper-utils :as mapper-utils]</code> namespace holds the functions to map to java object which are mostly not necesary.</p>"},{"location":"documentation/clientsdks/clojure-sdk.html#the-mapper-utilsjava-map-clj-map-protocol","title":"The mapper-utils/java-map-&gt;clj-map protocol","text":"<p>Will map a java map to a clojure map which may come in handy for workers implementation. for example consider a worker that sums two input parameters. For a workflow defined like this :</p> <pre><code>(metadata/register-workflow-def options {:name \"simple_wf\"\n                                         :description \"created programatically from clj\"\n                                         :version 1\n                                         :tasks [{:name \"simplest_task\"\n                                                  :taskReferenceName \"repl_task_ref\"\n                                                  :inputParameters {\"firstNumber\" \"${workflow.input.firstNumber}\"\n                                                                     \"secondNumber\" \"${workflow.input.secondNumber}\"}\n                                                  :type \"SIMPLE\"}]\n                                         :inputParameters [\"firstNumber\" \"secondNumber\"]\n                                         :outputParameters {\"result\" \"${repl_task_ref.output.:result}\"}\n                                         :schema-version 2\n                                         :restartable true\n                                         :ownerEmail \"mail@yahoo.com\"\n                                         :timeoutSeconds 0\n                                         :timeoutPolicy \"ALERT_ONLY\"})\n</code></pre> <p>To be able to use the input params you would need to use the string names like this:</p> <pre><code>(def instance (conductor/runner-executor-for-workers\n               (list {:name \"simplest_task\"\n                      :execute (fn [someData]\n\n                                 [:completed {\"result\" (+ (get someData \"firstNumber\") (get someData \"secondNumber\"))}])})\n               options))\n</code></pre> <p>A more clojure friendly way would be to convert to clojure our map :</p> <pre><code>(def instance (conductor/runner-executor-for-workers\n               (list {:name \"simplest_task\"\n                      :execute (fn [someData]\n                      (let [convertedToClj (-&gt; someData mapper-utils/java-map-&gt;clj-map)]\n                        [:completed {\"result\" (+ (:firstNumber convertedToClj) (:secondNumber convertedToClj))}]\n                      ))})\n               options))\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html","title":"C# SDK","text":"<p><code>conductor-csharp</code> repository provides the client SDKs to build Task Workers and Clients in C#</p> <p>The code for the C# SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/csharp-sdk.html#quick-start","title":"Quick Start","text":"<ol> <li>Get Secrets</li> <li>Write workers</li> <li>Run workers</li> <li>Worker Configurations</li> </ol>"},{"location":"documentation/clientsdks/csharp-sdk.html#dependencies","title":"Dependencies","text":"<p><code>conductor-csharp</code> packages are published to nugget package manager.  You can find the latest releases here.</p>"},{"location":"documentation/clientsdks/csharp-sdk.html#write-workers","title":"Write workers","text":"<pre><code> internal class MyWorkflowTask : IWorkflowTask\n    {\n        public MyWorkflowTask(){}\n\n        public string TaskType =&gt; \"test_ctask\";\n        public int? Priority =&gt; null;\n\n        public async Task&lt;TaskResult&gt; Execute(Conductor.Client.Models.Task task, CancellationToken token)\n        {\n           Dictionary&lt;string, object&gt; newOutput = new Dictionary&lt;string, object&gt;();\n           newOutput.Add(\"output\", \"1\");\n           return task.Completed(task.OutputData.MergeValues(newOutput));\n        }\n    }\n\n internal class MyWorkflowTask2 : IWorkflowTask\n    {\n        public MyWorkflowTask2(){}\n\n        public string TaskType =&gt; \"test_ctask2\";\n        public int? Priority =&gt; null;\n\n        public async Task&lt;TaskResult&gt; Execute(Conductor.Client.Models.Task task, CancellationToken token)\n        {\n           Dictionary&lt;string, object&gt; newOutput = new Dictionary&lt;string, object&gt;();\n           //Reuse the existing code written in C#\n           newOutput.Add(\"output\", \"success\");\n           return task.Completed(task.OutputData.MergeValues(newOutput));\n        }\n    }\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html#run-workers","title":"Run workers","text":"<pre><code>using System;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Collections.Generic;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\nusing Microsoft.Extensions.Logging;\nusing Conductor.Client.Models;\nusing Conductor.Client.Extensions;\nusing Conductor.Client.Interfaces;\n\nusing Task = System.Threading.Tasks.Task;\nusing Conductor.Client;\nusing System.Collections.Concurrent;\n\nnamespace TestOrkesSDK\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            new HostBuilder()\n                 .ConfigureServices((ctx, services) =&gt;\n                 {\n                    // First argument is optional headers which client wasnt to pass.\n                     Configuration configuration = new Configuration(new ConcurrentDictionary&lt;string, string&gt;(), \n                         \"KEY\",\n                         \"SECRET\");\n                     services.AddConductorWorker(configuration);\n                     services.AddConductorWorkflowTask&lt;MyWorkflowTask&gt;();\n                     services.AddHostedService&lt;WorkflowsWorkerService&gt;();\n                 })\n                 .ConfigureLogging(logging =&gt;\n                 {\n                     logging.SetMinimumLevel(LogLevel.Debug);\n                     logging.AddConsole();\n                 })\n                 .RunConsoleAsync();\n            Console.ReadLine();\n        }\n    }\n\n    internal class MyWorkflowTask : IWorkflowTask\n    {\n        public MyWorkflowTask() { }\n\n        public string TaskType =&gt; \"my_ctask\";\n        public int? Priority =&gt; null;\n\n        public async Task&lt;TaskResult&gt; Execute(Conductor.Client.Models.Task task, CancellationToken token)\n        {\n            Dictionary&lt;string, object&gt; newOutput = new Dictionary&lt;string, object&gt;();\n            newOutput.Add(\"output\", 1);\n            return task.Completed(task.OutputData.MergeValues(newOutput));\n        }\n    }\n}\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html#note","title":"Note:","text":"<p>Replace KEY and SECRET by obtaining a new key and secret from Orkes Playground</p> <p>Refer to Authentication for details.</p> <pre><code>    internal class WorkflowsWorkerService : BackgroundService\n    {\n        private readonly IWorkflowTaskCoordinator workflowTaskCoordinator;\n        private readonly IEnumerable&lt;IWorkflowTask&gt; workflowTasks;\n\n        public WorkflowsWorkerService(\n            IWorkflowTaskCoordinator workflowTaskCoordinator,\n            IEnumerable&lt;IWorkflowTask&gt; workflowTasks\n        )\n        {\n            this.workflowTaskCoordinator = workflowTaskCoordinator;\n            this.workflowTasks = workflowTasks;\n        }\n\n        protected override async Task ExecuteAsync(CancellationToken stoppingToken)\n        {\n            foreach (var worker in workflowTasks)\n            {\n                workflowTaskCoordinator.RegisterWorker(worker);\n            }\n            // start all the workers so that it can poll for the tasks\n            await workflowTaskCoordinator.Start();\n        }\n    }\n</code></pre>"},{"location":"documentation/clientsdks/csharp-sdk.html#worker-configurations","title":"Worker Configurations","text":"<p>Worker configuration is handled via Configuration object passed when initializing TaskHandler. <pre><code>Configuration configuration = \n    new Configuration(new ConcurrentDictionary&lt;string, string&gt;(), \"KEY\", \"SECRET\", \"https://developer.orkescloud.com/\");\n</code></pre></p>"},{"location":"documentation/clientsdks/csharp-sdk.html#registering-and-starting-the-workflow-using-sdk","title":"Registering and starting the workflow using SDK.","text":"<p>Below is the code snippet that shows how to register a simple workflow and start execution:</p> <pre><code>IDictionary&lt;string, string&gt; optionalHeaders = new ConcurrentDictionary&lt;string, string&gt;();\nConfiguration configuration = new Configuration(optionalHeaders, \"keyId\", \"keySecret\");\n\n//Create task definition\nMetadataResourceApi metadataResourceApi = new MetadataResourceApi(configuration);\nTaskDef taskDef = new TaskDef(name: \"test_task\");\ntaskDef.OwnerEmail = \"test@test.com\";\nmetadataResourceApi.RegisterTaskDef(new List&lt;TaskDef&gt;() { taskDef});\n\n//Create workflow definition\nWorkflowDef workflowDef = new WorkflowDef();\nworkflowDef.Name = \"test_workflow\";\nworkflowDef.OwnerEmail = \"test@test.com\";\nworkflowDef.SchemaVersion = 2;\n\nWorkflowTask workflowTask = new WorkflowTask();\nworkflowTask.Type = \"HTTP\";\nworkflowTask.Name = \"test_\"; //Same as registered task definition.\nIDictionary&lt;string, string&gt; requestParams = new Dictionary&lt;string, string&gt;();\nrequestParams.Add(\"uri\", \"https://www.google.com\"); //adding a key/value using the Add() method\nrequestParams.Add(\"method\", \"GET\");\nDictionary&lt;string, object&gt; request = new Dictionary&lt;string, object&gt;();\nrequest.Add(\"http_request\", requestParams);\nworkflowTask.InputParameters = request;\nworkflowDef.Tasks = new List&lt;WorkflowTask&gt;() { workflowTask };\n//Run a workflow\nWorkflowResourceApi workflowResourceApi = new WorkflowResourceApi(configuration);\nDictionary&lt;string, Object&gt; input = new Dictionary&lt;string, Object&gt;();\n//Fill the input map which workflow consumes.\nworkflowResourceApi.StartWorkflow(\"test_workflow\", input, 1);\nConsole.ReadLine();\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html","title":"Go SDK","text":"<p>The code for the Golang SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/go-sdk.html#quick-start","title":"Quick Start","text":"<ol> <li>Setup conductor-go package</li> <li>Create and run Task Workers</li> <li>Create workflows using Code</li> </ol>"},{"location":"documentation/clientsdks/go-sdk.html#setup-conductor-go-package","title":"Setup conductor go package","text":"<p>Create a folder to build your package: <pre><code>mkdir quickstart/\ncd quickstart/\ngo mod init quickstart\n</code></pre></p> <p>Get Conductor Go SDK</p> <pre><code>go get github.com/conductor-oss/go-sdk\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#configuration","title":"Configuration","text":""},{"location":"documentation/clientsdks/go-sdk.html#authentication-settings-optional","title":"Authentication settings (optional)","text":"<p>Use if your conductor server requires authentication * keyId: Key * keySecret: Secret for the Key</p> <pre><code>authenticationSettings := settings.NewAuthenticationSettings(\n  \"keyId\",\n  \"keySecret\",\n)\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#access-control-setup","title":"Access Control Setup","text":"<p>See Access Control for more details on role based access control with Conductor and generating API keys for your environment.</p>"},{"location":"documentation/clientsdks/go-sdk.html#configure-api-client","title":"Configure API Client","text":"<pre><code>apiClient := client.NewAPIClient(\n    settings.NewAuthenticationSettings(\n        KEY,\n        SECRET,\n    ),\n    settings.NewHttpSettings(\n        \"https://developer.orkescloud.com/\",\n    ),\n)\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#set-up-logging","title":"Set up Logging","text":"<p>SDK uses logrus for the logging.</p> <pre><code>func init() {\n    log.SetFormatter(&amp;log.TextFormatter{})\n    log.SetOutput(os.Stdout)\n    log.SetLevel(log.DebugLevel)\n}\n</code></pre>"},{"location":"documentation/clientsdks/go-sdk.html#next-create-and-run-task-workers","title":"Next: Create and run Task Workers","text":""},{"location":"documentation/clientsdks/java-sdk.html","title":"Java SDK","text":"<p>Conductor provides the following java clients to interact with the various APIs</p> Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task"},{"location":"documentation/clientsdks/java-sdk.html#worker","title":"Worker","text":"<p>Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server.</p> <p>Implement the Worker interface to execute the task.</p>"},{"location":"documentation/clientsdks/java-sdk.html#taskrunnerconfigurer","title":"TaskRunnerConfigurer","text":"<p>The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update).  </p> <p>Use the Builder to create an instance of the TaskRunnerConfigurer. The Builder constructor takes the following parameters.</p> Parameter Description TaskClient TaskClient used to communicate to the Conductor server Workers Workers that will be used for polling work and task execution. <p>The builder accepts the following parameters:</p> Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not.  When the server goes out of discovery, the polling is stopped unless <code>pollOutOfDiscovery</code> is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- withShutdownGracePeriodSeconds Waiting seconds before forcing shutdown of your worker 10 <p>Once an instance is created, call <code>init()</code> method to initialize the TaskPollExecutor and begin the polling and execution of tasks.</p> <p>Note</p> <p>To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided <code>shutdown()</code> hook in a <code>PreDestroy</code> block.</p>"},{"location":"documentation/clientsdks/java-sdk.html#properties","title":"Properties","text":"<p>The worker behavior can be further controlled by using these properties:</p> Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery   status. This is useful while running on a dev machine. false <p>Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM:</p> Name Description <code>conductor.worker.&lt;property&gt;</code> Applies to ALL the workers in the JVM. <code>conductor.worker.&lt;taskDefName&gt;.&lt;property&gt;</code> Applies to the specified worker.  Overrides the global property."},{"location":"documentation/clientsdks/java-sdk.html#examples","title":"Examples","text":"<ul> <li>Sample Worker Implementation</li> <li>Example</li> </ul>"},{"location":"documentation/clientsdks/js-sdk.html","title":"Javascript/TypeScript SDK","text":"<p>Refer to conductor-sdk/conductor-javascript</p>"},{"location":"documentation/clientsdks/python-sdk.html","title":"Python SDK","text":"<p>Software Development Kit for Conductor, written on and providing support for Python.</p> <p>The code for the Python SDk is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/python-sdk.html#quick-guide","title":"Quick Guide","text":"<ol> <li> <p>Create a virtual environment</p> <pre><code>$ virtualenv conductor\n$ source conductor/bin/activate\n$ python3 -m pip list\nPackage    Version\n---------- -------\npip        22.0.3\nsetuptools 60.6.0\nwheel      0.37.1\n</code></pre> </li> <li> <p>Install latest version of <code>conductor-python</code> from pypi</p> <pre><code>$ python3 -m pip install conductor-python\nCollecting conductor-python\nCollecting certifi&gt;=14.05.14\nCollecting urllib3&gt;=1.15.1\nRequirement already satisfied: setuptools&gt;=21.0.0 in ./conductor/lib/python3.8/site-packages (from conductor-python) (60.6.0)\nCollecting six&gt;=1.10\nInstalling collected packages: certifi, urllib3, six, conductor-python\nSuccessfully installed certifi-2021.10.8 conductor-python-1.0.7 six-1.16.0 urllib3-1.26.8\n</code></pre> </li> <li> <p>Create a worker capable of executing a <code>Task</code>. Example:</p> <pre><code>from conductor.client.worker.worker_interface import WorkerInterface\n\nclass SimplePythonWorker(WorkerInterface):\n    def execute(self, task):\n        task_result = self.get_task_result_from_task(task)\n        task_result.add_output_data('key', 'value')\n        task_result.status = 'COMPLETED'\n        return task_result\n</code></pre> <ul> <li>The <code>add_output_data</code> is the most relevant part, since you can store information in a dictionary, which will be sent within <code>TaskResult</code> as your execution response to Conductor</li> </ul> </li> <li> <p>Create a main method to start polling tasks to execute with your worker. Example:</p> <pre><code>from conductor.client.automator.task_handler import TaskHandler\nfrom conductor.client.configuration.configuration import Configuration\nfrom conductor.client.worker.sample.faulty_execution_worker import FaultyExecutionWorker\nfrom conductor.client.worker.sample.simple_python_worker import SimplePythonWorker\n\ndef main():\n    configuration = Configuration(debug=True)\n    task_definition_name = 'python_example_task'\n    workers = [\n        SimplePythonWorker(task_definition_name),\n        FaultyExecutionWorker(task_definition_name)\n    ]\n    with TaskHandler(workers, configuration) as task_handler:\n        task_handler.start()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ul> <li>This example contains two workers, each with a different execution method, capable of running the same <code>task_definition_name</code></li> </ul> </li> <li> <p>Now that you have implemented the example, you can start the Conductor server locally:</p> <ol> <li> <p>Clone Conductor repository:</p> <pre><code>$ git clone https://github.com/conductor-oss/conductor.git\n$ cd conductor/\n</code></pre> </li> <li> <p>Start the Conductor server:</p> <pre><code>/conductor$ ./gradlew bootRun\n</code></pre> </li> <li> <p>Start Conductor UI:</p> <pre><code>/conductor$ cd ui/\n/conductor/ui$ yarn install\n/conductor/ui$ yarn run start\n</code></pre> </li> </ol> <p>You should be able to access:   * Conductor API:     * {{ server_host }}/swagger-ui/index.html   * Conductor UI:     * http://localhost:5000</p> </li> <li> <p>Create a <code>Task</code> within <code>Conductor</code>. Example:</p> <pre><code>$ curl -X 'POST' \\\n    '{{ server_host }}{{ api_prefix }}/metadata/taskdefs' \\\n    -H 'accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -d '[\n    {\n        \"name\": \"python_task_example\",\n        \"description\": \"Python task example\",\n        \"retryCount\": 3,\n        \"retryLogic\": \"FIXED\",\n        \"retryDelaySeconds\": 10,\n        \"timeoutSeconds\": 300,\n        \"timeoutPolicy\": \"TIME_OUT_WF\",\n        \"responseTimeoutSeconds\": 180,\n        \"ownerEmail\": \"example@example.com\"\n    }\n    ]'\n</code></pre> </li> <li> <p>Create a <code>Workflow</code> within <code>Conductor</code>. Example:</p> <pre><code>$ curl -X 'POST' \\\n    '{{ server_host }}{{ api_prefix }}/metadata/workflow' \\\n    -H 'accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n    \"createTime\": 1634021619147,\n    \"updateTime\": 1630694890267,\n    \"name\": \"workflow_with_python_task_example\",\n    \"description\": \"Workflow with Python Task example\",\n    \"version\": 1,\n    \"tasks\": [\n        {\n        \"name\": \"python_task_example\",\n        \"taskReferenceName\": \"python_task_example_ref_1\",\n        \"inputParameters\": {},\n        \"type\": \"SIMPLE\"\n        }\n    ],\n    \"inputParameters\": [],\n    \"outputParameters\": {\n        \"workerOutput\": \"${python_task_example_ref_1.output}\"\n    },\n    \"schemaVersion\": 2,\n    \"restartable\": true,\n    \"ownerEmail\": \"example@example.com\",\n    \"timeoutPolicy\": \"ALERT_ONLY\",\n    \"timeoutSeconds\": 0\n    }'\n</code></pre> </li> <li> <p>Start a new workflow:</p> <pre><code>$ curl -X 'POST' \\\n    '{{ server_host }}{{ api_prefix }}/workflow/workflow_with_python_task_example' \\\n    -H 'accept: text/plain' \\\n    -H 'Content-Type: application/json' \\\n    -d '{}'\n</code></pre> <p>You should receive a Workflow ID at the Response body * Workflow ID example: <code>8ff0bc06-4413-4c94-b27a-b3210412a914</code></p> <p>Now you must be able to see its execution through the UI. * Example: <code>http://localhost:5000/execution/8ff0bc06-4413-4c94-b27a-b3210412a914</code></p> </li> <li> <p>Run your Python file with the <code>main</code> method</p> </li> </ol>"},{"location":"documentation/clientsdks/python-sdk.html#unit-tests","title":"Unit Tests","text":""},{"location":"documentation/clientsdks/python-sdk.html#simple-validation","title":"Simple validation","text":"<pre><code>/conductor-python/src$ python3 -m unittest -v\ntest_execute_task (tst.automator.test_task_runner.TestTaskRunner) ... ok\ntest_execute_task_with_faulty_execution_worker (tst.automator.test_task_runner.TestTaskRunner) ... ok\ntest_execute_task_with_invalid_task (tst.automator.test_task_runner.TestTaskRunner) ... ok\n\n----------------------------------------------------------------------\nRan 3 tests in 0.001s\n\nOK\n</code></pre>"},{"location":"documentation/clientsdks/python-sdk.html#run-with-code-coverage","title":"Run with code coverage","text":"<pre><code>/conductor-python/src$ python3 -m coverage run --source=conductor/ -m unittest\n</code></pre> <p>Report:</p> <pre><code>/conductor-python/src$ python3 -m coverage report\n</code></pre> <p>Visual coverage results:</p> <pre><code>/conductor-python/src$ python3 -m coverage html\n</code></pre>"},{"location":"documentation/configuration/appconf.html","title":"App Configuration","text":"<p>The Conductor application server offers extensive customization options to optimize its operation for specific environments.</p> <p>These configuration parameters allow fine-tuning of various aspects of the server's behavior, performance, and integration capabilities. All of these parameters are grouped under the <code>conductor.app</code> namespace.</p>"},{"location":"documentation/configuration/appconf.html#configuration","title":"Configuration","text":"Field Type Description Notes stack String Name of the stack within which the app is running. e.g. <code>devint</code>, <code>testintg</code>, <code>staging</code>, <code>prod</code> etc. Default is \"test\" appId String The ID with which the app has been registered. e.g. <code>conductor</code>, <code>myApp</code> Default is \"conductor\" executorServiceMaxThreadCount int The maximum number of threads to be allocated to the executor service threadpool. e.g. <code>50</code> Default is 50 workflowOffsetTimeout Duration The timeout duration to set when a workflow is pushed to the decider queue. Example: <code>30s</code> or <code>1m</code> Default is 30 seconds maxPostponeDurationSeconds Duration The maximum timeout duration to set when a workflow with running task is pushed to the decider queue. Example: <code>30m</code> or <code>1h</code> Default is 3600 seconds sweeperThreadCount int The number of threads to use for background sweeping on active workflows. Example: <code>8</code> if there are 4 processors (2x4) Default is 2 times the number of available processors sweeperWorkflowPollTimeout Duration The timeout for polling workflows to be swept. Example: <code>2000ms</code> or <code>2s</code> Default is 2000 milliseconds eventProcessorThreadCount int The number of threads to configure the threadpool in the event processor. Example: <code>4</code> Default is 2 eventMessageIndexingEnabled boolean Whether to enable indexing of messages within event payloads. Example: <code>true</code> or <code>false</code> Default is true eventExecutionIndexingEnabled boolean Whether to enable indexing of event execution results. Example: <code>true</code> or <code>false</code> Default is true workflowExecutionLockEnabled boolean Whether to enable the workflow execution lock. Example: <code>true</code> or <code>false</code> Default is false lockLeaseTime Duration The time for which the lock is leased. Example: <code>60000ms</code> or <code>1m</code> Default is 60000 milliseconds lockTimeToTry Duration The time for which the thread will block in an attempt to acquire the lock. Example: <code>500ms</code> or <code>1s</code> Default is 500 milliseconds activeWorkerLastPollTimeout Duration The time to consider if a worker is actively polling for a task. Example: <code>10s</code> Default is 10 seconds taskExecutionPostponeDuration Duration The time for which a task execution will be postponed if rate-limited or concurrent execution limited. Example: <code>60s</code> Default is 60 seconds taskIndexingEnabled boolean Whether to enable indexing of tasks. Example: <code>true</code> or <code>false</code> Default is true taskExecLogIndexingEnabled boolean Whether to enable indexing of task execution logs. Example: <code>true</code> or <code>false</code> Default is true asyncIndexingEnabled boolean Whether to enable asynchronous indexing to Elasticsearch. Example: <code>true</code> or <code>false</code> Default is false systemTaskWorkerThreadCount int The number of threads in the threadpool for system task workers. Example: <code>8</code> if there are 4 processors (2x4) Default is 2 times the number of available processors systemTaskMaxPollCount int The maximum number of threads to be polled within the threadpool for system task workers. Example: <code>8</code> Default is equal to systemTaskWorkerThreadCount systemTaskWorkerCallbackDuration Duration The interval after which a system task will be checked by the system task worker for completion. Example: <code>30s</code> Default is 30 seconds systemTaskWorkerPollInterval Duration The interval at which system task queues will be polled by system task workers. Example: <code>50ms</code> Default is 50 milliseconds systemTaskWorkerExecutionNamespace String The namespace for the system task workers to provide instance-level isolation. Example: <code>namespace1</code>, <code>namespace2</code> Default is an empty string isolatedSystemTaskWorkerThreadCount int The number of threads to be used within the threadpool for system task workers in each isolation group. Example: <code>4</code> Default is 1 asyncUpdateShortRunningWorkflowDuration Duration The duration of workflow execution qualifying as short-running when async indexing to Elasticsearch is enabled. Example: <code>30s</code> Default is 30 seconds asyncUpdateDelay Duration The delay with which short-running workflows will be updated in Elasticsearch when async indexing is enabled. Example: <code>60s</code> Default is 60 seconds ownerEmailMandatory boolean Whether to validate the owner email field as mandatory within workflow and task definitions. Example: <code>true</code> or <code>false</code> Default is true eventQueueSchedulerPollThreadCount int The number of threads used in the Scheduler for polling events from multiple event queues. Example: <code>8</code> if there are 4 processors (2x4) Default is equal to the number of available processors eventQueuePollInterval Duration The time interval at which the default event queues will be polled. Example: <code>100ms</code> Default is 100 milliseconds eventQueuePollCount int The number of messages to be polled from a default event queue in a single operation. Example: <code>10</code> Default is 10 eventQueueLongPollTimeout Duration The timeout for the poll operation on the default event queue. Example: <code>1000ms</code> Default is 1000 milliseconds workflowInputPayloadSizeThreshold DataSize The threshold of the workflow input payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: <code>5120KB</code> Default is 5120 kilobytes maxWorkflowInputPayloadSizeThreshold DataSize The maximum threshold of the workflow input payload size beyond which input will be rejected and the workflow marked as FAILED. Example: <code>10240KB</code> Default is 10240 kilobytes workflowOutputPayloadSizeThreshold DataSize The threshold of the workflow output payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: <code>5120KB</code> Default is 5120 kilobytes maxWorkflowOutputPayloadSizeThreshold DataSize The maximum threshold of the workflow output payload size beyond which output will be rejected and the workflow marked as FAILED. Example: <code>10240KB</code> Default is 10240 kilobytes taskInputPayloadSizeThreshold DataSize The threshold of the task input payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: <code>3072KB</code> Default is 3072 kilobytes maxTaskInputPayloadSizeThreshold DataSize The maximum threshold of the task input payload size beyond which the task input will be rejected and the task marked as FAILED_WITH_TERMINAL_ERROR. Example: <code>10240KB</code> Default is 10240 kilobytes taskOutputPayloadSizeThreshold DataSize The threshold of the task output payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: <code>3072KB</code> Default is 3072 kilobytes maxTaskOutputPayloadSizeThreshold DataSize The maximum threshold of the task output payload size beyond which the task output will be rejected and the task marked as FAILED_WITH_TERMINAL_ERROR. Example: <code>10240KB</code> Default is 10240 kilobytes maxWorkflowVariablesPayloadSizeThreshold DataSize The maximum threshold of the workflow variables payload size beyond which the task changes will be rejected and the task marked as FAILED_WITH_TERMINAL_ERROR. Example: <code>256KB</code> Default is 256 kilobytes taskExecLogSizeLimit int The maximum size of task execution logs. Example: <code>10000</code> Default is 10"},{"location":"documentation/configuration/appconf.html#example-usage","title":"Example usage","text":"<p>In your configuration file add the configuration as you need</p> <pre><code># Conductor App Configuration\n\n# Name of the stack within which the app is running. e.g. devint, testintg, staging, prod etc.\nconductor.app.stack=test\n\n# The ID with which the app has been registered. e.g. conductor, myApp\nconductor.app.appId=conductor\n\n# The maximum number of threads to be allocated to the executor service threadpool. e.g. 50\nconductor.app.executorServiceMaxThreadCount=50\n\n# The timeout duration to set when a workflow is pushed to the decider queue. Example: 30s or 1m\nconductor.app.workflowOffsetTimeout=30s\n\n# The number of threads to use for background sweeping on active workflows. Example: 8 if there are 4 processors (2x4)\nconductor.app.sweeperThreadCount=8\n\n# The timeout for polling workflows to be swept. Example: 2000ms or 2s\nconductor.app.sweeperWorkflowPollTimeout=2000ms\n\n# The number of threads to configure the threadpool in the event processor. Example: 4\nconductor.app.eventProcessorThreadCount=4\n\n# Whether to enable indexing of messages within event payloads. Example: true or false\nconductor.app.eventMessageIndexingEnabled=true\n\n# Whether to enable indexing of event execution results. Example: true or false\nconductor.app.eventExecutionIndexingEnabled=true\n\n# Whether to enable the workflow execution lock. Example: true or false\nconductor.app.workflowExecutionLockEnabled=false\n\n# The time for which the lock is leased. Example: 60000ms or 1m\nconductor.app.lockLeaseTime=60000ms\n\n# The time for which the thread will block in an attempt to acquire the lock. Example: 500ms or 1s\nconductor.app.lockTimeToTry=500ms\n\n# The time to consider if a worker is actively polling for a task. Example: 10s\nconductor.app.activeWorkerLastPollTimeout=10s\n\n# The time for which a task execution will be postponed if rate-limited or concurrent execution limited. Example: 60s\nconductor.app.taskExecutionPostponeDuration=60s\n\n# Whether to enable indexing of tasks. Example: true or false\nconductor.app.taskIndexingEnabled=true\n\n# Whether to enable indexing of task execution logs. Example: true or false\nconductor.app.taskExecLogIndexingEnabled=true\n\n# Whether to enable asynchronous indexing to Elasticsearch. Example: true or false\nconductor.app.asyncIndexingEnabled=false\n\n# The number of threads in the threadpool for system task workers. Example: 8 if there are 4 processors (2x4)\nconductor.app.systemTaskWorkerThreadCount=8\n\n# The maximum number of threads to be polled within the threadpool for system task workers. Example: 8\nconductor.app.systemTaskMaxPollCount=8\n\n# The interval after which a system task will be checked by the system task worker for completion. Example: 30s\nconductor.app.systemTaskWorkerCallbackDuration=30s\n\n# The interval at which system task queues will be polled by system task workers. Example: 50ms\nconductor.app.systemTaskWorkerPollInterval=50ms\n\n# The namespace for the system task workers to provide instance-level isolation. Example: namespace1, namespace2\nconductor.app.systemTaskWorkerExecutionNamespace=\n\n# The number of threads to be used within the threadpool for system task workers in each isolation group. Example: 4\nconductor.app.isolatedSystemTaskWorkerThreadCount=4\n\n# The duration of workflow execution qualifying as short-running when async indexing to Elasticsearch is enabled. Example: 30s\nconductor.app.asyncUpdateShortRunningWorkflowDuration=30s\n\n# The delay with which short-running workflows will be updated in Elasticsearch when async indexing is enabled. Example: 60s\nconductor.app.asyncUpdateDelay=60s\n\n# Whether to validate the owner email field as mandatory within workflow and task definitions. Example: true or false\nconductor.app.ownerEmailMandatory=true\n\n# The number of threads used in the Scheduler for polling events from multiple event queues. Example: 8 if there are 4 processors (2x4)\nconductor.app.eventQueueSchedulerPollThreadCount=8\n\n# The time interval at which the default event queues will be polled. Example: 100ms\nconductor.app.eventQueuePollInterval=100ms\n\n# The number of messages to be polled from a default event queue in a single operation. Example: 10\nconductor.app.eventQueuePollCount=10\n\n# The timeout for the poll operation on the default event queue. Example: 1000ms\nconductor.app.eventQueueLongPollTimeout=1000ms\n\n# The threshold of the workflow input payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: 5120KB\nconductor.app.workflowInputPayloadSizeThreshold=5120KB\n\n# The maximum threshold of the workflow input payload size beyond which input will be rejected and the workflow marked as FAILED. Example: 10240KB\nconductor.app.maxWorkflowInputPayloadSizeThreshold=10240KB\n\n# The threshold of the workflow output payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: 5120KB\nconductor.app.workflowOutputPayloadSizeThreshold=5120KB\n\n# The maximum threshold of the workflow output payload size beyond which output will be rejected and the workflow marked as FAILED. Example: 10240KB\nconductor.app.maxWorkflowOutputPayloadSizeThreshold=10240KB\n\n# The threshold of the task input payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: 3072KB\nconductor.app.taskInputPayloadSizeThreshold=3072KB\n\n# The maximum threshold of the task input payload size beyond which the task input will be rejected and the task marked as FAILED_WITH_TERMINAL_ERROR. Example: 10240KB\nconductor.app.maxTaskInputPayloadSizeThreshold=10240KB\n\n# The threshold of the task output payload size beyond which the payload will be stored in ExternalPayloadStorage. Example: 3072KB\nconductor.app.taskOutputPayloadSizeThreshold=3072KB\n\n# The maximum threshold of the task output payload size beyond which the task output will be rejected and the task marked as FAILED_WITH_TERMINAL_ERROR. Example: 10240KB\nconductor.app.maxTaskOutputPayloadSizeThreshold=10240KB\n\n# The maximum threshold of the workflow variables payload size beyond which the task changes will be rejected and the task marked as FAILED_WITH_TERMINAL_ERROR. Example: 256KB\nconductor.app.maxWorkflowVariablesPayloadSizeThreshold=256KB\n\n# The maximum size of task execution logs. Example: 10000\nconductor.app.taskExecLogSizeLimit=10000\n</code></pre>"},{"location":"documentation/configuration/eventhandlers.html","title":"Event Handlers","text":"<p>Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems.</p> <p>This includes:</p> <ol> <li>Being able to produce an event (message) in an external system like SQS, Kafka or internal to Conductor. </li> <li>Start a workflow when a specific event occurs that matches the provided criteria.</li> </ol> <p>Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow.  Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations.</p>"},{"location":"documentation/configuration/eventhandlers.html#event-task","title":"Event Task","text":"<p>Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS or Kafka. Event tasks are useful for creating event based dependencies for workflows and tasks.</p> <p>See Event Task for documentation.</p>"},{"location":"documentation/configuration/eventhandlers.html#event-handler","title":"Event Handler","text":"<p>Event handlers are listeners registered that executes an action when a matching event occurs.  The supported actions are:</p> <ol> <li>Start a Workflow</li> <li>Fail a Task</li> <li>Complete a Task</li> </ol> <p>Event Handlers can be configured to listen to Conductor Events or an external event like SQS or Kafka.</p>"},{"location":"documentation/configuration/eventhandlers.html#configuration","title":"Configuration","text":"<p>Event Handlers are configured via <code>/event/</code> APIs.</p>"},{"location":"documentation/configuration/eventhandlers.html#structure","title":"Structure","text":"<p><pre><code>{\n  \"name\" : \"descriptive unique name\",\n  \"event\": \"event_type:event_location\",\n  \"condition\": \"boolean condition\",\n  \"actions\": [\"see examples below\"]\n}\n</code></pre> <code>condition</code> is an expression that MUST evaluate to a boolean value.  A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to <code>true</code>.</p>"},{"location":"documentation/configuration/eventhandlers.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/eventhandlers.html#condition","title":"Condition","text":"<p>Given the following payload in the message:</p> <pre><code>{\n    \"fileType\": \"AUDIO\",\n    \"version\": 3,\n    \"metadata\": {\n       \"length\": 300,\n       \"codec\": \"aac\"\n    }\n}\n</code></pre> <p>The following expressions can be used in <code>condition</code> with the indicated results:</p> Expression Result <code>$.version &gt; 1</code> true <code>$.version &gt; 10</code> false <code>$.metadata.length == 300</code> true"},{"location":"documentation/configuration/eventhandlers.html#actions","title":"Actions","text":"<p>Examples of actions that can be configured in the <code>actions</code> array:</p> <p>To start a workflow</p> <pre><code>{\n    \"action\": \"start_workflow\",\n    \"start_workflow\": {\n        \"name\": \"WORKFLOW_NAME\",\n        \"version\": \"&lt;optional_param&gt;\",\n        \"input\": {\n            \"param1\": \"${param1}\" \n        }\n    }\n}\n</code></pre> <p>To complete a task</p> <pre><code>{\n    \"action\": \"complete_task\",\n    \"complete_task\": {\n      \"workflowId\": \"${workflowId}\",\n      \"taskRefName\": \"task_1\",\n      \"output\": {\n        \"response\": \"${result}\"\n      }\n    },\n    \"expandInlineJSON\": true\n}\n</code></pre> <p>To fail a task*</p> <p><pre><code>{\n    \"action\": \"fail_task\",\n    \"fail_task\": {\n      \"workflowId\": \"${workflowId}\",\n      \"taskRefName\": \"task_1\",\n      \"output\": {\n        \"response\": \"${result}\"\n      }\n    },\n    \"expandInlineJSON\": true\n}\n</code></pre> Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring task inputs.</p> <p>Expanding stringified JSON elements in payload</p> <p><code>expandInlineJSON</code> property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions. </p>"},{"location":"documentation/configuration/taskdef.html","title":"Task Definition","text":"<p>Task Definitions are used to register SIMPLE tasks (workers). Conductor maintains a registry of user task types. A task type MUST be registered before being used in a workflow.</p> <p>This should not be confused with Task Configurations which are part of the Workflow Definition, and are iterated in the <code>tasks</code> property in the definition.</p>"},{"location":"documentation/configuration/taskdef.html#schema","title":"Schema","text":"Field Type Description Notes name string Task Name. Unique name of the Task that resonates with its function. Must be unique description string Description of the task. Optional retryCount number Number of retries to attempt when a Task is marked as failure. Defaults to 3 with maximum allowed capped at 10 retryLogic string (enum) Mechanism for the retries. See Retry Logic retryDelaySeconds number Time to wait before retries. Defaults to 60 seconds timeoutPolicy string (enum) Task's timeout policy. Defaults to <code>TIME_OUT_WF</code>; See Timeout Policy timeoutSeconds number Time in seconds, after which the task is marked as <code>TIMED_OUT</code> if it has not reached a terminal state after transitioning to <code>IN_PROGRESS</code> status for the first time. No timeouts if set to 0 responseTimeoutSeconds number If greater than 0, the task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. Defaults to 600 pollTimeoutSeconds number Time in seconds, after which the task is marked as <code>TIMED_OUT</code> if not polled by a worker. No timeouts if set to 0 inputKeys array of string(s) Array of keys of task's expected input. Used for documenting task's input. Optional. See Using inputKeys and outputKeys. outputKeys array of string(s) Array of keys of task's expected output. Used for documenting task's output. Optional. See Using inputKeys and outputKeys. inputTemplate object Define default input values. Optional. See Using inputTemplate concurrentExecLimit number Number of tasks that can be executed at any given time. Optional rateLimitFrequencyInSeconds number Sets the rate limit frequency window. Optional. See Task Rate limits rateLimitPerFrequency number Sets the max number of tasks that can be given to workers within window. Optional. See Task Rate limits below ownerEmail string Email address of the team that owns the task. Required"},{"location":"documentation/configuration/taskdef.html#retry-logic","title":"Retry Logic","text":"<ul> <li>FIXED: Reschedule the task after <code>retryDelaySeconds</code></li> <li>EXPONENTIAL_BACKOFF: Reschedule the task after <code>retryDelaySeconds * (2 ^ attemptNumber)</code></li> <li>LINEAR_BACKOFF: Reschedule after <code>retryDelaySeconds * backoffRate * attemptNumber</code></li> </ul>"},{"location":"documentation/configuration/taskdef.html#timeout-policy","title":"Timeout Policy","text":"<ul> <li>RETRY: Retries the task again</li> <li>TIME_OUT_WF: Workflow is marked as TIMED_OUT and terminated. This is the default value.</li> <li>ALERT_ONLY: Registers a counter (task_timeout)</li> </ul>"},{"location":"documentation/configuration/taskdef.html#task-concurrent-execution-limits","title":"Task Concurrent Execution Limits","text":"<p><code>concurrentExecLimit</code> limits the number of simultaneous Task executions at any point.</p> <p>Example  You have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set <code>concurrentExecLimit</code> to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10.</p>"},{"location":"documentation/configuration/taskdef.html#task-rate-limits","title":"Task Rate Limits","text":"<p>Rate Limiting</p> <p>Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers.</p> <ul> <li><code>rateLimitFrequencyInSeconds</code> and <code>rateLimitPerFrequency</code> should be used together.</li> <li><code>rateLimitFrequencyInSeconds</code> sets the \"frequency window\", i.e the <code>duration</code> to be used in <code>events per duration</code>. Eg: 1s, 5s, 60s, 300s etc.</li> <li><code>rateLimitPerFrequency</code>defines the number of Tasks that can be given to Workers per given \"frequency window\". No rate limit if set to 0.</li> </ul> <p>Example Let's set <code>rateLimitFrequencyInSeconds = 5</code>, and <code>rateLimitPerFrequency = 12</code>. This means our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task.  </p> <p>Note that unlike <code>concurrentExecLimit</code>, rate limiting doesn't take into account tasks already in progress or a terminal state. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example.   </p>"},{"location":"documentation/configuration/taskdef.html#using-inputkeys-and-outputkeys","title":"Using <code>inputKeys</code> and <code>outputKeys</code>","text":"<ul> <li><code>inputKeys</code> and <code>outputKeys</code> can be considered as parameters and return values for the Task.</li> <li>Consider the task Definition as being represented by an interface: <code>(value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN);</code>.</li> <li>However, these parameters are not strictly enforced at the moment. Both <code>inputKeys</code> and <code>outputKeys</code> act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition.</li> <li>In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages.</li> </ul>"},{"location":"documentation/configuration/taskdef.html#using-inputtemplate","title":"Using <code>inputTemplate</code>","text":"<ul> <li><code>inputTemplate</code> allows to define default values, which can be overridden by values provided in Workflow.</li> <li>Eg: In your Task Definition, you can define your inputTemplate as:</li> </ul> <pre><code>\"inputTemplate\": {\n    \"url\": \"https://some_url:7004\"\n}\n</code></pre> <ul> <li>Now, in your workflow Definition, when using above task, you can use the default <code>url</code> or override with something else in the task's <code>inputParameters</code>.</li> </ul> <pre><code>\"inputParameters\": {\n    \"url\": \"${workflow.input.some_new_url}\"\n}\n</code></pre>"},{"location":"documentation/configuration/taskdef.html#complete-example","title":"Complete Example","text":"<p>This is an example of a Task Definition for a worker implementation named <code>encode_task</code>.</p> <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"pollTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"ownerEmail\": \"foo@bar.com\",\n  \"description\": \"Sample Encoding task\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/index.html","title":"Workflow Definition","text":"<p>The Workflow Definition contains all the information necessary to define the behavior of a workflow. The most important part of this definition is the <code>tasks</code> property, which is an array of Task Configurations. </p>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-properties","title":"Workflow Properties","text":"Field Type Description Notes name string Name of the workflow description string Description of the workflow Optional version number Numeric field used to identify the version of the schema. Use incrementing numbers. When starting a workflow execution, if not specified, the definition with highest version is used tasks array of object(s) An array of task configurations. Details inputParameters array of string(s) List of input parameters. Used for documenting the required inputs to workflow Optional. outputParameters object JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task inputTemplate object Default input values. See Using inputTemplate Optional. failureWorkflow string Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. Explanation Optional. schemaVersion number Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable boolean Flag to allow Workflow restarts Defaults to true workflowStatusListenerEnabled boolean Enable status callback. Explanation Defaults to false ownerEmail string Email address of the team that owns the workflow Required timeoutSeconds number The timeout in seconds after which the workflow will be marked as <code>TIMED_OUT</code> if it hasn't been moved to a terminal state No timeouts if set to 0 timeoutPolicy string (enum) Workflow's timeout policy Defaults to <code>TIME_OUT_WF</code>"},{"location":"documentation/configuration/workflowdef/index.html#failure-workflow","title":"Failure Workflow","text":"<p>The failure workflow gets the original failed workflow\u2019s input along with 3 additional items,</p> <ul> <li><code>workflowId</code> - The id of the failed workflow which triggered the failure workflow.</li> <li><code>reason</code> - A string containing the reason for workflow failure.</li> <li><code>failureStatus</code> - A string status representation of the failed workflow.</li> <li><code>failureTaskId</code> - The id of the failed task of the workflow that triggered the failure workflow.</li> </ul>"},{"location":"documentation/configuration/workflowdef/index.html#timeout-policy","title":"Timeout Policy","text":"<ul> <li>TIME_OUT_WF: Workflow is marked as TIMED_OUT and terminated</li> <li>ALERT_ONLY: Registers a counter (workflow_failure with status tag set to <code>TIMED_OUT</code>)</li> </ul>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-status-listener","title":"Workflow Status Listener","text":"<p>Setting the <code>workflowStatusListenerEnabled</code> field in your Workflow Definition to <code>true</code> enables notifications.</p> <p>To add a custom implementation of the Workflow Status Listener. Refer to this .</p> <p>The listener can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here.</p>"},{"location":"documentation/configuration/workflowdef/index.html#default-input-with-inputtemplate","title":"Default Input with <code>inputTemplate</code>","text":"<ul> <li><code>inputTemplate</code> allows you to define default input values, which can optionally be overridden at runtime (when the workflow is invoked).</li> <li>Eg: In your Workflow Definition, you can define your inputTemplate as:</li> </ul> <pre><code>\"inputTemplate\": {\n    \"url\": \"https://some_url:7004\"\n}\n</code></pre> <p>And <code>url</code> would be <code>https://some_url:7004</code> if no <code>url</code> was provided as input to your workflow.</p>"},{"location":"documentation/configuration/workflowdef/index.html#task-configurations","title":"Task Configurations","text":"<p>The <code>tasks</code> property in a Workflow Definition defines an array of Task Configurations. This is the blueprint for the workflow. Task Configurations can reference different types of Tasks.</p> <ul> <li>Simple Tasks</li> <li>System Tasks</li> <li>Operators</li> </ul> <p>Note: Task Configuration should not be confused with Task Definitions, which are used to register SIMPLE (worker based) tasks.</p> Field Type Description Notes name string Name of the task. MUST be registered as a Task Type with Conductor before starting workflow taskReferenceName string Alias used to refer the task within the workflow.  MUST be unique within workflow. type string Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description string Description of the task optional optional boolean true  or false.  When set to true - workflow continues even if the task fails.  The status of the task is reflected as <code>COMPLETED_WITH_ERRORS</code> Defaults to <code>false</code> inputParameters object JSON template that defines the input given to the task. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task. See Using Expressions for details inputExpression object JSONPath expression that defines the input given to the task. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task. See Using Expressions for details asyncComplete boolean <code>false</code> to mark status COMPLETED upon execution; <code>true</code> to keep the task IN_PROGRESS and wait for an external event to complete it. Defaults to <code>false</code> startDelay number Time in seconds to wait before making the task available to be polled by a worker. Defaults to 0. <p>In addition to these parameters, System Tasks have their own parameters. Check out System Tasks for more information.</p>"},{"location":"documentation/configuration/workflowdef/index.html#using-expressions","title":"Using Expressions","text":"<p>Each executed task is given an input based on the <code>inputParameters</code> template or the <code>inputExpression</code> configured in the task configuration. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task.</p>"},{"location":"documentation/configuration/workflowdef/index.html#inputparameters","title":"inputParameters","text":"<p><code>inputParameters</code> can use JSONPath expressions to extract values out of the workflow input and other tasks in the workflow.</p> <p>For example, workflows are supplied an <code>input</code> by the client/caller when a new execution is triggered. The workflow <code>input</code> is available via an expression of the form <code>${workflow.input...}</code>. Likewise, the <code>input</code> and <code>output</code> data of a previously executed task can also be extracted using an expression for use in the <code>inputParameters</code> of a subsequent task.</p> <p>Generally, <code>inputParameters</code> can use expressions of the following syntax:</p> <p><code>${SOURCE.input/output.JSONPath}</code></p> Field Description SOURCE Can be either <code>\"workflow\"</code> or the reference name of any task input/output Refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output <p>JSON Path Support</p> <p>Conductor supports JSONPath specification and uses Java implementation from here.</p> <p>Escaping expressions</p> <p>To escape an expression, prefix it with an extra $ character (ex.: <code>$${workflow.input...}</code>).</p>"},{"location":"documentation/configuration/workflowdef/index.html#inputexpression","title":"inputExpression","text":"<p><code>inputExpression</code> can be used to select an entire object from the workflow input, or the output of another task. The field supports all definite JSONPath expressions.</p> <p>The syntax for mapping values in <code>inputExpression</code> follows the pattern,</p> <p><code>SOURCE.input/output.JSONPath</code></p> <p>NOTE: The <code>inputExpression</code> field does not require the expression to be wrapped in <code>${}</code>.</p> <p>See example below.</p>"},{"location":"documentation/configuration/workflowdef/index.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/index.html#example-1-a-basic-workflow-definition","title":"Example 1 - A Basic Workflow Definition","text":"<p>Assume your business logic is to simply to get some shipping information and then do the shipping. You start by logically partitioning them into two tasks:</p> <ol> <li>shipping_info - The first task takes the provided account number, and outputs an address.  </li> <li>shipping_task - The 2nd task takes the address info and generates a shipping label.</li> </ol> <p>We can configure these two tasks in the <code>tasks</code> array of our Workflow Definition. Let's assume that <code>shipping info</code> takes an account number, and returns a name and address.</p> <pre><code>{\n  \"name\": \"mail_a_box\",\n  \"description\": \"shipping Workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"shipping_info\",\n      \"taskReferenceName\": \"shipping_info_ref\",\n      \"inputParameters\": {\n        \"account\": \"${workflow.input.accountNumber}\"\n      },\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"shipping_task\",\n      \"taskReferenceName\": \"shipping_task_ref\",\n      \"inputParameters\": {\n        \"name\": \"${shipping_info_ref.output.name}\",\n        \"streetAddress\": \"${shipping_info_ref.output.streetAddress}\",\n        \"city\": \"${shipping_info_ref.output.city}\",\n        \"state\": \"${shipping_info_ref.output.state}\",\n        \"zipcode\": \"${shipping_info_ref.output.zipcode}\",\n      },\n      \"type\": \"SIMPLE\"\n    }\n  ],\n  \"outputParameters\": {\n    \"trackingNumber\": \"${shipping_task_ref.output.trackingNumber}\"\n  },\n  \"failureWorkflow\": \"shipping_issues\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0,\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>Upon completion of the 2 tasks, the workflow outputs the tracking number generated in the 2nd task.  If the workflow fails, a second workflow named <code>shipping_issues</code> is run.</p>"},{"location":"documentation/configuration/workflowdef/index.html#example-2-task-configuration","title":"Example 2 - Task Configuration","text":"<p>Consider a task <code>http_task</code> with input configured to use input/output parameters from workflow and a task named <code>loc_task</code>.</p> <pre><code>{\n  \"name\": \"encode_workflow\",\n  \"description\": \"Encode movie.\",\n  \"version\": 1,\n  \"inputParameters\": [\n    \"movieId\", \"fileLocation\", \"recipe\"\n  ],\n  \"tasks\": [\n    {\n      \"name\": \"loc_task\",\n      \"taskReferenceName\": \"loc_task_ref\",\n      \"taskType\": \"SIMPLE\",\n      ...      \n    },    \n    {\n      \"name\": \"http_task\",\n      \"taskReferenceName\": \"http_task_ref\",\n      \"taskType\": \"HTTP\",\n      \"inputParameters\": {\n        \"movieId\": \"${workflow.input.movieId}\",\n        \"url\": \"${workflow.input.fileLocation}\",\n        \"lang\": \"${loc_task.output.languages[0]}\",\n        \"http_request\": {\n          \"method\": \"POST\",\n          \"url\": \"http://example.com/${loc_task.output.fileId}/encode\",\n          \"body\": {\n            \"recipe\": \"${workflow.input.recipe}\",\n            \"params\": {\n              \"width\": 100,\n              \"height\": 100\n            }\n          },\n          \"headers\": {\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\"\n          }\n        }\n      }\n    }\n  ],\n  \"ownerEmail\": \"conductor@example.com\",\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>Consider the following as the workflow input</p> <p><pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\":\"s3://moviebucket/file123\",\n  \"recipe\":\"png\"\n}\n</code></pre> And the output of the loc_task as the following;</p> <pre><code>{\n  \"fileId\": \"file_xxx_yyy_zzz\",\n  \"languages\": [\"en\",\"ja\",\"es\"]\n}\n</code></pre> <p>When scheduling the task, Conductor will merge the values from workflow input and <code>loc_task</code>'s output and create the input to the <code>http_task</code> as follows:</p> <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"url\": \"s3://moviebucket/file123\",\n  \"lang\": \"en\",\n  \"http_request\": {\n    \"method\": \"POST\",\n    \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\",\n    \"body\": {\n      \"recipe\": \"png\",\n      \"params\": {\n        \"width\": 100,\n        \"height\": 100\n      }\n    },\n    \"headers\": {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/json\"\n    }\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/index.html#example-3-inputexpression","title":"Example 3 - inputExpression","text":"<p>Given the following task configuration: <pre><code>{\n  \"name\": \"loc_task\",\n  \"taskReferenceName\": \"loc_task_ref\",\n  \"taskType\": \"SIMPLE\",\n  \"inputExpression\": {\n    \"expression\": \"workflow.input\",\n    \"type\": \"JSON_PATH\"\n  }  \n}\n</code></pre></p> <p>When the workflow is invoked with the following workflow input <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\":\"s3://moviebucket/file123\",\n  \"recipe\":\"png\"\n}\n</code></pre></p> <p>When the task <code>loc_task</code> is scheduled, the entire workflow input object will be passed in as the task input: <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\":\"s3://moviebucket/file123\",\n  \"recipe\":\"png\"\n}\n</code></pre></p>"},{"location":"documentation/configuration/workflowdef/operators/index.html","title":"Operators","text":"<p>Operators are built-in primitives in Conductor that allow you to define the workflow's control flow. They are similar to programming constructs such as for loops, if-else selections, and so on. Conductor supports most programming primitives, so that you can create various advanced workflows.</p> <p>Here are the operators available in Conductor OSS: </p> Operator Description Do While Do-while loops / For loops Dynamic Function pointer Dynamic Fork Dynamic parallel execution Fork Static parallel execution Join Map Set Variable Global variable declaration Sub Workflow Subroutine / Fork process Switch Switch / If..then...else selection Terminate Exit <p>The following operators are deprecated:</p> <ul> <li>Decision</li> <li>Exclusive Join</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html","title":"Do While","text":"<pre><code>\"type\" : \"DO_WHILE\"\n</code></pre> <p>The Do While task (<code>DO_WHILE</code>) sequentially executes a list of tasks as long as a given condition is true. The sequence of tasks gets executed before the condition is checked, even for the first iteration, just like a regular do.. while statement in programming.</p>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters in top level of the Do While task configuration.</p> Parameter Type Description Required / Optional loopCondition String The condition that is evaluated after each iteration. This is a JavaScript expression, evaluated using the Nashorn engine. Required. loopOver List[Task] The list of task configurations that will be executed as long as the condition is true. Required."},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Do While task.</p> <pre><code>{\n  \"name\": \"do_while\",\n  \"taskReferenceName\": \"do_while_ref\",\n  \"inputParameters\": {},\n  \"type\": \"DO_WHILE\",\n  \"loopCondition\": \"(function () {\\n  if ($.do_while_ref['iteration'] &lt; 5) {\\n    return true;\\n  }\\n  return false;\\n})();\",\n  \"loopOver\": [ // List of tasks to be executed in the loop\n    {\n        // task configuration\n    },\n    {\n        // task configuration\n    }\n  ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#output","title":"Output","text":"<p>The Do While task will return the following parameters.</p> Name Type Description iteration Integer The number of iterations.  If the Do While task is in progress, <code>iteration</code> will show the current iteration number. When completed, <code>iteration</code> will show the final number of iterations. <p>In addition, a map will be created for each iteration, keyed by its iteration number (e.g., 1, 2, 3), and will contain the task outputs for all of the <code>loopOver</code> tasks.</p> <p>Furthermore, if <code>loopCondition</code> declares any parameter, it will also appear in the output. For example, <code>storage</code> will appear in the output if <code>loopCondition</code> is <code>if ($.LoopTask['iteration'] &lt;= 10) {$.LoopTask.storage = 3; true } else {false}</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#execution","title":"Execution","text":"<p>When a Do While loop is executed, each task in the loop will have its <code>taskReferenceName</code> concatenated with __i, with i as the iteration number starting at 1. If one of the loop tasks fails, the Do While task status will be set as FAILED, and upon retry, the iteration number will restart from 1.</p> <p>Each loop task output is stored as part of the Do While task, indexed by the iteration value, allowing <code>loopCondition</code> to reference the output of a task for a specific iteration (e.g., <code>$.LoopTask['iteration]['first_task']</code>).</p>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the Do While task.</p>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#using-a-basic-script","title":"Using a basic script","text":"<p>In this example task configuration, the Do While task evaluates two criteria:</p> <pre><code>{\n    \"name\": \"Loop\",\n    \"taskReferenceName\": \"LoopTask\",\n    \"type\": \"DO_WHILE\",\n    \"inputParameters\": {\n      \"value\": \"${workflow.input.value}\"\n    },\n    \"loopCondition\": \"if ( ($.LoopTask['iteration'] &lt; $.value ) || ( $.first_task['response']['body'] &gt; 10)) { false; } else { true; }\",\n    \"loopOver\": [\n        {\n            \"name\": \"firstTask\",\n            \"taskReferenceName\": \"first_task\",\n            \"inputParameters\": {\n                \"http_request\": {\n                    \"uri\": \"http://localhost:8082\",\n                    \"method\": \"POST\"\n                }\n            },\n            \"type\": \"HTTP\"\n        },{\n            \"name\": \"secondTask\",\n            \"taskReferenceName\": \"second_task\",\n            \"inputParameters\": {\n                \"http_request\": {\n                    \"uri\": \"http://localhost:8082\",\n                    \"method\": \"POST\"\n                }\n            },\n            \"type\": \"HTTP\"\n        }\n    ],\n    \"startDelay\": 0,\n    \"optional\": false\n}\n</code></pre> <p>Assuming three executions occurred (<code>first_task__1</code>, <code>first_task__2</code>, <code>first_task__3</code>, <code>second_task__1</code>, <code>second_task__2</code>, and <code>second_task__3</code>), the Do While task will return the following will produce the following output: </p> <pre><code>{\n    \"iteration\": 3,\n    \"1\": {\n        \"first_task\": {\n            \"response\": {},\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        },\n        \"second_task\": {\n            \"response\": {},\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        }\n    },\n    \"2\": {\n        \"first_task\": {\n            \"response\": {},\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        },\n        \"second_task\": {\n            \"response\": {},\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        }\n    },\n    \"3\": {\n        \"first_task\": {\n            \"response\": {},\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        },\n        \"second_task\": {\n            \"response\": {},\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#using-the-iteration-key-in-a-loop-task","title":"Using the iteration key in a loop task","text":"<p>Sometimes, you may want to use the Do While iteration value/counter inside your loop tasks. In this example, an API call is made to a GitHub repository to get all stargazers and each iteration increases the pagination.</p> <p>To evaluate the current iteration, the parameter <code>$.get_all_stars_loop_ref['iteration']</code> is used in <code>loopCondition</code>. In the HTTP task embedded in the loop, <code>${get_all_stars_loop_ref.output.iteration}</code> is used to define which page the API should return.</p> <pre><code>{\n    \"name\": \"get_all_stars\",\n    \"taskReferenceName\": \"get_all_stars_loop_ref\",\n    \"inputParameters\": {\n        \"stargazers\": \"4000\"\n    },\n    \"type\": \"DO_WHILE\",\n    \"loopCondition\": \"if ($.get_all_stars_loop_ref['iteration'] &lt; Math.ceil($.stargazers/100)) { true; } else { false; }\",\n    \"loopOver\": [\n        {\n            \"name\": \"100_stargazers\",\n            \"taskReferenceName\": \"hundred_stargazers_ref\",\n            \"inputParameters\": {\n                \"counter\": \"${get_all_stars_loop_ref.output.iteration}\",\n                \"http_request\": {\n                    \"uri\": \"https://api.github.com/repos/ntflix/conductor/stargazers?page=${get_all_stars_loop_ref.output.iteration}&amp;per_page=100\",\n                    \"method\": \"GET\",\n                    \"headers\": {\n                        \"Authorization\": \"token ${workflow.input.gh_token}\",\n                        \"Accept\": \"application/vnd.github.v3.star+json\"\n                    }\n                }\n            },\n            \"type\": \"HTTP\"\n        }\n    ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/do-while-task.html#limitations","title":"Limitations","text":"<p>There are several limitations for the Do While task:</p> <ul> <li>Branching\u2014Within a Do While task, branching using Switch, Fork/Join, Dynamic Fork tasks are supported. However, since the loop tasks will be executed within the scope of the Do While task, any branching that crosses outside its scope will not be respected.</li> <li>Nested loops\u2014Nested Do While tasks are not supported. To achieve a similar functionality as a nested loop, you can use a Sub Workflow task inside the Do While task.</li> <li>Isolation group execution\u2014Isolation group execution is not supported. However, domain is supported for loop tasks inside the Do While task.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html","title":"Dynamic Fork","text":"<pre><code>\"type\" : \"FORK_JOIN_DYNAMIC\"\n</code></pre> <p>The Dynamic Fork task (<code>FORK_JOIN_DYNAMIC</code>) is used to run tasks in parallel, with the forking behavior (such as the task type and the number of forks) determined at runtime. This contrasts with the Fork task, where the forking behavior is defined at workflow creation. </p> <p>Like the Fork task, the Dynamic Fork task must be followed by a Join that waits on the forked tasks to finish before moving to the next task. This Join task collects the outputs from each forked tasks.</p> <p>Unlike the Fork/Join task, a Dynamic Fork task can only run one task per fork. A sub-workflow can be utilized if there is a need for multiple tasks per fork.</p> <p>There are two ways to run the Dynamic Fork task:</p> <ul> <li>Each fork runs a different task\u2014Use <code>dynamicForkTasksParam</code> and <code>dynamicForkTasksInputParamName</code>.</li> <li>All forks run the same task\u2014Use <code>forkTaskType</code> and <code>forkTaskInputs</code> for any task type, or <code>forkTaskWorkflow</code> and <code>forkTaskInputs</code> for Sub Workflow tasks.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters in top level of the Dynamic Fork task configuration. The input payload for the forked tasks should correspond with its expected input. For example, if the forked tasks are HTTP tasks, its input should include <code>http_request</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#for-different-tasks-in-each-fork","title":"For different tasks in each fork","text":"<p>To configure the Dynamic Fork task, provide a <code>dynamicForkTasksParam</code> and <code>dynamicForkTasksInputParamName</code> at the top level of the task configuration, as well as the matching parameters in <code>inputParameters</code> based on the <code>dynamicForkTasksParam</code> and <code>dynamicForkTasksInputParamName</code>.</p> Parameter Type Description Required / Optional dynamicForkTasksParam String The parameter name for <code>inputParameters</code> whose value is used to schedule the task. For example, \"dynamicTasks\". Required. inputParameters.dynamicTasks List[Task] The list of task configurations that will be executed across forks (one task per fork) Required. dynamicForkTasksInputParamName String The parameter name for <code>inputParameters</code> whose value is used to pass the required input parameters for each forked task.  For example, \"dynamicTasksInput\". Required. inputParameters.dynamicTasksInput Map[String, Map[String, Any]] The inputs for each forked task. The keys are the task reference names for each fork and the values are the input parameters that will be passed into its corresponding task. Required. <p>The Join task must run after the forked tasks. Add the Join task to complete the fork-join operations.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#for-the-same-task-any-task-type","title":"For the same task (any task type)","text":"<p>Use these parameters inside <code>inputParameters</code> in the Dynamic Fork task configuration to execute any task type (except Sub Workflow tasks) for all forks.</p> Parameter Type Description Required / Optional inputParameters.forkTaskType String (enum) The type of task that will be executed in each fork. For example, \"HTTP\", or \"SIMPLE\". Required. inputParameters.forkTaskName String The name of the Worker task (<code>SIMPLE</code>) that will be executed in each fork. Required only if <code>forkTaskType</code> is \"SIMPLE\". inputParameters.forkTaskInputs List[Map[String, Any]] The inputs for each forked task. The number of list items corresponds with the number of branches in the dynamic fork at execution. Required. <p>The Join task must run after the forked tasks. Configure the Join task as well to complete the fork-join operations.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#for-the-same-subworkflow","title":"For the same subworkflow","text":"<p>Use these parameters inside <code>inputParameters</code> in the Dynamic Fork task configuration to execute a Sub Workflow task for all forks.</p> Parameter Type Description Required / Optional inputParameters.forkTaskWorkflow String The name of the workflow that will be executed in each fork. Required. inputParameters.forkTaskWorkflowVersion Integer The version of the workflow to be executed. If unspecified, the latest version will be used. Optional. inputParameters.forkTaskInputs List[Map[String, Any]] The inputs for each forked task. The number of list items corresponds with the number of branches in the dynamic fork at execution. Required. <p>The Join task must run after the forked tasks. Configure the Join task as well to complete the fork-join operations.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#json-configuration","title":"JSON configuration","text":"<p>This is the task configuration for a Dynamic Fork task.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#for-different-tasks-in-each-fork_1","title":"For different tasks in each fork","text":"<pre><code>{\n  \"name\": \"fork_join_dynamic\",\n  \"taskReferenceName\": \"fork_join_dynamic_ref\",\n  \"inputParameters\": {\n    \"dynamicTasks\": [ // name of the tasks to execute\n      {\n        \"name\": \"http\",\n        \"taskReferenceName\": \"http_ref\",\n        \"type\": \"HTTP\",\n        \"inputParameters\": {}\n      },\n      { \n        // another task configuration \n      }\n\n    ],\n    \"dynamicTasksInput\": { // inputs for the tasks\n      \"taskReferenceName\" : {\n        \"key\": \"value\",\n        \"key\": \"value\"\n      },\n      \"anotherTaskReferenceName\" : {\n        \"key\": \"value\",\n        \"key\": \"value\"\n      }\n    }\n  },\n  \"type\": \"FORK_JOIN_DYNAMIC\",\n  \"dynamicForkTasksParam\": \"dynamicTasks\", // input parameter key that will hold the task names to execute\n  \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\" // input parameter key that will hold the input parameters for each task\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#for-the-same-task-any-task-type_1","title":"For the same task (any task type)","text":"<pre><code>{\n  \"name\": \"fork_join_dynamic\",\n  \"taskReferenceName\": \"fork_join_dynamic_ref\",\n  \"inputParameters\": {\n    \"forkTaskType\": \"HTTP\",\n    \"forkTaskInputs\": [\n      {\n        // inputs for the first branch\n      },\n      {\n        // inputs for the second branch\n      },\n      ...\n    ]\n  },\n  \"type\": \"FORK_JOIN_DYNAMIC\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#for-the-same-subworkflow_1","title":"For the same subworkflow","text":"<pre><code>{\n  \"name\": \"fork_join_dynamic\",\n  \"taskReferenceName\": \"fork_join_dynamic_ref\",\n  \"inputParameters\": {\n    \"forkTaskWorkflow\": \"someWorkflow\",\n    \"forkTaskWorkflowVersion\": 1,\n    \"forkTaskInputs\": [\n      {\n        // inputs for the first branch\n      },\n      {\n        // inputs for the second branch\n      },\n      ...\n    ]\n  },\n  \"type\": \"FORK_JOIN_DYNAMIC\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the Dynamic Fork task.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#running-different-tasks","title":"Running different tasks","text":"<p>To run a different task per fork, you must use <code>dynamicForkTasksParam</code> and <code>dynamicForkTasksInputParamName</code>.</p> <p>In this example workflow, the Dynamic Fork task spawns three forks, each running a different task (<code>HTTP</code>, <code>SIMPLE</code>, and <code>INLINE</code>). For true dynamism, you can add another task to prepare the list of tasks and inputs for the Dynamic Fork task.</p> <pre><code>{\n  \"name\": \"DynamicForkExample\",\n  \"description\": \"This workflow runs different tasks in a dynamic fork.\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"fork_join_dynamic\",\n      \"taskReferenceName\": \"fork_join_dynamic_ref\",\n      \"inputParameters\": {\n        \"dynamicTasks\": [\n          {\n            \"name\": \"inline\",\n            \"taskReferenceName\": \"task1\",\n            \"type\": \"INLINE\",\n            \"inputParameters\": {\n              \"expression\": \"(function () {\\n  return $.input;\\n})();\",\n              \"evaluatorType\": \"javascript\"\n            }\n          },\n          {\n            \"name\": \"http\",\n            \"taskReferenceName\": \"task2\",\n            \"type\": \"HTTP\",\n            \"inputParameters\": {}\n          },\n          {\n            \"name\": \"task_38\",\n            \"taskReferenceName\": \"simple_ref\",\n            \"type\": \"SIMPLE\"\n          }\n        ],\n        \"dynamicTasksInput\": {\n          \"task1\": {\n            \"input\": \"one\"\n          },\n          \"task2\": {\n            \"http_request\": {\n              \"method\": \"GET\",\n              \"uri\": \"https://randomuser.me/api/\",\n              \"connectionTimeOut\": 3000,\n              \"readTimeOut\": \"3000\",\n              \"accept\": \"application/json\",\n              \"contentType\": \"application/json\",\n              \"encode\": true\n            }\n          },\n          \"task3\": {\n            \"input\": {\n              \"someKey\": \"someValue\"\n            }\n          }\n        }\n      },\n      \"type\": \"FORK_JOIN_DYNAMIC\",\n      \"dynamicForkTasksParam\": \"dynamicTasks\",\n      \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n    },\n    {\n      \"name\": \"join\",\n      \"taskReferenceName\": \"join_ref\",\n      \"inputParameters\": {},\n      \"type\": \"JOIN\",\n      \"joinOn\": []\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {},\n  \"schemaVersion\": 2,\n  \"ownerEmail\": \"example@email.com\"\n}\n</code></pre> <p>Refer to the Join task for more details on the Join aspect of the Fork.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#running-the-same-task-worker-task","title":"Running the same task \u2014 Worker task","text":"<p>In this example workflow, a Dynamic Fork task is used to run Worker tasks (<code>SIMPLE</code>) that will resize uploaded images and store the resized images into a specified <code>location</code>.</p> <pre><code>{\n  \"name\": \"image_multiple_convert_resize_fork\",\n  \"description\": \"Image multiple convert resize example\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"image_multiple_convert_resize_dynamic_task\",\n      \"taskReferenceName\": \"image_multiple_convert_resize_dynamic_task_ref\",\n      \"inputParameters\": {\n        \"forkTaskName\": \"fork_task\",\n        \"forkTaskType\": \"SIMPLE\",\n        \"forkTaskInputs\": [\n           {\n            \"image\" : \"url1\",\n            \"location\" : \"location_url\",\n            \"width\" : 100,\n            \"height\" : 200\n           },\n           {\n            \"image\" : \"url2\",\n            \"location\" : \"location_url\",\n            \"width\" : 300,\n            \"height\" : 400\n           }\n       ]\n      },\n      \"type\": \"FORK_JOIN_DYNAMIC\",\n      \"dynamicForkTasksParam\": \"dynamicTasks\",\n      \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n    },\n    {\n      \"name\": \"image_multiple_convert_resize_join\",\n      \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\",\n      \"inputParameters\": {},\n      \"type\": \"JOIN\"\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"output\": \"${join_task_ref.output}\"\n  },\n  \"schemaVersion\": 2,\n  \"ownerEmail\": \"example@email.com\"\n}\n</code></pre> <p>Refer to the Join task for more details on the Join aspect of the Fork.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#running-the-same-task-http-task","title":"Running the same task \u2014 HTTP task","text":"<p>In this example workflow, the Dynamic Fork task runs HTTP tasks in parallel. The provided input in <code>forkTaskInputs</code> contains the typical payload expected in a HTTP task.</p> <pre><code>{\n  \"name\": \"dynamic_workflow_array_http\",\n  \"description\": \"Dynamic workflow array - run HTTP tasks\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"dynamic_workflow_array_http\",\n      \"taskReferenceName\": \"dynamic_workflow_array_http_ref\",\n      \"inputParameters\": {\n        \"forkTaskType\": \"HTTP\",\n        \"forkTaskInputs\": [\n          {\n            \"http_request\": {\n              \"method\": \"GET\",\n              \"uri\": \"https://randomuser.me/api/\"\n            }\n          },\n          {\n            \"http_request\": {\n              \"method\": \"GET\",\n              \"uri\": \"https://randomuser.me/api/\"\n            }\n          }\n        ]\n      },\n      \"type\": \"FORK_JOIN_DYNAMIC\",\n      \"dynamicForkTasksParam\": \"dynamicTasks\",\n      \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n    },\n    {\n      \"name\": \"dynamic_workflow_array_http_join\",\n      \"taskReferenceName\": \"dynamic_workflow_array_http_join_ref\",\n      \"inputParameters\": {},\n      \"type\": \"JOIN\",\n      \"joinOn\": []\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {},\n  \"schemaVersion\": 2,\n  \"ownerEmail\": \"example@email.com\"\n}\n</code></pre> <p>Refer to the Join task for more details on the Join aspect of the Fork.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-fork-task.html#running-the-same-task-sub-workflow-task","title":"Running the same task \u2014 Sub Workflow task","text":"<p>In this example workflow, the dynamic fork runs Sub Workflow tasks in parallel. Each sub-workflow will resize the image and store the resized image into a specified <code>location</code>.</p> <pre><code>{\n  \"name\": \"image_multiple_convert_resize_fork_subwf\",\n  \"description\": \"Image multiple convert resize example\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"image_multiple_convert_resize_dynamic_task_subworkflow\",\n      \"taskReferenceName\": \"image_multiple_convert_resize_dynamic_task_subworkflow_ref\",\n      \"inputParameters\": {\n        \"forkTaskWorkflow\": \"image_resize_subworkflow\",\n        \"forkTaskInputs\": [\n          {\n            \"image\": \"url1\",\n            \"location\": \"location url\",\n            \"width\": 100,\n            \"height\": 200\n          },\n          {\n            \"image\": \"url2\",\n            \"location\": \"locationurl\",\n            \"width\": 300,\n            \"height\": 400\n          }\n        ]\n      },\n      \"type\": \"FORK_JOIN_DYNAMIC\",\n      \"dynamicForkTasksParam\": \"dynamicTasks\",\n      \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n    },\n    {\n      \"name\": \"dynamic_workflow_array_http_subworkflow\",\n      \"taskReferenceName\": \"dynamic_workflow_array_http_subworkflow_ref\",\n      \"inputParameters\": {},\n      \"type\": \"JOIN\",\n      \"joinOn\": []\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {},\n  \"schemaVersion\": 2,\n  \"ownerEmail\": \"example@email.com\"\n}\n</code></pre> <p>Refer to the Join task for more details on the Join aspect of the Fork.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html","title":"Dynamic","text":"<pre><code>\"type\" : \"DYNAMIC\"\n</code></pre> <p>The Dynamic task (<code>DYNAMIC</code>) is used to execute a registered task dynamically at run-time. It is similar to a function pointer in programming, and can be used for when the decision to execute which task will only be made after the workflow has begun.</p> <p>The Dynamic task accepts as input the name of a task, which can be a system task or a Worker task (<code>SIMPLE</code>) registered on Conductor.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#task-parameters","title":"Task parameters","text":"<p>To configure the Dynamic task, provide a <code>dynamicTaskNameParam</code> at the top level of the task configuration, as well as a matching parameter in <code>inputParameters</code> based on the <code>dynamicTaskNameParam</code>.</p> <p>For example, if <code>dynamicTaskNameParam</code> is \"taskToExecute\", the task name to execute is specified in <code>taskToExecute</code> in <code>inputParameters</code>.</p> Parameter Type Description Required / Optional dynamicTaskNameParam String The parameter name for <code>inputParameters</code> whose value is used to schedule the task. For example, \"taskToExecute\". Required. inputParameters.taskToExecute String The name of the task that will be executed. Required. <p>You can also pass any other input for the Dynamic task into <code>inputParameters</code>.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Dynamic task.</p> <pre><code>{\n  \"name\": \"dynamic\",\n  \"taskReferenceName\": \"dynamic_ref\",\n  \"inputParameters\": {\n    \"taskToExecute\": \"${workflow.input.dynamicTaskName}\" // name of the task to execute\n  },\n  \"type\": \"DYNAMIC\",\n  \"dynamicTaskNameParam\": \"taskToExecute\" // input parameter key that will contain the task name to execute\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#output","title":"Output","text":"<p>During execution, the Dynamic task is replaced with whatever task that is called at runtime. The output of the Dynamic task will be whatever the output of the called task is.</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#execution","title":"Execution","text":"<p>At runtime, if an incorrect task name is provided and the task does not exist, the workflow will fail with the error \"Invalid task specified. Cannot find task by name in the task definitions.\"</p> <p>Likewise, if null reference is provided for the task name, the workflow will fail with the error \"Cannot map a dynamic task based on the parameter and input. Parameter= taskToExecute, input= {taskToExecute=null}\".</p>"},{"location":"documentation/configuration/workflowdef/operators/dynamic-task.html#examples","title":"Examples","text":"<p>In this example workflow, shipments are made with different couriers depending on the shipping address. </p> <p>The decision can only be made during runtime when the address is received, and the subsequent shipping task could be either <code>ship_via_fedex</code> or <code>ship_via_ups</code>. A Dynamic task can be used in this workflow so that the shipping task can be decided in real time.</p> <p>A preceding <code>shipping_info</code> generates an output to decide what task to run in the Dynamic task.</p> <p>Here is the workflow definition:</p> <pre><code>{\n  \"name\": \"Shipping_Flow\",\n  \"description\": \"Ships smartly based on the shipping address\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"shipping_info\",\n      \"taskReferenceName\": \"shipping_info_ref\",\n      \"inputParameters\": {},\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"shipping_task\",\n      \"taskReferenceName\": \"shipping_task_ref\",\n      \"inputParameters\": {\n        \"taskToExecute\": \"${shipping_info.output.shipping_service}\"\n      },\n      \"type\": \"DYNAMIC\",\n      \"dynamicTaskNameParam\": \"taskToExecute\"\n    }\n  ],\n  \"inputParameters\": [],\n    \"outputParameters\": {},\n  \"restartable\": true,\n  \"ownerEmail\":\"abc@example.com\",\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <p>Here is the corresponding workflow diagram:</p> <p></p> <p>The shipping service is decided based on the postal code. If the postal code starts with 9, <code>ship_via_fedex</code> is executed:</p> <p></p> <p>If the postal code starts with any other number, <code>ship_via_ups</code> is executed:</p> <p></p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html","title":"Fork","text":"<pre><code>\"type\" : \"FORK_JOIN\"\n</code></pre> <p>Also known as a static fork, a Fork task (<code>FORK_JOIN</code>) is used to run task sequences in parallel, including Sub Workflow tasks.</p> <p>The Fork task must be followed by a Join that waits on the forked tasks to finish before moving to the next task. This Join task collects the outputs from each forked tasks.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters in top level of the Fork task configuration.</p> Parameter Type Description Required / Optional forkTasks List[List[Task]] A list of tasks lists to be invoked in parallel (<code>[[...], [...]]</code>).  Each item in the outer list represents a fork that will be invoked in parallel, while each inner list contains the task configurations for a particular fork. The tasks defined within each sublist can be sequential or even more nested forks. Required. <p>The Join task must run after the forked tasks. Configure the Join task as well to complete the fork-join operations.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#json-configuration","title":"JSON configuration","text":"<p>This is the task configuration for a Fork task.</p> <pre><code>{\n  \"name\": \"fork\",\n  \"taskReferenceName\": \"fork_ref\",\n  \"inputParameters\": {},\n  \"type\": \"FORK_JOIN\",\n  \"forkTasks\": [\n    [ // fork branch\n      {\n        // task configuration\n      },\n      {\n        // task configuration\n      }\n    ],\n    [ // another fork branch \n      {\n        // task configuration\n      },\n      {\n        // task configuration\n      }\n    ]\n  ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#output","title":"Output","text":"<p>The Fork task has no output. It is used in conjunction with the JOIN task, which aggregates the outputs from the parallelized forks.</p>"},{"location":"documentation/configuration/workflowdef/operators/fork-task.html#examples","title":"Examples","text":"<p>In this example workflow, three notifications are sent: email, SMS, and HTTP. Since none of these tasks depend on each other, they can be run in parallel with a Fork task. The workflow diagram looks like this:</p> <p></p> <p>Here's the JSON configuration for the Fork task, along with its corresponding Join task:</p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_email\",\n          \"type\": \"SIMPLE\"\n        },\n        {\n          \"name\": \"email_notification\",\n          \"taskReferenceName\": \"email_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_sms\",\n          \"type\": \"SIMPLE\"\n        },\n        {\n          \"name\": \"sms_notification\",\n          \"taskReferenceName\": \"sms_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_http\",\n          \"type\": \"SIMPLE\"\n        },\n        {\n          \"name\": \"http_notification\",\n          \"taskReferenceName\": \"http_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"notification_join\",\n    \"taskReferenceName\": \"notification_join_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"email_notification_ref\",\n      \"sms_notification_ref\"\n    ]\n  }\n]\n</code></pre> <p>Refer to the Join task for more details on the Join aspect of the Fork.</p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html","title":"Join","text":"<pre><code>\"type\" : \"JOIN\"\n</code></pre> <p>A Join task is used in conjunction with a Fork or Dynamic Fork task to wait on and join the forks. The Join task also aggregates the forked tasks' outputs for subsequent use.</p> <p>The Join task's behavior varies based on the preceding fork type:</p> <ul> <li>When used with a Static Fork task, the Join task waits for a provided list of the forked tasks to be completed before proceeding with the next task. </li> <li>When used with a Dynamic Fork task, it implicitly waits for all the forked tasks to complete.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#task-parameters","title":"Task parameters","text":"<p>When used with a Static Fork, use these parameters in top level of the Join task configuration.</p> Parameter Type Description Required / Optional joinOn List[String] (For Static Forks only) A list of task reference names that the Join task will wait for completion before proceeding with the next task. If not specified, the Join will move on to the next task without waiting for any forked tasks to complete. Optional."},{"location":"documentation/configuration/workflowdef/operators/join-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Join task.</p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#with-a-static-fork","title":"With a static fork","text":"<pre><code>{\n  \"name\": \"join\",\n  \"taskReferenceName\": \"join_ref\",\n  \"inputParameters\": {},\n  \"type\": \"JOIN\",\n  \"joinOn\": [\n    // List of task reference names that the join should wait for\n  ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#with-a-dynamic-fork","title":"With a dynamic fork","text":"<pre><code>{\n  \"name\": \"join\",\n  \"taskReferenceName\": \"join_ref\",\n  \"inputParameters\": {},\n  \"type\": \"JOIN\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#output","title":"Output","text":"<p>The Join task will return a map of all completed forked task outputs (in other words, the output from all <code>joinOn</code> tasks.) The keys are task reference names of the tasks being joined and the values are the corresponding task outputs.</p> <p>Example:</p> <pre><code>{\n  \"taskReferenceName\": {\n    \"outputKey\": \"outputValue\"\n  },\n  \"anotherTaskReferenceName\": {\n    \"outputKey\": \"outputValue\"\n  },\n  \"someTaskReferenceName\": {\n    \"outputKey\": \"outputValue\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the Join task.</p>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#joining-on-all-forks","title":"Joining on all forks","text":"<p>In this example task configuration, the Join task will wait for the completion of tasks <code>my_task_ref_1</code> and <code>my_task_ref_2</code> as specified in <code>joinOn</code>.</p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"my_task\",\n          \"taskReferenceName\": \"my_task_ref_1\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"my_task\",\n          \"taskReferenceName\": \"my_task_ref_2\",\n          \"type\": \"SIMPLE\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"join_task\",\n    \"taskReferenceName\": \"my_join_task_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"my_task_ref_1\",\n      \"my_task_ref_2\"\n    ]\n  }\n]\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/join-task.html#ignoring-one-fork","title":"Ignoring one fork","text":"<p>In this example task configuration, the Fork task spawns three tasks: an <code>email_notification</code> task, a <code>sms_notification</code> task, and a <code>http_notification</code> task. </p> <p>Email and SMS are usually best-effort delivery systems, while a HTTP-based notification can be retried until it succeeds or eventually gives up. Therefore, when you set up a notification workflow, you may decide to continue the workflow after you have kicked off an email and SMS notification, but let the <code>http_notification</code> task continue to execute without blocking the rest of the workflow.</p> <p>In that case, you can specify the <code>joinOn</code> tasks as follows: </p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"email_notification\",\n          \"taskReferenceName\": \"email_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"sms_notification\",\n          \"taskReferenceName\": \"sms_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ],\n      [\n        {\n          \"name\": \"http_notification\",\n          \"taskReferenceName\": \"http_notification_ref\",\n          \"type\": \"SIMPLE\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"notification_join\",\n    \"taskReferenceName\": \"notification_join_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"email_notification_ref\",\n      \"sms_notification_ref\"\n    ]\n  }\n]\n</code></pre> <p>Here is the output of <code>notification_join</code>. The output is a map, where the keys are the task reference names of the <code>joinOn</code> tasks, and the corresponding values are the outputs of those tasks.</p> <pre><code>{\n  \"email_notification_ref\": {\n    \"email_sent_at\": \"2021-11-06T07:37:17+0000\",\n    \"email_sent_to\": \"test@example.com\"\n  },\n  \"sms_notification_ref\": {\n    \"sms_sent_at\": \"2021-11-06T07:37:17+0129\",\n    \"sms_sent_to\": \"+1-425-555-0189\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html","title":"Set Variable","text":"<pre><code>\"type\" : \"SET_VARIABLE\"\n</code></pre> <p>The Set Variable task (<code>SET_VARIABLE</code>) allows you to construct shared variables at the workflow level across tasks. </p> <p>These variables can be initialized, accessed, or overwritten at any point in the workflow:</p> <ul> <li>Once initialized, the variable can be referenced in any subsequent task using \"${workflow.variables.someName}\" (replacing someName with the actual variable name).</li> <li>Initialized values can be overwritten by a subsequent Set Variable task.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#task-parameters","title":"Task parameters","text":"<p>To configure the Set Variable task, set your desired variable names and their respective values in <code>inputParameters</code>. The values can be set in two ways:</p> <ul> <li>Hard-coded in the workflow definition, or</li> <li>A dynamic reference.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#json-configuration","title":"JSON configuration","text":"<p>This is the task configuration for a Set Variable task.</p> <pre><code>{\n  \"name\": \"set_variable\",\n  \"taskReferenceName\": \"set_variable_ref\",\n  \"type\": \"SET_VARIABLE\",\n  \"inputParameters\": {\n    \"variableName\": \"value\",\n    \"variableName2\": \"${workflow.input.someKey}\"\n    \"variableName3\": 5,\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#examples","title":"Examples","text":"<p>In this example workflow, a username is stored as a variable so that it can be reused in other tasks that require the username.</p> <pre><code>{\n  \"name\": \"Welcome_User_Workflow\",\n  \"description\": \"Designate a user to be welcomed\",\n  \"tasks\": [\n    {\n      \"name\": \"set_name\",\n      \"taskReferenceName\": \"set_name_ref\",\n      \"type\": \"SET_VARIABLE\",\n      \"inputParameters\": {\n        \"name\": \"${workflow.input.userName}\"\n      }\n    },\n    {\n      \"name\": \"greet_user\",\n      \"taskReferenceName\": \"greet_user_ref\",\n      \"inputParameters\": {\n        \"var_name\": \"${workflow.variables.name}\"\n      },\n      \"type\": \"SIMPLE\"\n    },\n    {\n      \"name\": \"send_reminder_email\",\n      \"taskReferenceName\": \"send_reminder_email_ref\",\n      \"inputParameters\": {\n        \"var_name\": \"${workflow.variables.name}\"\n      },\n      \"type\": \"SIMPLE\"\n    }\n  ]\n}\n</code></pre> <p>In the example above, <code>set_name</code> is a Set Variable task that initializes a variable <code>name</code> using a workflow input reference. In subsequent tasks, the variable is later referenced using \"${workflow.variables.name}\".</p>"},{"location":"documentation/configuration/workflowdef/operators/set-variable-task.html#limitations","title":"Limitations","text":"<p>Here are some limitation when using the Set Variable task:</p> <ul> <li>Payload limit\u2014By default, there is a hard limit for the payload size of variables defined in the JVM system properties (<code>conductor.max.workflow.variables.payload.threshold.kb</code>) of 256KB. Exceeding this limit will cause the Set Variable task to fail.</li> <li>Variable scope\u2014The scope of the Set Variable task is limited to its workflow. An initialized variable in one workflow will not carry over to another workflow or sub-workflow and will have to be re-initialized using another Set Variable task. </li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html","title":"Start Workflow","text":"<pre><code>\"type\" : \"START_WORKFLOW\"\n</code></pre> <p>The Start Workflow task (<code>START_WORKFLOW</code>) starts another workflow from the current workflow. Unlike the Sub Workflow task, the workflow triggered by the Start Workflow task will execute asynchronously. That means the current workflow proceeds to its next task without waiting for the started workflow to complete.</p> <p>A Start Workflow task is marked as COMPLETED when the requested workflow enters the RUNNING state, regardless of its final state.</p>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>inputParameters</code> in the Start Workflow task configuration.</p> Parameter Type Description Required / Optional inputParameters.startWorkflow Map[String, Any] A map that includes the requested workflow\u2019s configuration, such as the name and version. Refer to Start Workflow Request for what to include in this parameter. Required."},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#task-configuration","title":"Task configuration","text":"<p>Here is the task configuration for a Start Workflow task.\u200b</p> <pre><code>{\n  \"name\": \"start_workflow\",\n  \"taskReferenceName\": \"start_workflow_ref\",\n  \"inputParameters\": {\n    \"startWorkflow\": {\n      \"name\": \"someName\",\n      \"input\": {\n        \"someParameter\": \"someValue\",\n        \"anotherParameter\": \"anotherValue\"\n      },\n      \"version\": 1,\n      \"correlationId\": \"\"\n    }\n  },\n  \"type\": \"START_WORKFLOW\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#output","title":"Output","text":"<p>The Start Workflow task will return the following parameters.</p> Name Type Description workflowId String The workflow execution ID of the started workflow."},{"location":"documentation/configuration/workflowdef/operators/start-workflow-task.html#limitations","title":"Limitations","text":"<p>Because the Start Workflow task will neither wait for the completion of the started workflow nor pass back its output, it is not possible to access the output of the started workflow from the current workflow. If required, you can use the Sub Workflow task instead.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html","title":"Sub Workflow","text":"<pre><code>\"type\" : \"SUB_WORKFLOW\"\n</code></pre> <p>The Sub Workflow task executes another workflow within the current workflow. This allows you to nest and reuse common workflows across multiple workflows. </p> <p>Unlike the Start Workflow task, the Sub Workflow task provides synchronous execution and the executed sub-workflow will contain a reference to its parent workflow.</p> <p>The Sub Workflow task can also be used to overcome the limitations of other tasks:</p> <ul> <li>Use it in a Do While task to achieve nested Do While loops.</li> <li>Use it in a Dynamic Fork task to execute more than one task in each fork.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>subWorkflowParam</code> in the Sub Workflow task configuration.</p> Parameter Type Description Required / Optional subWorkflowParam.name String Name of the workflow to be executed. This workflow should have a pre-existing definition in Conductor. Required. subWorkflowParam.version Integer The version of the workflow to be executed. If unspecified, the latest version will be used. Required. subWorkflowParam.taskToDomain Map[String, String] Allows scheduling the sub-workflow's tasks to specific domain mappings.  Refer to Task Domains for how to configure <code>taskToDomain</code>. Optional. inputParameters Map[String, Any] Contains the sub-workflow's input parameters, if any. Optional."},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#task-configuration","title":"Task configuration","text":"<p>Here is the task configuration for a Start Workflow task.\u200b</p> <pre><code>{\n  \"name\": \"start_workflow\",\n  \"taskReferenceName\": \"start_workflow_ref\",\n  \"inputParameters\": {\n    \"startWorkflow\": {\n      \"name\": \"someName\",\n      \"input\": {\n        \"someParameter\": \"someValue\",\n        \"anotherParameter\": \"anotherValue\"\n      },\n      \"version\": 1,\n      \"correlationId\": \"\"\n    }\n  },\n  \"type\": \"START_WORKFLOW\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#output","title":"Output","text":"<p>The Sub Workflow task will return the following parameters.</p> Name Type Description subWorkflowId String The workflow execution ID of the sub-workflow. <p>In addition, the task output will also contain the sub-workflow's outputs.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#execution","title":"Execution","text":"<p>During execution, the Sub Workflow task will be marked as COMPLETED only upon the completion of the spawned workflow. If the sub-workflow fails or terminates, the Sub Workflow task will be marked as FAILED and retried if configured. </p> <p>If the Sub Workflow task is defined as optional in the parent workflow definition, the Sub Workflow task will not be retried if sub-workflow fails or terminates. In addition, even if the sub-workflow is retried/rerun/restarted after reaching to a terminal status, the parent workflow task status will remain as it is.</p>"},{"location":"documentation/configuration/workflowdef/operators/sub-workflow-task.html#examples","title":"Examples","text":"<p>In this example workflow, a Fork task containing two tasks is used to simultaneously create two images from one image:</p> <p></p> <p>The left fork will create a JPG file, and the right fork a WEBP file. Maintaining this workflow might be cumbersome, as changes made to one of the fork tasks do not automatically propagate the other.  Rather than using two tasks, we can define a single, reuseable <code>image_convert_resize</code> workflow that can be called as a sub-workflow in both forks:</p> <pre><code>{\n    \"name\": \"image_convert_resize_subworkflow1\",\n    \"description\": \"Image Processing Workflow\",\n    \"version\": 1,\n    \"tasks\": [{\n            \"name\": \"image_convert_resize_multipleformat_fork\",\n            \"taskReferenceName\": \"image_convert_resize_multipleformat_ref\",\n            \"inputParameters\": {},\n            \"type\": \"FORK_JOIN\",\n            \"decisionCases\": {},\n            \"defaultCase\": [],\n            \"forkTasks\": [\n                [{\n                    \"name\": \"image_convert_resize_sub\",\n                    \"taskReferenceName\": \"subworkflow_jpg_ref\",\n                    \"inputParameters\": {\n                        \"fileLocation\": \"${workflow.input.fileLocation}\",\n                        \"recipeParameters\": {\n                            \"outputSize\": {\n                                \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                                \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n                            },\n                            \"outputFormat\": \"jpg\"\n                        }\n                    },\n                    \"type\": \"SUB_WORKFLOW\",\n                    \"subWorkflowParam\": {\n                        \"name\": \"image_convert_resize\",\n                        \"version\": 1\n                    }\n                }],\n                [{\n                        \"name\": \"image_convert_resize_sub\",\n                        \"taskReferenceName\": \"subworkflow_webp_ref\",\n                        \"inputParameters\": {\n                            \"fileLocation\": \"${workflow.input.fileLocation}\",\n                            \"recipeParameters\": {\n                                \"outputSize\": {\n                                    \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                                    \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n                                },\n                                \"outputFormat\": \"webp\"\n                            }\n                        },\n                        \"type\": \"SUB_WORKFLOW\",\n                        \"subWorkflowParam\": {\n                            \"name\": \"image_convert_resize\",\n                            \"version\": 1\n                        }\n                    }\n\n                ]\n            ]\n        },\n        {\n            \"name\": \"image_convert_resize_multipleformat_join\",\n            \"taskReferenceName\": \"image_convert_resize_multipleformat_join_ref\",\n            \"inputParameters\": {},\n            \"type\": \"JOIN\",\n            \"decisionCases\": {},\n            \"defaultCase\": [],\n            \"forkTasks\": [],\n            \"startDelay\": 0,\n            \"joinOn\": [\n                \"subworkflow_jpg_ref\",\n                \"upload_toS3_webp_ref\"\n            ],\n            \"optional\": false,\n            \"defaultExclusiveJoinTask\": [],\n            \"asyncComplete\": false,\n            \"loopOver\": []\n        }\n    ],\n    \"inputParameters\": [],\n    \"outputParameters\": {\n        \"fileLocationJpg\": \"${subworkflow_jpg_ref.output.fileLocation}\",\n        \"fileLocationWebp\": \"${subworkflow_webp_ref.output.fileLocation}\"\n    },\n    \"schemaVersion\": 2,\n    \"restartable\": true,\n    \"workflowStatusListenerEnabled\": true,\n    \"ownerEmail\": \"conductor@example.com\",\n    \"timeoutPolicy\": \"ALERT_ONLY\",\n    \"timeoutSeconds\": 0,\n    \"variables\": {},\n    \"inputTemplate\": {}\n}\n</code></pre> <p>Here is the corresponding workflow diagram:</p> <p></p> <p>Now that the tasks are abstracted into a sub-workflow, any changes to the sub-workflow will automatically apply to both forks.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html","title":"Switch","text":"<pre><code>\"type\" : \"SWITCH\"\n</code></pre> <p>The Switch task (<code>SWITCH</code>) is used for conditional branching logic. It represents if...then...else or switch...case statements in programming, which is useful for executing one of many task sequences based on pre-defined conditions.</p> <p>At runtime, the Switch task evaluates an expression and matches the expression's output with the name of the switch cases defined in the task configuration. The workflow then executes the tasks in the matching branch. If there is matching branch found, the default branch will be executed.</p> <p>The Switch task supports two types of evaluators:</p> <ul> <li><code>value-param</code>\u2014A reference to the task input parameter key.</li> <li><code>javascript</code>\u2014A complex JavaScript expression.</li> </ul>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters in top level of the Switch task configuration.</p> Parameter Type Description Required / Optional evaluatorType String (enum) The type of the evaluator used. Supported types: <ul><li><code>value-param</code>\u2014Evaluates the input parameter referenced in <code>expression</code>.</li><li><code>javascript</code>\u2014Evaluates the JavaScript script in <code>expression</code>and computes the value.</li></ul> Required. expression String The expression evaluated by the Switch task. The expression format depends on the evaluator type: <ul><li>For <code>value-param</code>, the expression should be a parameter key provided in <code>inputParameters</code>.</li><li><code>javascript</code>, the expression should be a JavaScript expression.</li></ul> Required. decisionCases Map[String, List[task]] A map of the possible switch cases and their tasks. The keys are the possible values that can result from the evaluation of <code>expression</code>, while the values are the lists of task configurations that will be executed. Required. defaultCase List[Task] The default switch case, containing the list of tasks to be executed if no matching switch case is found in <code>decisionCases</code>. Required. inputParameters Map[String, Any] The input parameters for the task.  Note: If <code>evaluatorType</code> is <code>value-param</code>, <code>inputParameters</code> must be populated with the key specified in <code>expression</code>. Optional."},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Switch task.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#using-value-param","title":"Using <code>value-param</code>","text":"<pre><code>{\n  \"name\": \"switch\",\n  \"taskReferenceName\": \"switch_ref\",\n  \"inputParameters\": {\n    \"switchCaseValue\": \"${workflow.input}\"\n  },\n  \"type\": \"SWITCH\",\n  \"decisionCases\": {\n    \"caseName1\": [\n      {\n        // task configuration\n      }\n    ],\n    \"caseName2\": [\n      {\n        // task configuration\n      },\n      {\n        // task configuration\n      }\n    ]\n  },\n  \"defaultCase\": [\n    {// task configuration}\n  ],\n  \"evaluatorType\": \"value-param\",\n  \"expression\": \"switchCaseValue\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#using-javascript","title":"Using <code>javascript</code>","text":"<pre><code>{\n  \"name\": \"switch\",\n  \"taskReferenceName\": \"switch_ref\",\n  \"inputParameters\": {\n    \"switchCaseValue\": \"${workflow.input.num}\"\n  },\n  \"type\": \"SWITCH\",\n  \"decisionCases\": {\n    \"apples\": [\n      {\n        // task configuration\n      }\n    ],\n    \"tomatoes\":  [\n      {\n        // task configuration\n      }\n    ],\n    \"oranges\":  [\n      {\n        // task configuration\n      }\n    ]\n  },\n  \"defaultCase\": [],\n  \"evaluatorType\": \"graaljs\",\n  \"expression\": \"(function () {\\n    switch ($.switchCaseValue) {\\n      case \\\"1\\\":\\n        return \\\"apple\\\";\\n      case \\\"2\\\":\\n        return \\\"tomatoes\\\";\\n      case \\\"3\\\":\\n        return \\\"oranges\\\"\\n    }\\n  }())\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#output","title":"Output","text":"<p>The Switch task will return the following parameters.</p> Name Type Description evaluationResult List[String] A list of values representing the list of cases that matched. selectedCase String The evaluation result of the Switch task."},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the Switch task.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#using-value-param_1","title":"Using <code>value-param</code>","text":"<p>In this example workflow, a package with be shipped by a specific shipping provider, based on the given workflow input. Here is the Switch task configuration, using the <code>value-param</code> evaluatorType:</p> <pre><code>{\n  \"name\": \"switch\",\n  \"taskReferenceName\": \"switch_ref\",\n  \"inputParameters\": {\n    \"switchCaseValue\": \"${workflow.input.service}\"\n  },\n  \"type\": \"SWITCH\",\n  \"evaluatorType\": \"value-param\",\n  \"expression\": \"switchCaseValue\",\n  \"defaultCase\": [\n    {\n      ...\n    }\n  ],\n  \"decisionCases\": {\n    \"fedex\": [\n      {\n        ...\n      }\n    ],\n    \"ups\": [\n      {\n        ...\n      }\n    ]\n  }\n}\n</code></pre> <p>In the Switch task above, the value of the task input <code>switchCaseValue</code> is used to determine the selected case. The evaluator type is <code>value-param</code> and the expression is a direct reference to the name of the input parameter. </p> <p>If the value of <code>switchCaseValue</code> is <code>fedex</code>, then the <code>fedex</code> branch containing the <code>ship_via_fedex</code>task will be executed as shown below.</p> <p></p> <p>Likewise, if the input is <code>ups</code>, then the <code>ship_via_ups</code> task will be executed. If none of the cases match, then the default path will be executed.</p>"},{"location":"documentation/configuration/workflowdef/operators/switch-task.html#using-javascript_1","title":"Using <code>javascript</code>","text":"<p>In this example, the switch cases are selected using the <code>javascript</code> evaluatorType:</p> <pre><code>{\n  \"name\": \"switch\",\n  \"taskReferenceName\": \"switch_ref\",\n  \"inputParameters\": {\n    \"shipping\": \"${workflow.input.service}\"\n  },\n  \"type\": \"SWITCH\",\n  \"evaluatorType\": \"javascript\",\n  \"expression\": \"$.shipping == 'fedex' ? 'fedex' : 'ups'\",\n  \"defaultCase\": [\n    {\n      ...\n    }\n  ],\n  \"decisionCases\": {\n    \"fedex\": [\n      {\n        ...\n      }\n    ],\n    \"ups\": [\n      {\n        ...\n      }\n    ]\n  }\n}\n</code></pre> <p>Inside the task's JavaScript-based expression, the task's input parameter is referenced using \"$.shipping\".</p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html","title":"Terminate","text":"<pre><code>\"type\" : \"TERMINATE\"\n</code></pre> <p>The Terminate task (<code>TERMINATE</code>) terminates the current workflow with a termination status and reason, and sets the workflow output with any supplied values. </p> <p>Often used in Switch tasks, the Terminate task can act as a return statement for cases where you want the workflow to be terminated without continuing to the subsequent tasks.</p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>inputParameters</code> in the Terminate task configuration.</p> Parameter Type Description Required / Optional inputParameters.terminationStatus String (enum) The termination status. Supported types: <ul><li>COMPLETED</li><li>FAILED</li><li>TERMINATED</li></ul> Required. inputParameters.terminationReason String The reason for terminating the current workflow, which will provide the context of the termination.  For FAILED workflows, this reason is passed to any configured <code>failureWorkflow</code>. Optional. inputParameters.workflowOutput Any The expected workflow output upon termination. Optional."},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#configuration-json","title":"Configuration JSON","text":"<p>Here is the task configuration for a Terminate task.</p> <pre><code>{\n  \"name\": \"terminate\",\n  \"taskReferenceName\": \"terminate_ref\",\n  \"inputParameters\": {\n    \"terminationStatus\": \"TERMINATED\",\n    \"terminationReason\": \"\",\n    \"workflowOutput\": \"${someTask.output}\"\n  },\n  \"type\": \"TERMINATE\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#output","title":"Output","text":"<p>The Terminate task will return the following parameters.</p> Name Type Description output Map[String, Any] A map of the workflow output on termination, as defined in <code>inputParameters.workflowOutput</code>. If <code>workflowOutput</code> is not set in the Terminate task configuration, the output will be an empty object."},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the Terminate task.</p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#using-the-terminate-task-in-a-switch-case","title":"Using the Terminate task in a switch case","text":"<p>In this example workflow, a decision is made to ship with a specific shipping provider based on the provided workflow input. If the provided input does not match the available shipping providers, then the workflow will terminate with a FAILED status. Here is a snippet that shows the default switch case terminating the workflow:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"type\": \"SWITCH\",\n  \"defaultCase\": [\n      {\n      \"name\": \"terminate\",\n      \"taskReferenceName\": \"terminate_ref\",\n      \"type\": \"TERMINATE\",\n      \"inputParameters\": {\n          \"terminationStatus\": \"FAILED\",\n          \"terminationReason\":\"Shipping provider not found.\"\n      }      \n    }\n   ]\n}\n</code></pre> <p>The full workflow with the Terminate task looks like this:</p> <p></p>"},{"location":"documentation/configuration/workflowdef/operators/terminate-task.html#best-practices","title":"Best practices","text":"<p>Here are some best practices for handling workflow termination:</p> <ul> <li>Include a termination reason when terminating the workflow with FAILED status, so that it is easy to understand the cause.</li> <li>Include any additional details in the workflow output (e.g., output of the tasks, the selected switch case), to add context to the path taken to termination.</li> </ul>"},{"location":"documentation/configuration/workflowdef/systemtasks/index.html","title":"System Tasks","text":"<p>System tasks are built-in tasks that are general purpose and reusable. These tasks run on the Conductor servers and are executed by Conductor workers, allowing you to get started without having to write custom workers.</p> <p>Here are the system tasks available in Conductor OSS: </p> System Task Description Event Publish events to an external eventing system (AMQP, SQS, Kafka, and so on). HTTP Invoke an HTTP(S) endpoint. Human Wait for an external trigger. Inline Execute lightweight JavaScript code inline. No Op Do nothing. JSON JQ Transform Use jq to transform JSON data. Kafka Publish Publish messages to Kafka. Wait Wait until a certain time has passed. <p>The following tasks are deprecated:</p> <ul> <li>Lambda</li> </ul>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html","title":"Event Task","text":"<pre><code>\"type\" : \"EVENT\"\n</code></pre> <p>The Event task (<code>EVENT</code>) is used to publish events to supported eventing systems. It enables event-based dependencies within workflows and tasks, making it possible to trigger external systems as part of the workflow execution.</p> <p>The following queuing systems are supported:</p> <ul> <li>Conductor internal queue</li> <li>AMQP</li> <li>Kafka</li> <li>NATS</li> <li>NATS Streaming</li> <li>SQS</li> </ul>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters in top level of the Event task configuration.</p> Parameter Type Description Required / Optional sink String The target event queue in the format <code>prefix:location</code>, where the prefix denotes the queuing system, and the location represents the specific queue name (e.g., <code>send_email_queue</code>). Supported prefixes: <ul><li><code>conductor</code></li> <li><code>ampq</code>, <code>amqp_queue</code>, or <code>amqp_exchange</code></li> <li><code>kafka</code></li> <li><code>nats</code></li> <li><code>nats-stream</code></li> <li><code>sqs</code></li></ul> Note: For all queuing systems except the Conductor queue, you should use the queue's name, not the URI in <code>location</code>. The URI will be looked up based on the queue name. Refer to Conductor sink configuration for more details on how to use the Conductor queue. Required. inputParameters Map[String, Any]. Any other input parameters for the Event task, which will be published to the queuing system. Optional. asyncComplete Boolean Whether the task is completed asynchronously. The default value is false. <ul><li>false\u2014Task status is set to COMPLETED upon successful execution.</li> <li>true\u2014Task status is kept as IN_PROGRESS until an external event marks it as complete.</li></ul> Optional."},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#conductor-sink-configuration","title":"Conductor sink configuration","text":"<p>When using Conductor as sink, you have two options to set the sink:  * <code>conductor</code>  * <code>conductor:&lt;workflow_name&gt;:&lt;queue_name&gt;</code> (same as the <code>event</code> value of the event handler)</p> <p>If the workflow name and queue name is omitted, it will default to the Event task's workflow name and its own <code>taskReferenceName</code> for the queue name.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#configuration-json","title":"Configuration JSON","text":"<p>Here is the task configuration for an Event task.</p> <pre><code>{\n  \"name\": \"event\",\n  \"taskReferenceName\": \"event_ref\",\n  \"type\": \"EVENT\",\n  \"inputParameters\": {},\n  \"sink\": \"sqs:sqs_queue_name\",\n  \"asyncComplete\": false\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#output","title":"Output","text":"<p>The Event task will return the following parameters.</p> Name Type Description event_produced String The name of the event produced. When producing an event with Conductor as a sink, the event name will be formatted as <code>conductor:&lt;workflow_name&gt;:&lt;task_reference_name&gt;</code>. workflowInstanceId String The workflow execution ID. workflowType String The workflow name. workflowVersion Integer The workflow version. correlationId String The workflow correlation ID. sink String The <code>sink</code> value. asyncComplete Boolean The <code>asyncComplete</code> value. taskToDomain Map[String, String] The Event task's domain mapping, if any. <p>The published event's payload is identical to the task output, minus <code>event_produced</code>.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/event-task.html#examples","title":"Examples","text":"<p>In this example, the Event task sends a message to the Conductor queue.</p> <pre><code>{\n  \"name\": \"event_task\",\n  \"taskReferenceName\": \"event_0\",\n  \"inputParameters\": {\n    \"mod\": \"${workflow.input.mod}\",\n    \"oddEven\": \"${workflow.input.oddEven}\",\n    \"sink\": \"conductor\",\n    \"asyncComplete\": false\n  },\n  \"type\": \"EVENT\",\n  \"decisionCases\": {},\n  \"defaultCase\": [],\n  \"forkTasks\": [],\n  \"startDelay\": 0,\n  \"joinOn\": [],\n  \"sink\": \"conductor\",\n  \"optional\": false,\n  \"defaultExclusiveJoinTask\": [],\n  \"asyncComplete\": false,\n  \"loopOver\": [],\n  \"onStateChange\": {},\n  \"permissive\": false\n}\n</code></pre> <p>Here is the Event task output upon execution:</p> <pre><code>{\n  \"event_produced\": \"conductor:test workflow:event_0\",\n  \"mod\": \"2\",\n  \"oddEven\": \"5\",\n  \"asyncComplete\": false,\n  \"sink\": \"conductor\",\n  \"workflowType\": \"test workflow\",\n  \"correlationId\": null,\n  \"taskToDomain\": {},\n  \"workflowVersion\": 1,\n  \"workflowInstanceId\": \"b7c1e6d9-4a80-48b6-b901-487afef9d7c1\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html","title":"HTTP Task","text":"<pre><code>\"type\" : \"HTTP\"\n</code></pre> <p>The HTTP task (<code>HTTP</code>) is useful for make calls to remote services exposed over HTTP/HTTPS. It supports various HTTP methods, headers, body content, and other configurations needed for interacting with APIs or remote services.</p> <p>The data returned in the HTTP call can be referenced in subsequent tasks as inputs, enabling you to chain multiple tasks or HTTP calls to create complex flows without writing any additional code.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>inputParameters</code> in the HTTP task configuration.</p> Parameter Type Description Required / Optional inputParameters.http_request HttpRequest JSON object containing the URI, method, and more. Required. inputParameters.http_request.uri String The URI for the HTTP service. You can construct the URI using dynamic references, as shown in the GET example below. Required. inputParameters.http_request.method String The HTTP method. Supported methods: <ul><li>GET</li> <li>PUT</li> <li>POST</li> <li>PATCH</li><li>DELETE</li> <li>OPTIONS</li> <li>HEAD</li> <li>TRACE</li></ul> Required. inputParameters.http_request.accept String The accept header required by the server. The default value is <code>application/json</code>. Supported values include: <ul><li>application/json</li> <li>application/xml</li> <li>application/pdf</li> <li>application/octet-stream</li> <li>application/x-www-form-urlencoded</li> <li>text/plain</li> <li>text/html</li> <li>text/xml</li> <li>image/jpeg</li> <li>image/png</li> <li>image/gif</li></ul> Optional. inputParameters.http_request.contentType String The content type for the server. The default value is <code>application/json</code>. Supported values include: <ul><li>application/json</li> <li>text/plain</li> <li>text/html</li> </ul> Optional. inputParameters.http_request.headers Map[String, Any] A map of additional HTTP headers to be sent along with the request.  Tip: If the remote address that you are connecting to is a secure location, add the Authorization header with <code>Bearer &lt;access_token&gt;</code> to <code>headers</code>. Optional. inputParameters.http_request.body Map[String, Any] The request body. Required for POST, PUT, or PATCH methods. inputParameters.http_request.asyncComplete Boolean Whether the task is completed asynchronously. The default value is false. <ul><li>false\u2014Task status is set to COMPLETED upon successful execution.</li> <li>true\u2014Task status is kept as IN_PROGRESS until an external event (via Conductor or SQS or EventHandler) marks it as complete.</li></ul> Tip: If the remote service sends an asynchronous event to signal the completion of the request, consider setting <code>asyncComplete</code> to <code>true</code>. Optional. inputParameters.http_request.connectionTimeOut Integer The connection timeout in milliseconds. The default is 100.  Set to 0 for no timeout. Optional. inputParameters.http_request.readTimeOut Integer Read timeout in milliseconds. The default is 150.  Set to 0 for no timeout. Optional."},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#configuration-json","title":"Configuration JSON","text":"<p>Here is the task configuration for an HTTP task.</p> <pre><code>{\n  \"name\": \"http\",\n  \"taskReferenceName\": \"http_ref\",\n  \"type\": \"HTTP\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://orkes-api-tester.orkesconductor.com/api\",\n      \"method\": \"POST\",\n      \"accept\": \"application/json\",\n      \"contentType\": \"application/json\",\n      \"encode\": true,\n      \"headers\": {\n        \"header-1\": \"${workflow.input.header-1}\"\n      },\n      \"body\": {\n        \"key\": \"value\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#output","title":"Output","text":"<p>The HTTP task will return the following parameters.</p> Name Type Description response Map[String, Any] The JSON body containing the request response, if available. response.headers Map[String, Any] The response headers. response.statusCode Integer The HTTP status code indicating the request outcome. response.reasonPhrase String The reason phrase associated with the HTTP status code. response.body Map[String, Any] The response body containing the data returned by the endpoint."},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#execution","title":"Execution","text":"<p>The HTTP task is moved to COMPLETED status once the remote service responds successfully.</p> <p>If your HTTP tasks are not getting picked up, you might have too many HTTP tasks in the task queue. Consider using Isolation Groups to prioritize certain HTTP tasks over others. </p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the HTTP task.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#get-method","title":"GET Method","text":"<pre><code>{\n  \"name\": \"Get Example\",\n  \"taskReferenceName\": \"get_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/${workflow.input.queryid}\",\n      \"method\": \"GET\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#post-method","title":"POST Method","text":"<pre><code>{\n  \"name\": \"http_post_example\",\n  \"taskReferenceName\": \"post_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/\",\n      \"method\": \"POST\",\n      \"body\": {\n        \"title\": \"${get_example.output.response.body.title}\",\n        \"userId\": \"${get_example.output.response.body.userId}\",\n        \"action\": \"doSomething\"\n      }\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#put-method","title":"PUT Method","text":"<pre><code>{\n  \"name\": \"http_put_example\",\n  \"taskReferenceName\": \"put_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\",\n      \"method\": \"PUT\",\n      \"body\": {\n        \"title\": \"${get_example.output.response.body.title}\",\n        \"userId\": \"${get_example.output.response.body.userId}\",\n        \"action\": \"doSomethingDifferent\"\n      }\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/http-task.html#delete-method","title":"DELETE Method","text":"<pre><code>{\n  \"name\": \"DELETE Example\",\n  \"taskReferenceName\": \"delete_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\",\n      \"method\": \"DELETE\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html","title":"Human Task","text":"<pre><code>\"type\" : \"HUMAN\"\n</code></pre> <p>The Human task (<code>HUMAN</code>) is used to pause the workflow and wait for an external signal. It acts as a gate that remains in IN_PROGRESS until marked as COMPLETED or FAILED by an external trigger.</p> <p>The Human task can be used when the workflow needs to pause and wait for human intervention, such as manual approval. It can also be used with an event coming from external source such as Kafka, SQS, or Conductor's internal queueing mechanism.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#task-parameters","title":"Task parameters","text":"<p>No parameters are required to configure the Human task.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Human task.</p> <pre><code>{\n    \"name\": \"human\",\n  \"taskReferenceName\": \"human_ref\",\n    \"inputParameters\": {},\n    \"type\": \"HUMAN\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#completing-the-human-task","title":"Completing the Human task","text":"<p>There are several ways to complete the Human task:</p> <ul> <li>Using the Task Update API</li> <li>Using an event handler</li> </ul>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#task-update-api","title":"Task Update API","text":"<p>Use the Task Update API (<code>POST api/tasks</code>) to complete a Human task. Provide the <code>taskId</code>, the task status, and the desired task output.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/human-task.html#event-handler","title":"Event handler","text":"<p>If SQS integration is enabled, the Human task can also be resolved using the Update Queue APIs:</p> <ol> <li><code>POST api/queue/update/{workflowId}/{taskRefName}/{status}</code> </li> <li><code>POST api/queue/update/{workflowId}/task/{taskId}/{status}</code> </li> </ol> <p>Any parameter that is sent in the body of the POST message will be repeated as the output of the task.  For example, if we send a COMPLETED message as follows:</p> <pre><code>curl -X \"POST\" \"{{ server_host }}{{ api_prefix }}/queue/update/{workflowId}/waiting_around_ref/COMPLETED\" -H 'Content-Type: application/json' -d '{\"data_key\":\"somedatatoWait1\",\"data_key2\":\"somedatatoWAit2\"}'\n</code></pre> <p>The output of the Human task will be:</p> <pre><code>{\n  \"data_key\":\"somedatatoWait1\",\n  \"data_key2\":\"somedatatoWAit2\"\n}\n</code></pre> <p>Alternatively, an event handler using the <code>complete_task</code> action can also be configured.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html","title":"Inline Task","text":"<pre><code>\"type\": \"INLINE\"\n</code></pre> <p>The Inline task (<code>INLINE</code>) executes lightweight scripting logic inside the Conductor server JVM and immediately returns a result that can be wired into downstream tasks.</p> <p>The Inline task is best for small, deterministic logic like simple validation or calculation. For heavy, custom logic, it is best to use a Worker task (<code>SIMPLE</code>) instead.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>inputParameters</code> in the Inline task configuration.</p> Parameter Type Description Required / Optional inputParameters.evaluatorType String The type of evaluator used. Supported types: <code>javascript</code> Required. inputParameters.expression String The expression to be evaluated by the evaluator. The expression must return a value.  The JavaScript evaluator accepts code written to the ECMAScript 5.1(ES5) standard.  Note: To use ES6 instead, set the environment variable <code>CONDUCTOR_NASHORN_ES6_ENABLED</code> to <code>true</code>. Required. inputParameters Map[String, Any] Any other input parameters for the Inline task. You can include any other input values required for evaluation here, which can be referenced in <code>expression</code> as <code>$.value</code>. Optional."},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for an Inline task.</p> <pre><code>{\n  \"name\": \"inline\",\n  \"taskReferenceName\": \"inline_ref\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n    \"evaluatorType\": \"javascript\",\n    \"expression\": \"(function(){ return $.input1 + $.input2; })()\",\n    \"input1\": 1,\n    \"input2\": 2\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#output","title":"Output","text":"<p>The Inline task will return the following parameters.</p> Name Type Description result Map Contains the output returned by the evaluator based on the <code>expression</code>."},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the Inline task.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#simple-example","title":"Simple example","text":"<pre><code>{\n  \"name\": \"INLINE_TASK\",\n  \"taskReferenceName\": \"inline_test\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n      \"inlineValue\": \"${workflow.input.inlineValue}\",\n      \"evaluatorType\": \"javascript\",\n      \"expression\": \"function scriptFun(){if ($.inlineValue == 1){ return {testvalue: true} } else { return\n      {testvalue: false} }} scriptFun();\"\n  }\n}\n</code></pre> <p>The Inline task output can then be referenced in downstream tasks using the expression <code>\"${inline_test.output.result.testvalue}\"</code>.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/inline-task.html#formatting-data","title":"Formatting data","text":"<p>In this example, the Inline task is used to ensure that downstream tasks only receive weather data in Celcius.</p> <pre><code>{\n  \"name\": \"INLINE_TASK\",\n  \"taskReferenceName\": \"inline_test\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n      \"scale\": \"${workflow.input.tempScale}\",\n        \"temperature\": \"${workflow.input.temperature}\",\n      \"evaluatorType\": \"javascript\",\n      \"expression\": \"function SIvaluesOnly(){if ($.scale === \"F\"){ centigrade = ($.temperature -32)*5/9; return {temperature: centigrade} } else { return \n      {temperature: $.temperature} }} SIvaluesOnly();\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html","title":"JSON JQ Transform Task","text":"<pre><code>\"type\" : \"JSON_JQ_TRANSFORM\"\n</code></pre> <p>The JSON JQ Transform task (<code>JSON_JQ_TRANSFORM</code>) processes JSON data using jq. It is useful for transforming data from one task's output into the input of another task.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>inputParameters</code> in the JSON JQ Transform task configuration.</p> <p><code>queryExpression</code> is appended to the <code>inputParameters</code> of <code>JSON_JQ_TRANSFORM</code>, along side any other input values needed for the evaluation.</p> Parameter Type Description Required / Optional inputParameters.queryExpression String The jq filter, which is the expression used to transform the JSON data.  Refer to the JQ Manual for more information on constructing filters. Required. inputParameters Map[String, Any] Contains the inputs for the jq transformation. Required."},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a JSON JQ Transform task.</p> <pre><code>{\n  \"name\": \"json_transform\",\n  \"taskReferenceName\": \"json_transform_ref\",\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"inputParameters\": {\n    \"persons\": [\n      {\n        \"name\": \"some\",\n        \"last\": \"name\",\n        \"email\": \"mail@mail.com\",\n        \"id\": 1\n      },\n      {\n        \"name\": \"some2\",\n        \"last\": \"name2\",\n        \"email\": \"mail2@mail.com\",\n        \"id\": 2\n      }\n    ],\n    \"queryExpression\": \".persons | map({user:{email,id}})\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#output","title":"Output","text":"<p>The JSON JQ Transform task will return the following parameters.</p> Name Type Description result List[Map[String, Any]] The first element of the <code>resultList</code> returned by the jq filter. resultList List[List[Map[String, Any]]] A list of results returned by the jq filter. error String An optional error message if the jq filter failed."},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#examples","title":"Examples","text":"<p>Here are some examples for using the JSON JQ Transform task.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#simple-example","title":"Simple example","text":"<p>In this example, the jq filter expression <code>key3: (.key1.value1 + .key2.value2)</code> will concatenate the two provided string arrays in <code>key1</code> and <code>key2</code> into a single array named <code>key3</code>.</p> <pre><code>{\n  \"name\": \"jq_example_task\",\n  \"taskReferenceName\": \"my_jq_example_task\",\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"inputParameters\": {\n    \"key1\": {\n      \"value1\": [\n        \"a\",\n        \"b\"\n      ]\n    },\n    \"key2\": {\n      \"value2\": [\n        \"c\",\n        \"d\"\n      ]\n    },\n    \"queryExpression\": \"{ key3: (.key1.value1 + .key2.value2) }\"\n  }\n}\n</code></pre> <p>The above JSON JQ Transform task will provide the following output. In this case, both <code>resultList</code> and <code>result</code> are the same.</p> <pre><code>{\n  \"result\": {\n    \"key3\": [\n      \"a\",\n      \"b\",\n      \"c\",\n      \"d\"\n    ]\n  },\n  \"resultList\": [\n    {\n      \"key3\": [\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/json-jq-transform-task.html#simplifying-data","title":"Simplifying data","text":"<p>In this example, the JSON JQ Transform task is used to simplify and extract data from an extremely dense API response. The HTTP task retrieves a list of stargazers (users who have starred a repository) from GitHub, and the response for just one user looks like this:</p> <pre><code>\"body\":[\n  {\n  \"starred_at\":\"2016-12-14T19:55:46Z\",\n  \"user\":{\n    \"login\":\"lzehrung\",\n    \"id\":924226,\n    \"node_id\":\"MDQ6VXNlcjkyNDIyNg==\",\n    \"avatar_url\":\"https://avatars.githubusercontent.com/u/924226?v=4\",\n    \"gravatar_id\":\"\",\n    \"url\":\"https://api.github.com/users/lzehrung\",\n    \"html_url\":\"https://github.com/lzehrung\",\n    \"followers_url\":\"https://api.github.com/users/lzehrung/followers\",\n    \"following_url\":\"https://api.github.com/users/lzehrung/following{/other_user}\",\n    \"gists_url\":\"https://api.github.com/users/lzehrung/gists{/gist_id}\",\n    \"starred_url\":\"https://api.github.com/users/lzehrung/starred{/owner}{/repo}\",\n    \"subscriptions_url\":\"https://api.github.com/users/lzehrung/subscriptions\",\n    \"organizations_url\":\"https://api.github.com/users/lzehrung/orgs\",\n    \"repos_url\":\"https://api.github.com/users/lzehrung/repos\",\n    \"events_url\":\"https://api.github.com/users/lzehrung/events{/privacy}\",\n    \"received_events_url\":\"https://api.github.com/users/lzehrung/received_events\",\n    \"type\":\"User\",\n    \"site_admin\":false\n  }\n}\n]\n</code></pre> <p>Since the only data required are the <code>starred_at</code> and <code>login</code> parameters for users who starred the repository after a given date (provided as a workflow input <code>${workflow.input.cutoff_date}</code>), we can use the JSON JQ Transform task to simplify the output:</p> <pre><code>{\n  \"name\": \"jq_cleanup_stars\",\n  \"taskReferenceName\": \"jq_cleanup_stars_ref\",\n  \"inputParameters\": {\n    \"starlist\": \"${hundred_stargazers_ref.output.response.body}\",\n    \"queryExpression\": \"[.starlist[] | select (.starred_at &gt; \\\"${workflow.input.cutoff_date}\\\") |{occurred_at:.starred_at, member: {github:  .user.login}}]\"\n  },\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"decisionCases\": {},\n  \"defaultCase\": [],\n  \"forkTasks\": [],\n  \"startDelay\": 0,\n  \"joinOn\": [],\n  \"optional\": false,\n  \"defaultExclusiveJoinTask\": [],\n  \"asyncComplete\": false,\n  \"loopOver\": []\n}\n</code></pre> <p>In the above task configuration, the API response JSON is stored in the <code>starlist</code> parameter.  The <code>queryExpression</code> reads the JSON, selects only entries where the <code>starred_at</code> value meets the date criteria, and generates output JSON in the following format:</p> <pre><code>{\n  \"occurred_at\": \"date from JSON\",\n  \"member\":{\n    \"github\" : \"github Login from JSON\"\n  }\n}\n</code></pre> <p>The <code>queryExpression</code> is wrapped in <code>[]</code> to indicate that the response should be an array.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html","title":"Kafka Publish Task","text":"<pre><code>\"type\" : \"KAFKA_PUBLISH\"\n</code></pre> <p>The Kafka Publish task (<code>KAFKA_PUBLISH</code>) is used to push messages to another microservice via Kafka.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#task-parameters","title":"Task parameters","text":"<p>The task expects a field named <code>kafka_request</code> as part of the task's <code>inputParameters</code>.</p> <p>Use these parameters inside <code>inputParameters</code> in the Kafka Publish task configuration.</p> Parameter Type Description Required / Optional inputParameters.kafka_request KafkaRequest JSON object containing the bootstrap server, message, and more. Required. inputParameters.kafka_request.bootStrapServers String The bootstrap server for connecting to the Kafka cluster. Required. inputParameters.kafka_request.topic String The topic to publish the message to. Required. inputParameters.kafka_request.value Any The message to publish. Required. inputParameters.kafka_request.key String The Kafka message key. Messages with the same key will be sent to the same topic partition. Optional. inputParameters.kafka_request.keySerializer String (enum) The serializer used for serializing the message key. The default is <code>StringSerializer</code>. Supported values: <ul><li><code>org.apache.kafka.common.serialization.IntegerSerializer</code></li> <li><code>org.apache.kafka.common.serialization.LongSerializer</code></li> <li><code>org.apache.kafka.common.serialization.StringSerializer</code></li></ul> Optional. inputParameters.kafka_request.headers Map[String, Any] Any additional headers to be sent along with the Kafka message. Optional. inputParameters.kafka_request.requestTimeoutMs Integer The request timeout in milliseconds while awaiting a response. Optional. inputParameters.kafka_request.maxBlockMs Integer The maximum blocking time while publishing to Kafka. Optional."},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Kafka Publish task.</p> <pre><code>{\n  \"name\": \"kafka\",\n  \"taskReferenceName\": \"kafka_ref\",\n  \"inputParameters\": {\n    \"kafka_request\": {\n      \"topic\": \"userTopic\",\n      \"value\": \"Message to publish\",\n      \"bootStrapServers\": \"localhost:9092\",\n      \"headers\": {\n        \"x-Auth\":\"Auth-key\"    \n      },\n      \"key\": \"123\",\n      \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\"\n    }\n  },\n  \"type\": \"KAFKA_PUBLISH\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/kafka-publish-task.html#output","title":"Output","text":"<p>The task transitions to COMPLETED if the message has been successfully published to the Kafka queue, or marked as FAILED if the message could not be published.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/noop-task.html","title":"No Op Task","text":"<pre><code>\"type\" : \"NOOP\"\n</code></pre> <p>The No Op task (NOOP) is a no-op task. It can be used in Switch tasks in cases where there are switch cases that require no action.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/noop-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a No Op task.</p> <pre><code>{\n    \"name\": \"noop\",\n    \"taskReferenceName\": \"noop_ref\",\n    \"inputParameters\": {},\n    \"type\": \"NOOP\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html","title":"Wait Task","text":"<pre><code>\"type\" : \"WAIT\"\n</code></pre> <p>The Wait task (<code>WAIT</code>) is used to pause the workflow until a certain duration or timestamp. It is a a no-op task that will remain IN_PROGRESS until the configured time has passed, at which point it will be marked as COMPLETED.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#task-parameters","title":"Task parameters","text":"<p>Use these parameters inside <code>inputParameters</code> in the Wait task configuration. You can configure the Wait task using either <code>duration</code> or <code>until</code> in <code>inputParameters</code>.</p> Parameter Type Description Required / Optional inputParameters.duration String The wait duration in the format <code>x days y hours z minutes aa seconds</code>. The accepted units in this field are: <ul><li>days, or d for days</li> <li>hours, hrs, or h for hours</li> <li>minutes, mins, or m for minutes</li> <li>seconds, secs, or s for seconds</li></ul> Required for duration wait type. inputParameters.until String The datetime and timezone to wait until, in one of the following formats: <ul><li>yyyy-MM-dd HH:mm z</li> <li>yyyy-MM-dd HH:mm</li> <li>yyyy-MM-dd</li></ul>  For example, 2024-04-30 15:20 GMT+04:00. Required for until wait type."},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#json-configuration","title":"JSON configuration","text":"<p>Here is the task configuration for a Wait task.</p>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#using-duration","title":"Using <code>duration</code>","text":"<pre><code>{\n    \"name\": \"wait\",\n    \"taskReferenceName\": \"wait_ref\",\n    \"inputParameters\": {\n        \"duration\": \"10m20s\"\n    },\n    \"type\": \"WAIT\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#using-until","title":"Using <code>until</code>","text":"<pre><code>{\n    \"name\": \"wait\",\n    \"taskReferenceName\": \"wait_ref\",\n    \"inputParameters\": {\n        \"until\": \"2022-12-31 11:59\"\n    },\n    \"type\": \"WAIT\"\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/systemtasks/wait-task.html#overriding-the-wait-task","title":"Overriding the Wait task","text":"<p>In edge cases, the Task Update API (<code>POST api/tasks</code>) can be used to set the status of the Wait task to COMPLETED prior to the configured wait duration or timestamp.</p> <p>However, if the workflow does not require a specific wait duration or timestamp, it is recommended to directly use the Human task instead, which waits for an external trigger.</p>"},{"location":"documentation/metrics/client.html","title":"Client Metrics","text":"<p>When using the Java client, the following metrics are published:</p> Name Purpose Tags task_execution_queue_full Counter to record execution queue has saturated taskType task_poll_error Client error when polling for a task queue taskType, includeRetries, status task_paused Counter for number of times the task has been polled, when the worker has been paused taskType task_execute_error Execution error taskType task_ack_failed Task ack failed taskType task_ack_error Task ack has encountered an exception taskType task_update_error Task status cannot be updated back to server taskType task_poll_counter Incremented each time polling is done taskType task_poll_time Time to poll for a batch of tasks taskType task_execute_time Time to execute a task taskType task_result_size Records output payload size of a task taskType workflow_input_size Records input payload size of a workflow workflowType, workflowVersion external_payload_used Incremented each time external payload storage is used name, operation, payloadType <p>Metrics on client side supplements the one collected from server in identifying the network as well as client side issues.</p>"},{"location":"documentation/metrics/server.html","title":"Server Metrics","text":"<p>Conductor uses spectator to collect the metrics.</p> <ul> <li>To enable conductor serve to publish metrics, add this dependency to your build.gradle.</li> <li>Create your own AbstractModule that overides configure function and registers the Spectator metrics registry.</li> <li>Initialize the Registry and add it to the global registry via <code>((CompositeRegistry)Spectator.globalRegistry()).add(...)</code>.</li> </ul> <p>The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks.</p> Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType"},{"location":"documentation/metrics/server.html#collecting-metrics-with-log4j","title":"Collecting metrics with Log4j","text":"<p>One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel.</p> <p>Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties:</p> <pre><code>conductor.metrics-logger.enabled = true\nconductor.metrics-logger.reportPeriodSeconds = 15\n</code></pre> <p>This will push all available metrics into log4j every 15 seconds.</p> <p>By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file:</p> <pre><code>log4j.rootLogger=INFO,console,file\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=/app/logs/conductor.log\nlog4j.appender.file.MaxFileSize=10MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n# Dedicated file appender for metrics\nlog4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender\nlog4j.appender.fileMetrics.File=/app/logs/metrics.log\nlog4j.appender.fileMetrics.MaxFileSize=10MB\nlog4j.appender.fileMetrics.MaxBackupIndex=10\nlog4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout\nlog4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\nlog4j.logger.ConductorMetrics=INFO,console,fileMetrics\nlog4j.additivity.ConductorMetrics=false\n</code></pre> <p>This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var:</p> <pre><code>LOG4J_PROP=log4j-file-appender.properties\n</code></pre> <p>This variable is used by startup.sh script.</p>"},{"location":"documentation/metrics/server.html#integration-with-logstash-using-a-log-file","title":"Integration with logstash using a log file","text":"<p>The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender -&gt; logstash -&gt; ElasticSearch.</p> <p>Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch.</p> <p>Following configuration needs to be used in logstash to achieve it:</p> <p>pipeline.yml:</p> <pre><code>- pipeline.id: conductor_metrics\n  path.config: \"/usr/share/logstash/pipeline/logstash_metrics.conf\"\n  pipeline.workers: 2\n</code></pre> <p>logstash_metrics.conf</p> <pre><code>input {\n\n file {\n  path =&gt; [\"/conductor-server-logs/metrics.log\"]\n  codec =&gt; multiline {\n      pattern =&gt; \"^%{TIMESTAMP_ISO8601} \"\n      negate =&gt; true\n      what =&gt; previous\n  }\n }\n}\n\nfilter {\n    kv {\n        field_split =&gt; \", \"\n        include_keys =&gt; [ \"name\", \"type\", \"count\", \"value\" ]\n    }\n    mutate {\n        convert =&gt; {\n          \"count\" =&gt; \"integer\"\n          \"value\" =&gt; \"float\"\n        }\n      }\n}\n\noutput {\n elasticsearch {\n  hosts =&gt; [\"elasticsearch:9200\"]\n }\n}\n</code></pre> <p>Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types</p>"},{"location":"documentation/metrics/server.html#integration-with-fluentd-using-a-syslog-channel","title":"Integration with fluentd using a syslog channel","text":"<p>Another example of metrics collection uses: log4j syslog appender -&gt; fluentd -&gt; prometheus.</p> <p>In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel:</p> <pre><code>    log4j.rootLogger=INFO,console,file\n\n    log4j.appender.console=org.apache.log4j.ConsoleAppender\n    log4j.appender.console.layout=org.apache.log4j.PatternLayout\n    log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    log4j.appender.file=org.apache.log4j.RollingFileAppender\n    log4j.appender.file.File=/app/logs/conductor.log\n    log4j.appender.file.MaxFileSize=10MB\n    log4j.appender.file.MaxBackupIndex=10\n    log4j.appender.file.layout=org.apache.log4j.PatternLayout\n    log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    # Syslog based appender streaming metrics into fluentd\n    log4j.appender.server=org.apache.log4j.net.SyslogAppender\n    log4j.appender.server.syslogHost=fluentd:5170\n    log4j.appender.server.facility=LOCAL1\n    log4j.appender.server.layout=org.apache.log4j.PatternLayout\n    log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    log4j.logger.ConductorMetrics=INFO,console,server\n    log4j.additivity.ConductorMetrics=false\n</code></pre> <p>And on the fluentd side you need following configuration:</p> <pre><code>    &lt;source&gt;\n      @type prometheus\n    &lt;/source&gt;\n\n    &lt;source&gt;\n      @type syslog\n      port 5170\n      bind 0.0.0.0\n      tag conductor\n        &lt;parse&gt;\n         ; only allow TIMER metrics of workflow execution and extract tenant ID\n          @type regexp\n          expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(?&lt;tenant&gt;.*)_(?&lt;workflow&gt;.+), count=(?&lt;count&gt;\\d+), min=(?&lt;min&gt;[\\d.]+), max=(?&lt;max&gt;[\\d.]+), mean=(?&lt;mean&gt;[\\d.]+).*$/\n          types count:integer,min:float,max:float,mean:float\n        &lt;/parse&gt;\n    &lt;/source&gt;\n\n    &lt;filter conductor.local1.info&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name conductor_workflow_count\n          type gauge\n          desc The total number of executed workflows\n          key count\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n        &lt;metric&gt;\n          name conductor_workflow_max_duration\n          type gauge\n          desc Max duration in millis for a workflow\n          key max\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n        &lt;metric&gt;\n          name conductor_workflow_mean_duration\n          type gauge\n          desc Mean duration in millis for a workflow\n          key mean\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n    &lt;/filter&gt;\n\n    &lt;match **&gt;\n      @type stdout\n    &lt;/match&gt;\n</code></pre> <p>With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics </p>"},{"location":"documentation/metrics/server.html#collecting-metrics-with-prometheus","title":"Collecting metrics with Prometheus","text":"<p>Another way to collect metrics is using Prometheus client to push them to Prometheus server.</p> <p>Conductor provides optional modules that connect metrics registry with Prometheus. To enable these modules, configure following additional module property in config.properties:</p> <pre><code>conductor.metrics-prometheus.enabled = true\n</code></pre> <p>This will simply push these metrics via Prometheus collector. However, you need to configure your own Prometheus collector and expose the metrics via an endpoint.</p>"},{"location":"resources/contributing.html","title":"Contributing","text":"<p>Thanks for your interest in Conductor! This guide helps to find the most efficient way to contribute, ask questions, and report issues.</p>"},{"location":"resources/contributing.html#code-of-conduct","title":"Code of conduct","text":"<p>Please review our code of conduct.</p>"},{"location":"resources/contributing.html#i-have-a-question","title":"I have a question!","text":"<p>We have a dedicated discussion forum for asking \"how to\" questions and to discuss ideas. The discussion forum is a great place to start if you're considering creating a feature request or work on a Pull Request. Please do not create issues to ask questions.</p>"},{"location":"resources/contributing.html#i-want-to-contribute","title":"I want to contribute!","text":"<p>We welcome Pull Requests and already had many outstanding community contributions! Creating and reviewing Pull Requests take considerable time. This section helps you to set up a smooth Pull Request experience.</p> <p>The stable branch is main.</p> <p>Please create pull requests for your contributions against main only.</p> <p>It's a great idea to discuss the new feature you're considering on the discussion forum before writing any code. There are often different ways you can implement a feature. Getting some discussion about different options helps shape the best solution. When starting directly with a Pull Request, there is the risk of having to make considerable changes. Sometimes that is the best approach, though! Showing an idea with code can be very helpful; be aware that it might be throw-away work. Some of our best Pull Requests came out of multiple competing implementations, which helped shape it to perfection.</p> <p>Also, consider that not every feature is a good fit for Conductor. A few things to consider are:</p> <ul> <li>Is it increasing complexity for the user, or might it be confusing?</li> <li>Does it, in any way, break backward compatibility (this is seldom acceptable)</li> <li>Does it require new dependencies (this is rarely acceptable for core modules)</li> <li>Should the feature be opt-in or enabled by default. For integration with a new Queuing recipe or persistence module, a separate module which can be optionally enabled is the right choice.  </li> <li>Should the feature be implemented in the main Conductor repository, or would it be better to set up a separate repository? Especially for integration with other systems, a separate repository is often the right choice because the life-cycle of it will be different.</li> </ul> <p>Of course, for more minor bug fixes and improvements, the process can be more light-weight.</p> <p>We'll try to be responsive to Pull Requests. Do keep in mind that because of the inherently distributed nature of open source projects, responses to a PR might take some time because of time zones, weekends, and other things we may be working on.</p>"},{"location":"resources/contributing.html#i-want-to-report-an-issue","title":"I want to report an issue","text":"<p>If you found a bug, it is much appreciated if you create an issue. Please include clear instructions on how to reproduce the issue, or even better, include a test case on a branch. Make sure to come up with a descriptive title for the issue because this helps while organizing issues.</p>"},{"location":"resources/contributing.html#i-have-a-great-idea-for-a-new-feature","title":"I have a great idea for a new feature","text":"<p>Many features in Conductor have come from ideas from the community. If you think something is missing or certain use cases could be supported better, let us know! You can do so by opening a discussion on the discussion forum. Provide as much relevant context to why and when the feature would be helpful. Providing context is especially important for \"Support XYZ\" issues since we might not be familiar with what \"XYZ\" is and why it's useful. If you have an idea of how to implement the feature, include that as well.</p> <p>Once we have decided on a direction, it's time to summarize the idea by creating a new issue.</p>"},{"location":"resources/contributing.html#code-style","title":"Code Style","text":"<p>We use spotless to enforce consistent code style for the project, so make sure to run <code>gradlew spotlessApply</code> to fix any violations after code changes.</p>"},{"location":"resources/contributing.html#license","title":"License","text":"<p>By contributing your code, you agree to license your contribution under the terms of the APLv2: https://github.com/conductor-oss/conductor/blob/main/LICENSE</p> <p>All files are released with the Apache 2.0 license, and the following license header will be automatically added to your new file if none present:</p> <pre><code>/**\n * Copyright $YEAR Conductor authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n */\n</code></pre>"},{"location":"resources/license.html","title":"License","text":"<p>Copyright 2023 Conductor authors.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"resources/related.html","title":"Community projects related to Conductor","text":""},{"location":"resources/related.html#client-sdks","title":"Client SDKs","text":"<p>Further, all of the (non-Java) SDKs have a new GitHub home: the Conductor SDK repository is your new source for Conductor SDKs:</p> <ul> <li>Java</li> <li>JavaScript</li> <li>Go</li> <li>Python</li> <li>C#</li> <li>Clojure</li> </ul> <p>All contributions on the above client SDKs can be made on Conductor OSS repository.</p>"},{"location":"resources/related.html#microservices-operations","title":"Microservices operations","text":"<ul> <li> <p>https://github.com/flaviostutz/schellar - Schellar is a scheduler tool for instantiating Conductor workflows from time to time, mostly like a cron job, but with transport of input/output variables between calls.</p> </li> <li> <p>https://github.com/flaviostutz/backtor - Backtor is a backup scheduler tool that uses Conductor workers to handle backup operations and decide when to expire backups (ex.: keep backup 3 days, 2 weeks, 2 months, 1 semester)</p> </li> <li> <p>https://github.com/cquon/conductor-tools - Conductor CLI for launching workflows, polling tasks, listing running tasks etc</p> </li> </ul>"},{"location":"resources/related.html#conductor-deployment","title":"Conductor deployment","text":"<ul> <li> <p>https://github.com/flaviostutz/conductor-server - Docker container for running Conductor with  Prometheus metrics plugin installed and some tweaks to ease provisioning of workflows from json files embedded to the container</p> </li> <li> <p>https://github.com/flaviostutz/conductor-ui - Docker container for running Conductor UI so that you can easily scale UI independently</p> </li> <li> <p>https://github.com/flaviostutz/elasticblast - \"Elasticsearch to Bleve\" bridge tailored for running Conductor on top of Bleve indexer. The footprint of Elasticsearch may cost too much for small deployments on Cloud environment.</p> </li> <li> <p>https://github.com/mohelsaka/conductor-prometheus-metrics - Conductor plugin for exposing Prometheus metrics over path '/metrics'</p> </li> </ul>"},{"location":"resources/related.html#oauth20-security-configuration","title":"OAuth2.0 Security Configuration","text":"<p>OAuth2.0 Role Based Security! - Spring Security with easy configuration to secure the Conductor server APIs.</p> <p>Docker image published to Docker Hub</p>"},{"location":"resources/related.html#conductor-worker-utilities","title":"Conductor Worker utilities","text":"<ul> <li> <p>https://github.com/ggrcha/conductor-go-client - Conductor Golang client for writing Workers in Golang</p> </li> <li> <p>https://github.com/courosh12/conductor-dotnet-client - Conductor DOTNET client for writing Workers in DOTNET</p> </li> <li> <p>https://github.com/TwoUnderscorez/serilog-sinks-conductor-task-log - Serilog sink for sending worker log events to Conductor</p> </li> <li> <p>https://github.com/davidwadden/conductor-workers - Various ready made Conductor workers for common operations on some platforms (ex.: Jira, Github, Concourse)</p> </li> </ul>"},{"location":"resources/related.html#conductor-web-ui","title":"Conductor Web UI","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor-ng-ui - Angular based - Conductor Workflow Management UI</li> </ul>"},{"location":"resources/related.html#conductor-persistence","title":"Conductor Persistence","text":""},{"location":"resources/related.html#mongo-persistence","title":"Mongo Persistence","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor/tree/mongo_persistence - With option to use Mongo Database as persistence unit.</li> <li>Mongo Persistence / Option to use Mongo Database as persistence unit.</li> <li>Docker Compose example with MongoDB Container.</li> </ul>"},{"location":"resources/related.html#oracle-persistence","title":"Oracle Persistence","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor/tree/oracle_persistence - With option to use Oracle Database as persistence unit.</li> <li>Oracle Persistence / Option to use Oracle Database as persistence unit : version &gt; 12.2 - Tested well with 19C</li> <li>Docker Compose example with Oracle Container.</li> </ul>"},{"location":"resources/related.html#schedule-conductor-workflow","title":"Schedule Conductor Workflow","text":"<ul> <li>https://github.com/jas34/scheduledwf - It solves the following problem statements:<ul> <li>At times there are use cases in which we need to run some tasks/jobs only at a scheduled time.</li> <li>In microservice architecture maintaining schedulers in various microservices is a pain.</li> <li>We should have a central dedicate service that can do scheduling for us and provide a trigger to a microservices at expected time.</li> </ul> </li> <li>It offers an additional module <code>io.github.jas34.scheduledwf.config.ScheduledWfServerModule</code> built on the existing core  of conductor and does not require deployment of any additional service. For more details refer: Schedule Conductor Workflows and Capability In Conductor To Schedule Workflows</li> </ul>"}]}